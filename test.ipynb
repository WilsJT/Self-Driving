{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1c5cc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras_tuner.tuners import RandomSearch\n",
    "from keras_tuner.engine.hyperparameters import HyperParameters\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, RandomizedSearchCV, learning_curve, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b16ea9a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>30</th>\n",
       "      <th>45</th>\n",
       "      <th>60</th>\n",
       "      <th>90</th>\n",
       "      <th>120</th>\n",
       "      <th>135</th>\n",
       "      <th>150</th>\n",
       "      <th>speed</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.497866</td>\n",
       "      <td>-0.156357</td>\n",
       "      <td>-0.730413</td>\n",
       "      <td>-1.140428</td>\n",
       "      <td>-0.805707</td>\n",
       "      <td>-0.156357</td>\n",
       "      <td>1.491395</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.442252</td>\n",
       "      <td>-0.169266</td>\n",
       "      <td>-0.728503</td>\n",
       "      <td>-1.127933</td>\n",
       "      <td>-0.801854</td>\n",
       "      <td>-0.169266</td>\n",
       "      <td>1.554570</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.497866</td>\n",
       "      <td>-0.156357</td>\n",
       "      <td>-0.730413</td>\n",
       "      <td>-1.140428</td>\n",
       "      <td>-0.805707</td>\n",
       "      <td>-0.156357</td>\n",
       "      <td>1.491395</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.497866</td>\n",
       "      <td>-0.156357</td>\n",
       "      <td>-0.730413</td>\n",
       "      <td>-1.140428</td>\n",
       "      <td>-0.805707</td>\n",
       "      <td>-0.156357</td>\n",
       "      <td>1.491395</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.480616</td>\n",
       "      <td>-0.167738</td>\n",
       "      <td>-0.747360</td>\n",
       "      <td>-1.173858</td>\n",
       "      <td>-0.747360</td>\n",
       "      <td>-0.148480</td>\n",
       "      <td>1.504180</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29050</th>\n",
       "      <td>1.395053</td>\n",
       "      <td>-0.045195</td>\n",
       "      <td>-0.808261</td>\n",
       "      <td>-1.203214</td>\n",
       "      <td>-0.808261</td>\n",
       "      <td>-0.045195</td>\n",
       "      <td>1.515073</td>\n",
       "      <td>358</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29051</th>\n",
       "      <td>1.463368</td>\n",
       "      <td>-0.065307</td>\n",
       "      <td>-0.801349</td>\n",
       "      <td>-1.193423</td>\n",
       "      <td>-0.801349</td>\n",
       "      <td>-0.065307</td>\n",
       "      <td>1.463368</td>\n",
       "      <td>367</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29052</th>\n",
       "      <td>1.286868</td>\n",
       "      <td>-0.089803</td>\n",
       "      <td>-0.800793</td>\n",
       "      <td>-1.205045</td>\n",
       "      <td>-0.800793</td>\n",
       "      <td>-0.003265</td>\n",
       "      <td>1.612830</td>\n",
       "      <td>323</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29053</th>\n",
       "      <td>1.367514</td>\n",
       "      <td>-0.096175</td>\n",
       "      <td>-0.805693</td>\n",
       "      <td>-1.215923</td>\n",
       "      <td>-0.757765</td>\n",
       "      <td>-0.046524</td>\n",
       "      <td>1.554566</td>\n",
       "      <td>323</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29054</th>\n",
       "      <td>2.206032</td>\n",
       "      <td>0.312789</td>\n",
       "      <td>-0.536776</td>\n",
       "      <td>-1.045008</td>\n",
       "      <td>-0.536776</td>\n",
       "      <td>0.178806</td>\n",
       "      <td>-0.579067</td>\n",
       "      <td>295</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29055 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             30        45        60        90       120       135       150  \\\n",
       "0      1.497866 -0.156357 -0.730413 -1.140428 -0.805707 -0.156357  1.491395   \n",
       "1      1.442252 -0.169266 -0.728503 -1.127933 -0.801854 -0.169266  1.554570   \n",
       "2      1.497866 -0.156357 -0.730413 -1.140428 -0.805707 -0.156357  1.491395   \n",
       "3      1.497866 -0.156357 -0.730413 -1.140428 -0.805707 -0.156357  1.491395   \n",
       "4      1.480616 -0.167738 -0.747360 -1.173858 -0.747360 -0.148480  1.504180   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "29050  1.395053 -0.045195 -0.808261 -1.203214 -0.808261 -0.045195  1.515073   \n",
       "29051  1.463368 -0.065307 -0.801349 -1.193423 -0.801349 -0.065307  1.463368   \n",
       "29052  1.286868 -0.089803 -0.800793 -1.205045 -0.800793 -0.003265  1.612830   \n",
       "29053  1.367514 -0.096175 -0.805693 -1.215923 -0.757765 -0.046524  1.554566   \n",
       "29054  2.206032  0.312789 -0.536776 -1.045008 -0.536776  0.178806 -0.579067   \n",
       "\n",
       "       speed  label  \n",
       "0          0      0  \n",
       "1          0      0  \n",
       "2          0      0  \n",
       "3          0      0  \n",
       "4         23      1  \n",
       "...      ...    ...  \n",
       "29050    358      4  \n",
       "29051    367      4  \n",
       "29052    323      4  \n",
       "29053    323      4  \n",
       "29054    295      4  \n",
       "\n",
       "[29055 rows x 9 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data.csv\")\n",
    "df.drop([\"Unnamed: 0\"], inplace=True, axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21b0eab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(df[\"label\"])\n",
    "data = np.array(df[[\"30\", \"45\", \"60\", \"90\", \"120\", \"135\", \"150\", \"speed\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36f38d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a1c6dc",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24ecfec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38523307957034103"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KNN = KNeighborsClassifier()\n",
    "\n",
    "KNN.fit(X_train, y_train)\n",
    "\n",
    "KNN.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "506f9b03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_examples, train_score, valid_score = learning_curve(KNN, X_train, y_train, cv=StratifiedKFold(3))\n",
    "plt.plot(training_examples, np.mean(train_score, axis=1), label=\"train_score\")\n",
    "plt.plot(training_examples, np.mean(valid_score, axis=1), label=\"valid_score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0be2acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean_fit_time': array([0.01333102, 0.01333372, 0.01365527, 0.01293778, 0.01421364,\n",
      "       0.01564749, 0.01329788, 0.01296401, 0.01364398, 0.01562277,\n",
      "       0.01530186, 0.01993378, 0.01663319, 0.01396163, 0.01396243,\n",
      "       0.0149591 , 0.01961517, 0.02011887, 0.01660506, 0.01214981,\n",
      "       0.01195757, 0.01197823, 0.01224661, 0.01297752, 0.01195566,\n",
      "       0.01361322, 0.01263452, 0.01230558, 0.01363007, 0.0122989 ,\n",
      "       0.01262863, 0.01812204, 0.01883602, 0.01861811, 0.0189476 ,\n",
      "       0.0208923 , 0.01695752, 0.01775511, 0.01779477, 0.01630457,\n",
      "       0.01758297, 0.01767055, 0.01482201, 0.01130176, 0.01495918,\n",
      "       0.01065071, 0.01162354, 0.01096932, 0.01114027, 0.01097147,\n",
      "       0.01163999, 0.01217198, 0.01198212, 0.01230502, 0.01994824,\n",
      "       0.02027973, 0.01996017, 0.01995842, 0.0199465 , 0.01930809,\n",
      "       0.01914446, 0.01994602, 0.01928123, 0.01961613, 0.01915606,\n",
      "       0.01927018, 0.02127473, 0.03096024, 0.02991883, 0.03257807,\n",
      "       0.03058505, 0.03004281, 0.02499779, 0.01662191, 0.0172871 ,\n",
      "       0.0166475 , 0.02426958, 0.02942292, 0.0287083 , 0.02877959,\n",
      "       0.02878459, 0.01662159, 0.0162913 , 0.01698033, 0.01628955,\n",
      "       0.01660959, 0.0169421 , 0.01694043, 0.01728773, 0.01727478,\n",
      "       0.01564964, 0.01562556, 0.015625  , 0.01562643, 0.02328046,\n",
      "       0.02725291, 0.02925102, 0.02645524, 0.02472734, 0.0243818 ,\n",
      "       0.02338878, 0.02493238, 0.02559463, 0.02426831, 0.02653241,\n",
      "       0.02426887, 0.02692866, 0.02872904]), 'std_fit_time': array([1.88222210e-03, 1.88570656e-03, 1.70459207e-03, 6.69052860e-04,\n",
      "       1.86102575e-03, 4.87459494e-04, 4.69965953e-04, 1.52040533e-06,\n",
      "       4.52994840e-04, 3.08570857e-03, 1.25588522e-03, 5.62503086e-03,\n",
      "       3.86458838e-03, 1.07214749e-06, 1.12391596e-07, 1.40905349e-03,\n",
      "       1.24222360e-03, 6.15286784e-04, 3.38506936e-03, 6.05953753e-04,\n",
      "       1.75377713e-05, 1.87067025e-05, 4.33853558e-04, 1.52119441e-05,\n",
      "       1.66350953e-05, 2.33483833e-03, 4.71207084e-04, 4.65931816e-04,\n",
      "       1.69593405e-03, 4.68279595e-04, 4.75304099e-04, 5.39737480e-04,\n",
      "       1.48744799e-03, 1.24612143e-03, 8.15271137e-04, 3.47992621e-03,\n",
      "       1.59784258e-03, 1.53614552e-03, 1.52929194e-03, 9.46605319e-04,\n",
      "       1.71218861e-03, 9.27465114e-04, 2.61515492e-03, 4.71036437e-04,\n",
      "       5.64104660e-03, 4.51701866e-04, 4.61980539e-04, 2.27297936e-06,\n",
      "       6.18178667e-04, 1.58547857e-06, 4.61705702e-04, 5.82124273e-04,\n",
      "       1.73312797e-05, 4.63677492e-04, 2.94122601e-05, 4.70640302e-04,\n",
      "       2.11113784e-05, 1.69219349e-05, 8.14296062e-04, 4.52660010e-04,\n",
      "       2.76034034e-04, 9.60274217e-07, 4.70302724e-04, 4.71540157e-04,\n",
      "       2.64981655e-04, 4.53604280e-04, 3.29155648e-03, 1.44601991e-03,\n",
      "       8.23045576e-04, 2.05064647e-03, 4.65987308e-04, 1.54230950e-03,\n",
      "       5.09235246e-03, 4.69403995e-04, 4.70977235e-04, 4.88798261e-04,\n",
      "       5.42137279e-03, 1.08236581e-03, 1.08091464e-03, 9.46543706e-04,\n",
      "       1.95691682e-03, 9.66975693e-04, 4.71763734e-04, 7.97288800e-04,\n",
      "       4.69965550e-04, 9.47291199e-04, 7.99256752e-04, 1.68016978e-05,\n",
      "       4.68168903e-04, 4.78567375e-04, 4.63084023e-04, 4.69741170e-04,\n",
      "       4.45818815e-04, 4.68548055e-04, 5.73093424e-03, 1.25244341e-03,\n",
      "       3.08533093e-03, 7.04338826e-04, 1.88775628e-03, 6.31584641e-04,\n",
      "       2.61981203e-03, 3.37174788e-07, 1.69233371e-03, 4.71089626e-04,\n",
      "       9.72741160e-04, 1.24330579e-03, 8.13907347e-04, 1.05265606e-03]), 'mean_score_time': array([0.19836259, 0.20618025, 0.20638339, 0.21035647, 0.22910158,\n",
      "       0.23147559, 0.26164524, 0.20511961, 0.26595338, 0.22865582,\n",
      "       0.34931175, 0.29310727, 0.27027949, 0.26434135, 0.26861978,\n",
      "       0.3152318 , 0.39735341, 0.42917434, 0.26577727, 0.21465405,\n",
      "       0.22521869, 0.21717143, 0.2224803 , 0.22473137, 0.28841186,\n",
      "       0.21243366, 0.29035378, 0.22391359, 0.3173689 , 0.23903481,\n",
      "       0.33180714, 0.42121021, 0.42903304, 0.43789951, 0.46148833,\n",
      "       0.4506135 , 0.33864148, 0.33075023, 0.35194842, 0.33472411,\n",
      "       0.35668564, 0.36104902, 0.38682095, 0.22541102, 0.32906421,\n",
      "       0.24134811, 0.34550007, 0.24367142, 0.31566811, 0.32433891,\n",
      "       0.35373052, 0.36661061, 0.37739714, 0.37048952, 0.18668   ,\n",
      "       0.17421206, 0.18221656, 0.18118056, 0.18451111, 0.18816543,\n",
      "       0.16288638, 0.17954739, 0.17055249, 0.18136303, 0.17334223,\n",
      "       0.17987967, 0.2044704 , 0.27721914, 0.26247176, 0.28447692,\n",
      "       0.26864282, 0.27347287, 0.2222662 , 0.16050959, 0.16362397,\n",
      "       0.16362103, 0.25398485, 0.30585909, 0.27198188, 0.28497529,\n",
      "       0.25027966, 0.16383441, 0.16189941, 0.16488314, 0.15625421,\n",
      "       0.15791305, 0.16262046, 0.16908654, 0.17120862, 0.17100978,\n",
      "       0.17021569, 0.1737419 , 0.17262379, 0.17510009, 0.28306484,\n",
      "       0.32176471, 0.29977353, 0.2618595 , 0.27054548, 0.2909894 ,\n",
      "       0.27001953, 0.28921914, 0.29908848, 0.27440723, 0.30336634,\n",
      "       0.30657784, 0.31270305, 0.31393027]), 'std_score_time': array([0.00400299, 0.00820953, 0.00253963, 0.00384867, 0.00926534,\n",
      "       0.01879684, 0.00792425, 0.00378852, 0.00406946, 0.00664776,\n",
      "       0.02848496, 0.02363635, 0.01560606, 0.01213962, 0.00047643,\n",
      "       0.05436729, 0.0183107 , 0.0057064 , 0.03723955, 0.00343488,\n",
      "       0.00423001, 0.00448526, 0.00458509, 0.00542008, 0.01330475,\n",
      "       0.00371109, 0.00529494, 0.00398998, 0.01038611, 0.01503664,\n",
      "       0.04955538, 0.00682077, 0.01303833, 0.01971232, 0.01603292,\n",
      "       0.00869586, 0.00532989, 0.01877256, 0.00639384, 0.01572294,\n",
      "       0.00881026, 0.01574535, 0.06205468, 0.00084653, 0.00598677,\n",
      "       0.01174259, 0.00784134, 0.0040262 , 0.0075306 , 0.01313216,\n",
      "       0.01792534, 0.00603581, 0.01438583, 0.02053999, 0.00690313,\n",
      "       0.00330647, 0.0020812 , 0.00167175, 0.00215984, 0.00207579,\n",
      "       0.0012286 , 0.0014815 , 0.00122057, 0.00342557, 0.00299166,\n",
      "       0.00153707, 0.04098377, 0.01000908, 0.00704521, 0.01637766,\n",
      "       0.00390501, 0.00398024, 0.04119319, 0.00146862, 0.00158256,\n",
      "       0.00149903, 0.03907245, 0.01745598, 0.003455  , 0.00655715,\n",
      "       0.03078158, 0.002135  , 0.00124208, 0.0012295 , 0.0027389 ,\n",
      "       0.00207979, 0.00288182, 0.00188332, 0.00169525, 0.0008385 ,\n",
      "       0.00094943, 0.00105903, 0.00148958, 0.00156122, 0.01503408,\n",
      "       0.02567789, 0.01279866, 0.00361707, 0.00957866, 0.00476577,\n",
      "       0.03713336, 0.00633956, 0.0158143 , 0.01189251, 0.00593673,\n",
      "       0.02223373, 0.0067152 , 0.00099972]), 'param_KNN__algorithm': masked_array(data=['ball_tree', 'ball_tree', 'ball_tree', 'ball_tree',\n",
      "                   'ball_tree', 'ball_tree', 'ball_tree', 'ball_tree',\n",
      "                   'ball_tree', 'ball_tree', 'ball_tree', 'ball_tree',\n",
      "                   'ball_tree', 'ball_tree', 'ball_tree', 'ball_tree',\n",
      "                   'ball_tree', 'ball_tree', 'ball_tree', 'ball_tree',\n",
      "                   'ball_tree', 'ball_tree', 'ball_tree', 'ball_tree',\n",
      "                   'ball_tree', 'ball_tree', 'ball_tree', 'ball_tree',\n",
      "                   'ball_tree', 'ball_tree', 'ball_tree', 'ball_tree',\n",
      "                   'ball_tree', 'ball_tree', 'ball_tree', 'ball_tree',\n",
      "                   'ball_tree', 'ball_tree', 'ball_tree', 'ball_tree',\n",
      "                   'ball_tree', 'ball_tree', 'ball_tree', 'ball_tree',\n",
      "                   'ball_tree', 'ball_tree', 'ball_tree', 'ball_tree',\n",
      "                   'ball_tree', 'ball_tree', 'ball_tree', 'ball_tree',\n",
      "                   'ball_tree', 'ball_tree', 'kd_tree', 'kd_tree',\n",
      "                   'kd_tree', 'kd_tree', 'kd_tree', 'kd_tree', 'kd_tree',\n",
      "                   'kd_tree', 'kd_tree', 'kd_tree', 'kd_tree', 'kd_tree',\n",
      "                   'kd_tree', 'kd_tree', 'kd_tree', 'kd_tree', 'kd_tree',\n",
      "                   'kd_tree', 'kd_tree', 'kd_tree', 'kd_tree', 'kd_tree',\n",
      "                   'kd_tree', 'kd_tree', 'kd_tree', 'kd_tree', 'kd_tree',\n",
      "                   'kd_tree', 'kd_tree', 'kd_tree', 'kd_tree', 'kd_tree',\n",
      "                   'kd_tree', 'kd_tree', 'kd_tree', 'kd_tree', 'kd_tree',\n",
      "                   'kd_tree', 'kd_tree', 'kd_tree', 'kd_tree', 'kd_tree',\n",
      "                   'kd_tree', 'kd_tree', 'kd_tree', 'kd_tree', 'kd_tree',\n",
      "                   'kd_tree', 'kd_tree', 'kd_tree', 'kd_tree', 'kd_tree',\n",
      "                   'kd_tree', 'kd_tree'],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_KNN__leaf_size': masked_array(data=[8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "                   16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "                   16, 16, 16, 16, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,\n",
      "                   32, 32, 32, 32, 32, 32, 32, 32, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "                   8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 16, 16, 16, 16, 16, 16,\n",
      "                   16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 32, 32,\n",
      "                   32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,\n",
      "                   32, 32],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_KNN__metric': masked_array(data=['euclidean', 'euclidean', 'euclidean', 'euclidean',\n",
      "                   'euclidean', 'euclidean', 'minkowski', 'minkowski',\n",
      "                   'minkowski', 'minkowski', 'minkowski', 'minkowski',\n",
      "                   'manhattan', 'manhattan', 'manhattan', 'manhattan',\n",
      "                   'manhattan', 'manhattan', 'euclidean', 'euclidean',\n",
      "                   'euclidean', 'euclidean', 'euclidean', 'euclidean',\n",
      "                   'minkowski', 'minkowski', 'minkowski', 'minkowski',\n",
      "                   'minkowski', 'minkowski', 'manhattan', 'manhattan',\n",
      "                   'manhattan', 'manhattan', 'manhattan', 'manhattan',\n",
      "                   'euclidean', 'euclidean', 'euclidean', 'euclidean',\n",
      "                   'euclidean', 'euclidean', 'minkowski', 'minkowski',\n",
      "                   'minkowski', 'minkowski', 'minkowski', 'minkowski',\n",
      "                   'manhattan', 'manhattan', 'manhattan', 'manhattan',\n",
      "                   'manhattan', 'manhattan', 'euclidean', 'euclidean',\n",
      "                   'euclidean', 'euclidean', 'euclidean', 'euclidean',\n",
      "                   'minkowski', 'minkowski', 'minkowski', 'minkowski',\n",
      "                   'minkowski', 'minkowski', 'manhattan', 'manhattan',\n",
      "                   'manhattan', 'manhattan', 'manhattan', 'manhattan',\n",
      "                   'euclidean', 'euclidean', 'euclidean', 'euclidean',\n",
      "                   'euclidean', 'euclidean', 'minkowski', 'minkowski',\n",
      "                   'minkowski', 'minkowski', 'minkowski', 'minkowski',\n",
      "                   'manhattan', 'manhattan', 'manhattan', 'manhattan',\n",
      "                   'manhattan', 'manhattan', 'euclidean', 'euclidean',\n",
      "                   'euclidean', 'euclidean', 'euclidean', 'euclidean',\n",
      "                   'minkowski', 'minkowski', 'minkowski', 'minkowski',\n",
      "                   'minkowski', 'minkowski', 'manhattan', 'manhattan',\n",
      "                   'manhattan', 'manhattan', 'manhattan', 'manhattan'],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_KNN__n_neighbors': masked_array(data=[3, 3, 5, 5, 7, 7, 3, 3, 5, 5, 7, 7, 3, 3, 5, 5, 7, 7,\n",
      "                   3, 3, 5, 5, 7, 7, 3, 3, 5, 5, 7, 7, 3, 3, 5, 5, 7, 7,\n",
      "                   3, 3, 5, 5, 7, 7, 3, 3, 5, 5, 7, 7, 3, 3, 5, 5, 7, 7,\n",
      "                   3, 3, 5, 5, 7, 7, 3, 3, 5, 5, 7, 7, 3, 3, 5, 5, 7, 7,\n",
      "                   3, 3, 5, 5, 7, 7, 3, 3, 5, 5, 7, 7, 3, 3, 5, 5, 7, 7,\n",
      "                   3, 3, 5, 5, 7, 7, 3, 3, 5, 5, 7, 7, 3, 3, 5, 5, 7, 7],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_KNN__p': masked_array(data=[1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2,\n",
      "                   1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2,\n",
      "                   1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2,\n",
      "                   1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2,\n",
      "                   1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2,\n",
      "                   1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'KNN__algorithm': 'ball_tree', 'KNN__leaf_size': 8, 'KNN__metric': 'euclidean', 'KNN__n_neighbors': 3, 'KNN__p': 1}, {'KNN__algorithm': 'ball_tree', 'KNN__leaf_size': 8, 'KNN__metric': 'euclidean', 'KNN__n_neighbors': 3, 'KNN__p': 2}, {'KNN__algorithm': 'ball_tree', 'KNN__leaf_size': 8, 'KNN__metric': 'euclidean', 'KNN__n_neighbors': 5, 'KNN__p': 1}, {'KNN__algorithm': 'ball_tree', 'KNN__leaf_size': 8, 'KNN__metric': 'euclidean', 'KNN__n_neighbors': 5, 'KNN__p': 2}, {'KNN__algorithm': 'ball_tree', 'KNN__leaf_size': 8, 'KNN__metric': 'euclidean', 'KNN__n_neighbors': 7, 'KNN__p': 1}, {'KNN__algorithm': 'ball_tree', 'KNN__leaf_size': 8, 'KNN__metric': 'euclidean', 'KNN__n_neighbors': 7, 'KNN__p': 2}, {'KNN__algorithm': 'ball_tree', 'KNN__leaf_size': 8, 'KNN__metric': 'minkowski', 'KNN__n_neighbors': 3, 'KNN__p': 1}, {'KNN__algorithm': 'ball_tree', 'KNN__leaf_size': 8, 'KNN__metric': 'minkowski', 'KNN__n_neighbors': 3, 'KNN__p': 2}, {'KNN__algorithm': 'ball_tree', 'KNN__leaf_size': 8, 'KNN__metric': 'minkowski', 'KNN__n_neighbors': 5, 'KNN__p': 1}, {'KNN__algorithm': 'ball_tree', 'KNN__leaf_size': 8, 'KNN__metric': 'minkowski', 'KNN__n_neighbors': 5, 'KNN__p': 2}, {'KNN__algorithm': 'ball_tree', 'KNN__leaf_size': 8, 'KNN__metric': 'minkowski', 'KNN__n_neighbors': 7, 'KNN__p': 1}, {'KNN__algorithm': 'ball_tree', 'KNN__leaf_size': 8, 'KNN__metric': 'minkowski', 'KNN__n_neighbors': 7, 'KNN__p': 2}, {'KNN__algorithm': 'ball_tree', 'KNN__leaf_size': 8, 'KNN__metric': 'manhattan', 'KNN__n_neighbors': 3, 'KNN__p': 1}, {'KNN__algorithm': 'ball_tree', 'KNN__leaf_size': 8, 'KNN__metric': 'manhattan', 'KNN__n_neighbors': 3, 'KNN__p': 2}, {'KNN__algorithm': 'ball_tree', 'KNN__leaf_size': 8, 'KNN__metric': 'manhattan', 'KNN__n_neighbors': 5, 'KNN__p': 1}, {'KNN__algorithm': 'ball_tree', 'KNN__leaf_size': 8, 'KNN__metric': 'manhattan', 'KNN__n_neighbors': 5, 'KNN__p': 2}, {'KNN__algorithm': 'ball_tree', 'KNN__leaf_size': 8, 'KNN__metric': 'manhattan', 'KNN__n_neighbors': 7, 'KNN__p': 1}, {'KNN__algorithm': 'ball_tree', 'KNN__leaf_size': 8, 'KNN__metric': 'manhattan', 'KNN__n_neighbors': 7, 'KNN__p': 2}, {'KNN__algorithm': 'ball_tree', 'KNN__leaf_size': 16, 'KNN__metric': 'euclidean', 'KNN__n_neighbors': 3, 'KNN__p': 1}, {'KNN__algorithm': 'ball_tree', 'KNN__leaf_size': 16, 'KNN__metric': 'euclidean', 'KNN__n_neighbors': 3, 'KNN__p': 2}, {'KNN__algorithm': 'ball_tree', 'KNN__leaf_size': 16, 'KNN__metric': 'euclidean', 'KNN__n_neighbors': 5, 'KNN__p': 1}, {'KNN__algorithm': 'ball_tree', 'KNN__leaf_size': 16, 'KNN__metric': 'euclidean', 'KNN__n_neighbors': 5, 'KNN__p': 2}, {'KNN__algorithm': 'ball_tree', 'KNN__leaf_size': 16, 'KNN__metric': 'euclidean', 'KNN__n_neighbors': 7, 'KNN__p': 1}, {'KNN__algorithm': 'ball_tree', 'KNN__leaf_size': 16, 'KNN__metric': 'euclidean', 'KNN__n_neighbors': 7, 'KNN__p': 2}, {'KNN__algorithm': 'ball_tree', 'KNN__leaf_size': 16, 'KNN__metric': 'minkowski', 'KNN__n_neighbors': 3, 'KNN__p': 1}, {'KNN__algorithm': 'ball_tree', 'KNN__leaf_size': 16, 'KNN__metric': 'minkowski', 'KNN__n_neighbors': 3, 'KNN__p': 2}, {'KNN__algorithm': 'ball_tree', 'KNN__leaf_size': 16, 'KNN__metric': 'minkowski', 'KNN__n_neighbors': 5, 'KNN__p': 1}, {'KNN__algorithm': 'ball_tree', 'KNN__leaf_size': 16, 'KNN__metric': 'minkowski', 'KNN__n_neighbors': 5, 'KNN__p': 2}, {'KNN__algorithm': 'ball_tree', 'KNN__leaf_size': 16, 'KNN__metric': 'minkowski', 'KNN__n_neighbors': 7, 'KNN__p': 1}, {'KNN__algorithm': 'ball_tree', 'KNN__leaf_size': 16, 'KNN__metric': 'minkowski', 'KNN__n_neighbors': 7, 'KNN__p': 2}, {'KNN__algorithm': 'ball_tree', 'KNN__leaf_size': 16, 'KNN__metric': 'manhattan', 'KNN__n_neighbors': 3, 'KNN__p': 1}, {'KNN__algorithm': 'ball_tree', 'KNN__leaf_size': 16, 'KNN__metric': 'manhattan', 'KNN__n_neighbors': 3, 'KNN__p': 2}, {'KNN__algorithm': 'ball_tree', 'KNN__leaf_size': 16, 'KNN__metric': 'manhattan', 'KNN__n_neighbors': 5, 'KNN__p': 1}, {'KNN__algorithm': 'ball_tree', 'KNN__leaf_size': 16, 'KNN__metric': 'manhattan', 'KNN__n_neighbors': 5, 'KNN__p': 2}, {'KNN__algorithm': 'ball_tree', 'KNN__leaf_size': 16, 'KNN__metric': 'manhattan', 'KNN__n_neighbors': 7, 'KNN__p': 1}, {'KNN__algorithm': 'ball_tree', 'KNN__leaf_size': 16, 'KNN__metric': 'manhattan', 'KNN__n_neighbors': 7, 'KNN__p': 2}, {'KNN__algorithm': 'ball_tree', 'KNN__leaf_size': 32, 'KNN__metric': 'euclidean', 'KNN__n_neighbors': 3, 'KNN__p': 1}, {'KNN__algorithm': 'ball_tree', 'KNN__leaf_size': 32, 'KNN__metric': 'euclidean', 'KNN__n_neighbors': 3, 'KNN__p': 2}, {'KNN__algorithm': 'ball_tree', 'KNN__leaf_size': 32, 'KNN__metric': 'euclidean', 'KNN__n_neighbors': 5, 'KNN__p': 1}, {'KNN__algorithm': 'ball_tree', 'KNN__leaf_size': 32, 'KNN__metric': 'euclidean', 'KNN__n_neighbors': 5, 'KNN__p': 2}, {'KNN__algorithm': 'ball_tree', 'KNN__leaf_size': 32, 'KNN__metric': 'euclidean', 'KNN__n_neighbors': 7, 'KNN__p': 1}, {'KNN__algorithm': 'ball_tree', 'KNN__leaf_size': 32, 'KNN__metric': 'euclidean', 'KNN__n_neighbors': 7, 'KNN__p': 2}, {'KNN__algorithm': 'ball_tree', 'KNN__leaf_size': 32, 'KNN__metric': 'minkowski', 'KNN__n_neighbors': 3, 'KNN__p': 1}, {'KNN__algorithm': 'ball_tree', 'KNN__leaf_size': 32, 'KNN__metric': 'minkowski', 'KNN__n_neighbors': 3, 'KNN__p': 2}, {'KNN__algorithm': 'ball_tree', 'KNN__leaf_size': 32, 'KNN__metric': 'minkowski', 'KNN__n_neighbors': 5, 'KNN__p': 1}, {'KNN__algorithm': 'ball_tree', 'KNN__leaf_size': 32, 'KNN__metric': 'minkowski', 'KNN__n_neighbors': 5, 'KNN__p': 2}, {'KNN__algorithm': 'ball_tree', 'KNN__leaf_size': 32, 'KNN__metric': 'minkowski', 'KNN__n_neighbors': 7, 'KNN__p': 1}, {'KNN__algorithm': 'ball_tree', 'KNN__leaf_size': 32, 'KNN__metric': 'minkowski', 'KNN__n_neighbors': 7, 'KNN__p': 2}, {'KNN__algorithm': 'ball_tree', 'KNN__leaf_size': 32, 'KNN__metric': 'manhattan', 'KNN__n_neighbors': 3, 'KNN__p': 1}, {'KNN__algorithm': 'ball_tree', 'KNN__leaf_size': 32, 'KNN__metric': 'manhattan', 'KNN__n_neighbors': 3, 'KNN__p': 2}, {'KNN__algorithm': 'ball_tree', 'KNN__leaf_size': 32, 'KNN__metric': 'manhattan', 'KNN__n_neighbors': 5, 'KNN__p': 1}, {'KNN__algorithm': 'ball_tree', 'KNN__leaf_size': 32, 'KNN__metric': 'manhattan', 'KNN__n_neighbors': 5, 'KNN__p': 2}, {'KNN__algorithm': 'ball_tree', 'KNN__leaf_size': 32, 'KNN__metric': 'manhattan', 'KNN__n_neighbors': 7, 'KNN__p': 1}, {'KNN__algorithm': 'ball_tree', 'KNN__leaf_size': 32, 'KNN__metric': 'manhattan', 'KNN__n_neighbors': 7, 'KNN__p': 2}, {'KNN__algorithm': 'kd_tree', 'KNN__leaf_size': 8, 'KNN__metric': 'euclidean', 'KNN__n_neighbors': 3, 'KNN__p': 1}, {'KNN__algorithm': 'kd_tree', 'KNN__leaf_size': 8, 'KNN__metric': 'euclidean', 'KNN__n_neighbors': 3, 'KNN__p': 2}, {'KNN__algorithm': 'kd_tree', 'KNN__leaf_size': 8, 'KNN__metric': 'euclidean', 'KNN__n_neighbors': 5, 'KNN__p': 1}, {'KNN__algorithm': 'kd_tree', 'KNN__leaf_size': 8, 'KNN__metric': 'euclidean', 'KNN__n_neighbors': 5, 'KNN__p': 2}, {'KNN__algorithm': 'kd_tree', 'KNN__leaf_size': 8, 'KNN__metric': 'euclidean', 'KNN__n_neighbors': 7, 'KNN__p': 1}, {'KNN__algorithm': 'kd_tree', 'KNN__leaf_size': 8, 'KNN__metric': 'euclidean', 'KNN__n_neighbors': 7, 'KNN__p': 2}, {'KNN__algorithm': 'kd_tree', 'KNN__leaf_size': 8, 'KNN__metric': 'minkowski', 'KNN__n_neighbors': 3, 'KNN__p': 1}, {'KNN__algorithm': 'kd_tree', 'KNN__leaf_size': 8, 'KNN__metric': 'minkowski', 'KNN__n_neighbors': 3, 'KNN__p': 2}, {'KNN__algorithm': 'kd_tree', 'KNN__leaf_size': 8, 'KNN__metric': 'minkowski', 'KNN__n_neighbors': 5, 'KNN__p': 1}, {'KNN__algorithm': 'kd_tree', 'KNN__leaf_size': 8, 'KNN__metric': 'minkowski', 'KNN__n_neighbors': 5, 'KNN__p': 2}, {'KNN__algorithm': 'kd_tree', 'KNN__leaf_size': 8, 'KNN__metric': 'minkowski', 'KNN__n_neighbors': 7, 'KNN__p': 1}, {'KNN__algorithm': 'kd_tree', 'KNN__leaf_size': 8, 'KNN__metric': 'minkowski', 'KNN__n_neighbors': 7, 'KNN__p': 2}, {'KNN__algorithm': 'kd_tree', 'KNN__leaf_size': 8, 'KNN__metric': 'manhattan', 'KNN__n_neighbors': 3, 'KNN__p': 1}, {'KNN__algorithm': 'kd_tree', 'KNN__leaf_size': 8, 'KNN__metric': 'manhattan', 'KNN__n_neighbors': 3, 'KNN__p': 2}, {'KNN__algorithm': 'kd_tree', 'KNN__leaf_size': 8, 'KNN__metric': 'manhattan', 'KNN__n_neighbors': 5, 'KNN__p': 1}, {'KNN__algorithm': 'kd_tree', 'KNN__leaf_size': 8, 'KNN__metric': 'manhattan', 'KNN__n_neighbors': 5, 'KNN__p': 2}, {'KNN__algorithm': 'kd_tree', 'KNN__leaf_size': 8, 'KNN__metric': 'manhattan', 'KNN__n_neighbors': 7, 'KNN__p': 1}, {'KNN__algorithm': 'kd_tree', 'KNN__leaf_size': 8, 'KNN__metric': 'manhattan', 'KNN__n_neighbors': 7, 'KNN__p': 2}, {'KNN__algorithm': 'kd_tree', 'KNN__leaf_size': 16, 'KNN__metric': 'euclidean', 'KNN__n_neighbors': 3, 'KNN__p': 1}, {'KNN__algorithm': 'kd_tree', 'KNN__leaf_size': 16, 'KNN__metric': 'euclidean', 'KNN__n_neighbors': 3, 'KNN__p': 2}, {'KNN__algorithm': 'kd_tree', 'KNN__leaf_size': 16, 'KNN__metric': 'euclidean', 'KNN__n_neighbors': 5, 'KNN__p': 1}, {'KNN__algorithm': 'kd_tree', 'KNN__leaf_size': 16, 'KNN__metric': 'euclidean', 'KNN__n_neighbors': 5, 'KNN__p': 2}, {'KNN__algorithm': 'kd_tree', 'KNN__leaf_size': 16, 'KNN__metric': 'euclidean', 'KNN__n_neighbors': 7, 'KNN__p': 1}, {'KNN__algorithm': 'kd_tree', 'KNN__leaf_size': 16, 'KNN__metric': 'euclidean', 'KNN__n_neighbors': 7, 'KNN__p': 2}, {'KNN__algorithm': 'kd_tree', 'KNN__leaf_size': 16, 'KNN__metric': 'minkowski', 'KNN__n_neighbors': 3, 'KNN__p': 1}, {'KNN__algorithm': 'kd_tree', 'KNN__leaf_size': 16, 'KNN__metric': 'minkowski', 'KNN__n_neighbors': 3, 'KNN__p': 2}, {'KNN__algorithm': 'kd_tree', 'KNN__leaf_size': 16, 'KNN__metric': 'minkowski', 'KNN__n_neighbors': 5, 'KNN__p': 1}, {'KNN__algorithm': 'kd_tree', 'KNN__leaf_size': 16, 'KNN__metric': 'minkowski', 'KNN__n_neighbors': 5, 'KNN__p': 2}, {'KNN__algorithm': 'kd_tree', 'KNN__leaf_size': 16, 'KNN__metric': 'minkowski', 'KNN__n_neighbors': 7, 'KNN__p': 1}, {'KNN__algorithm': 'kd_tree', 'KNN__leaf_size': 16, 'KNN__metric': 'minkowski', 'KNN__n_neighbors': 7, 'KNN__p': 2}, {'KNN__algorithm': 'kd_tree', 'KNN__leaf_size': 16, 'KNN__metric': 'manhattan', 'KNN__n_neighbors': 3, 'KNN__p': 1}, {'KNN__algorithm': 'kd_tree', 'KNN__leaf_size': 16, 'KNN__metric': 'manhattan', 'KNN__n_neighbors': 3, 'KNN__p': 2}, {'KNN__algorithm': 'kd_tree', 'KNN__leaf_size': 16, 'KNN__metric': 'manhattan', 'KNN__n_neighbors': 5, 'KNN__p': 1}, {'KNN__algorithm': 'kd_tree', 'KNN__leaf_size': 16, 'KNN__metric': 'manhattan', 'KNN__n_neighbors': 5, 'KNN__p': 2}, {'KNN__algorithm': 'kd_tree', 'KNN__leaf_size': 16, 'KNN__metric': 'manhattan', 'KNN__n_neighbors': 7, 'KNN__p': 1}, {'KNN__algorithm': 'kd_tree', 'KNN__leaf_size': 16, 'KNN__metric': 'manhattan', 'KNN__n_neighbors': 7, 'KNN__p': 2}, {'KNN__algorithm': 'kd_tree', 'KNN__leaf_size': 32, 'KNN__metric': 'euclidean', 'KNN__n_neighbors': 3, 'KNN__p': 1}, {'KNN__algorithm': 'kd_tree', 'KNN__leaf_size': 32, 'KNN__metric': 'euclidean', 'KNN__n_neighbors': 3, 'KNN__p': 2}, {'KNN__algorithm': 'kd_tree', 'KNN__leaf_size': 32, 'KNN__metric': 'euclidean', 'KNN__n_neighbors': 5, 'KNN__p': 1}, {'KNN__algorithm': 'kd_tree', 'KNN__leaf_size': 32, 'KNN__metric': 'euclidean', 'KNN__n_neighbors': 5, 'KNN__p': 2}, {'KNN__algorithm': 'kd_tree', 'KNN__leaf_size': 32, 'KNN__metric': 'euclidean', 'KNN__n_neighbors': 7, 'KNN__p': 1}, {'KNN__algorithm': 'kd_tree', 'KNN__leaf_size': 32, 'KNN__metric': 'euclidean', 'KNN__n_neighbors': 7, 'KNN__p': 2}, {'KNN__algorithm': 'kd_tree', 'KNN__leaf_size': 32, 'KNN__metric': 'minkowski', 'KNN__n_neighbors': 3, 'KNN__p': 1}, {'KNN__algorithm': 'kd_tree', 'KNN__leaf_size': 32, 'KNN__metric': 'minkowski', 'KNN__n_neighbors': 3, 'KNN__p': 2}, {'KNN__algorithm': 'kd_tree', 'KNN__leaf_size': 32, 'KNN__metric': 'minkowski', 'KNN__n_neighbors': 5, 'KNN__p': 1}, {'KNN__algorithm': 'kd_tree', 'KNN__leaf_size': 32, 'KNN__metric': 'minkowski', 'KNN__n_neighbors': 5, 'KNN__p': 2}, {'KNN__algorithm': 'kd_tree', 'KNN__leaf_size': 32, 'KNN__metric': 'minkowski', 'KNN__n_neighbors': 7, 'KNN__p': 1}, {'KNN__algorithm': 'kd_tree', 'KNN__leaf_size': 32, 'KNN__metric': 'minkowski', 'KNN__n_neighbors': 7, 'KNN__p': 2}, {'KNN__algorithm': 'kd_tree', 'KNN__leaf_size': 32, 'KNN__metric': 'manhattan', 'KNN__n_neighbors': 3, 'KNN__p': 1}, {'KNN__algorithm': 'kd_tree', 'KNN__leaf_size': 32, 'KNN__metric': 'manhattan', 'KNN__n_neighbors': 3, 'KNN__p': 2}, {'KNN__algorithm': 'kd_tree', 'KNN__leaf_size': 32, 'KNN__metric': 'manhattan', 'KNN__n_neighbors': 5, 'KNN__p': 1}, {'KNN__algorithm': 'kd_tree', 'KNN__leaf_size': 32, 'KNN__metric': 'manhattan', 'KNN__n_neighbors': 5, 'KNN__p': 2}, {'KNN__algorithm': 'kd_tree', 'KNN__leaf_size': 32, 'KNN__metric': 'manhattan', 'KNN__n_neighbors': 7, 'KNN__p': 1}, {'KNN__algorithm': 'kd_tree', 'KNN__leaf_size': 32, 'KNN__metric': 'manhattan', 'KNN__n_neighbors': 7, 'KNN__p': 2}], 'split0_test_score': array([0.3613808 , 0.3613808 , 0.37186007, 0.37186007, 0.37232239,\n",
      "       0.37232239, 0.36785329, 0.3613808 , 0.37740792, 0.37186007,\n",
      "       0.3792572 , 0.37232239, 0.36785329, 0.36785329, 0.37740792,\n",
      "       0.37740792, 0.3792572 , 0.3792572 , 0.3613808 , 0.3613808 ,\n",
      "       0.37186007, 0.37186007, 0.37232239, 0.37232239, 0.36785329,\n",
      "       0.3613808 , 0.37740792, 0.37186007, 0.3792572 , 0.37232239,\n",
      "       0.36785329, 0.36785329, 0.37740792, 0.37740792, 0.3792572 ,\n",
      "       0.3792572 , 0.3613808 , 0.3613808 , 0.37186007, 0.37186007,\n",
      "       0.37232239, 0.37232239, 0.36785329, 0.3613808 , 0.37740792,\n",
      "       0.37186007, 0.3792572 , 0.37232239, 0.36785329, 0.36785329,\n",
      "       0.37740792, 0.37740792, 0.3792572 , 0.3792572 , 0.3613808 ,\n",
      "       0.3613808 , 0.37186007, 0.37186007, 0.37232239, 0.37232239,\n",
      "       0.36785329, 0.3613808 , 0.37740792, 0.37186007, 0.3792572 ,\n",
      "       0.37232239, 0.36785329, 0.36785329, 0.37740792, 0.37740792,\n",
      "       0.3792572 , 0.3792572 , 0.3613808 , 0.3613808 , 0.37186007,\n",
      "       0.37186007, 0.37232239, 0.37232239, 0.36785329, 0.3613808 ,\n",
      "       0.37740792, 0.37186007, 0.3792572 , 0.37232239, 0.36785329,\n",
      "       0.36785329, 0.37740792, 0.37740792, 0.3792572 , 0.3792572 ,\n",
      "       0.3613808 , 0.3613808 , 0.37186007, 0.37186007, 0.37232239,\n",
      "       0.37232239, 0.36785329, 0.3613808 , 0.37740792, 0.37186007,\n",
      "       0.3792572 , 0.37232239, 0.36785329, 0.36785329, 0.37740792,\n",
      "       0.37740792, 0.3792572 , 0.3792572 ]), 'split1_test_score': array([0.37679149, 0.37679149, 0.3792572 , 0.3792572 , 0.3791031 ,\n",
      "       0.3791031 , 0.37971953, 0.37679149, 0.38002774, 0.3792572 ,\n",
      "       0.39081523, 0.3791031 , 0.37971953, 0.37971953, 0.38002774,\n",
      "       0.38002774, 0.39081523, 0.39081523, 0.37679149, 0.37679149,\n",
      "       0.3792572 , 0.3792572 , 0.3791031 , 0.3791031 , 0.37971953,\n",
      "       0.37679149, 0.38002774, 0.3792572 , 0.39081523, 0.3791031 ,\n",
      "       0.37971953, 0.37971953, 0.38002774, 0.38002774, 0.39081523,\n",
      "       0.39081523, 0.37679149, 0.37679149, 0.3792572 , 0.3792572 ,\n",
      "       0.3791031 , 0.3791031 , 0.37971953, 0.37679149, 0.38002774,\n",
      "       0.3792572 , 0.39081523, 0.3791031 , 0.37971953, 0.37971953,\n",
      "       0.38002774, 0.38002774, 0.39081523, 0.39081523, 0.37679149,\n",
      "       0.37679149, 0.3792572 , 0.3792572 , 0.3791031 , 0.3791031 ,\n",
      "       0.37971953, 0.37679149, 0.38002774, 0.3792572 , 0.39081523,\n",
      "       0.3791031 , 0.37971953, 0.37971953, 0.38002774, 0.38002774,\n",
      "       0.39081523, 0.39081523, 0.37679149, 0.37679149, 0.3792572 ,\n",
      "       0.3792572 , 0.3791031 , 0.3791031 , 0.37971953, 0.37679149,\n",
      "       0.38002774, 0.3792572 , 0.39081523, 0.3791031 , 0.37971953,\n",
      "       0.37971953, 0.38002774, 0.38002774, 0.39081523, 0.39081523,\n",
      "       0.37679149, 0.37679149, 0.3792572 , 0.3792572 , 0.3791031 ,\n",
      "       0.3791031 , 0.37971953, 0.37679149, 0.38002774, 0.3792572 ,\n",
      "       0.39081523, 0.3791031 , 0.37971953, 0.37971953, 0.38002774,\n",
      "       0.38002774, 0.39081523, 0.39081523]), 'split2_test_score': array([0.37607891, 0.37607891, 0.3810111 , 0.3810111 , 0.37268804,\n",
      "       0.37268804, 0.37808261, 0.37607891, 0.38717633, 0.3810111 ,\n",
      "       0.38424784, 0.37268804, 0.37808261, 0.37808261, 0.38717633,\n",
      "       0.38717633, 0.38424784, 0.38424784, 0.37607891, 0.37607891,\n",
      "       0.3810111 , 0.3810111 , 0.37268804, 0.37268804, 0.37808261,\n",
      "       0.37607891, 0.38717633, 0.3810111 , 0.38424784, 0.37268804,\n",
      "       0.37808261, 0.37808261, 0.38717633, 0.38717633, 0.38424784,\n",
      "       0.38424784, 0.37607891, 0.37607891, 0.3810111 , 0.3810111 ,\n",
      "       0.37268804, 0.37268804, 0.37808261, 0.37607891, 0.38717633,\n",
      "       0.3810111 , 0.38424784, 0.37268804, 0.37808261, 0.37808261,\n",
      "       0.38717633, 0.38717633, 0.38424784, 0.38424784, 0.37607891,\n",
      "       0.37607891, 0.3810111 , 0.3810111 , 0.37268804, 0.37268804,\n",
      "       0.37808261, 0.37607891, 0.38717633, 0.3810111 , 0.38424784,\n",
      "       0.37268804, 0.37808261, 0.37808261, 0.38717633, 0.38717633,\n",
      "       0.38424784, 0.38424784, 0.37607891, 0.37607891, 0.3810111 ,\n",
      "       0.3810111 , 0.37268804, 0.37268804, 0.37808261, 0.37607891,\n",
      "       0.38717633, 0.3810111 , 0.38424784, 0.37268804, 0.37808261,\n",
      "       0.37808261, 0.38717633, 0.38717633, 0.38424784, 0.38424784,\n",
      "       0.37607891, 0.37607891, 0.3810111 , 0.3810111 , 0.37268804,\n",
      "       0.37268804, 0.37808261, 0.37607891, 0.38717633, 0.3810111 ,\n",
      "       0.38424784, 0.37268804, 0.37808261, 0.37808261, 0.38717633,\n",
      "       0.38717633, 0.38424784, 0.38424784]), 'mean_test_score': array([0.37141707, 0.37141707, 0.37737612, 0.37737612, 0.37470451,\n",
      "       0.37470451, 0.37521848, 0.37141707, 0.38153733, 0.37737612,\n",
      "       0.38477342, 0.37470451, 0.37521848, 0.37521848, 0.38153733,\n",
      "       0.38153733, 0.38477342, 0.38477342, 0.37141707, 0.37141707,\n",
      "       0.37737612, 0.37737612, 0.37470451, 0.37470451, 0.37521848,\n",
      "       0.37141707, 0.38153733, 0.37737612, 0.38477342, 0.37470451,\n",
      "       0.37521848, 0.37521848, 0.38153733, 0.38153733, 0.38477342,\n",
      "       0.38477342, 0.37141707, 0.37141707, 0.37737612, 0.37737612,\n",
      "       0.37470451, 0.37470451, 0.37521848, 0.37141707, 0.38153733,\n",
      "       0.37737612, 0.38477342, 0.37470451, 0.37521848, 0.37521848,\n",
      "       0.38153733, 0.38153733, 0.38477342, 0.38477342, 0.37141707,\n",
      "       0.37141707, 0.37737612, 0.37737612, 0.37470451, 0.37470451,\n",
      "       0.37521848, 0.37141707, 0.38153733, 0.37737612, 0.38477342,\n",
      "       0.37470451, 0.37521848, 0.37521848, 0.38153733, 0.38153733,\n",
      "       0.38477342, 0.38477342, 0.37141707, 0.37141707, 0.37737612,\n",
      "       0.37737612, 0.37470451, 0.37470451, 0.37521848, 0.37141707,\n",
      "       0.38153733, 0.37737612, 0.38477342, 0.37470451, 0.37521848,\n",
      "       0.37521848, 0.38153733, 0.38153733, 0.38477342, 0.38477342,\n",
      "       0.37141707, 0.37141707, 0.37737612, 0.37737612, 0.37470451,\n",
      "       0.37470451, 0.37521848, 0.37141707, 0.38153733, 0.37737612,\n",
      "       0.38477342, 0.37470451, 0.37521848, 0.37521848, 0.38153733,\n",
      "       0.38153733, 0.38477342, 0.38477342]), 'std_test_score': array([0.00710267, 0.00710267, 0.00396562, 0.00396562, 0.00311385,\n",
      "       0.00311385, 0.00525067, 0.00710267, 0.00412832, 0.00396562,\n",
      "       0.00473316, 0.00311385, 0.00525067, 0.00525067, 0.00412832,\n",
      "       0.00412832, 0.00473316, 0.00473316, 0.00710267, 0.00710267,\n",
      "       0.00396562, 0.00396562, 0.00311385, 0.00311385, 0.00525067,\n",
      "       0.00710267, 0.00412832, 0.00396562, 0.00473316, 0.00311385,\n",
      "       0.00525067, 0.00525067, 0.00412832, 0.00412832, 0.00473316,\n",
      "       0.00473316, 0.00710267, 0.00710267, 0.00396562, 0.00396562,\n",
      "       0.00311385, 0.00311385, 0.00525067, 0.00710267, 0.00412832,\n",
      "       0.00396562, 0.00473316, 0.00311385, 0.00525067, 0.00525067,\n",
      "       0.00412832, 0.00412832, 0.00473316, 0.00473316, 0.00710267,\n",
      "       0.00710267, 0.00396562, 0.00396562, 0.00311385, 0.00311385,\n",
      "       0.00525067, 0.00710267, 0.00412832, 0.00396562, 0.00473316,\n",
      "       0.00311385, 0.00525067, 0.00525067, 0.00412832, 0.00412832,\n",
      "       0.00473316, 0.00473316, 0.00710267, 0.00710267, 0.00396562,\n",
      "       0.00396562, 0.00311385, 0.00311385, 0.00525067, 0.00710267,\n",
      "       0.00412832, 0.00396562, 0.00473316, 0.00311385, 0.00525067,\n",
      "       0.00525067, 0.00412832, 0.00412832, 0.00473316, 0.00473316,\n",
      "       0.00710267, 0.00710267, 0.00396562, 0.00396562, 0.00311385,\n",
      "       0.00311385, 0.00525067, 0.00710267, 0.00412832, 0.00396562,\n",
      "       0.00473316, 0.00311385, 0.00525067, 0.00525067, 0.00412832,\n",
      "       0.00412832, 0.00473316, 0.00473316]), 'rank_test_score': array([91, 91, 37, 37, 73, 73, 55, 91, 19, 37,  1, 73, 55, 55, 19, 19,  1,\n",
      "        1, 91, 91, 37, 37, 73, 73, 55, 91, 19, 37,  1, 73, 55, 55, 19, 19,\n",
      "        1,  1, 91, 91, 37, 37, 73, 73, 55, 91, 19, 37,  1, 73, 55, 55, 19,\n",
      "       19,  1,  1, 91, 91, 37, 37, 73, 73, 55, 91, 19, 37,  1, 73, 55, 55,\n",
      "       19, 19,  1,  1, 91, 91, 37, 37, 73, 73, 55, 91, 19, 37,  1, 73, 55,\n",
      "       55, 19, 19,  1,  1, 91, 91, 37, 37, 73, 73, 55, 91, 19, 37,  1, 73,\n",
      "       55, 55, 19, 19,  1,  1])}\n"
     ]
    }
   ],
   "source": [
    "KNN = KNeighborsClassifier()\n",
    "params = {\n",
    "    'KNN__n_neighbors': [3, 5, 7],\n",
    "    'KNN__algorithm': [\"ball_tree\", \"kd_tree\"],\n",
    "    'KNN__leaf_size': [8, 16, 32],\n",
    "    'KNN__metric': [\"euclidean\", \"minkowski\", \"manhattan\"],\n",
    "    'KNN__p': [1, 2],\n",
    "}\n",
    "\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy=\"mean\")\n",
    "pipe = Pipeline(steps=[(\"imputer\", imp), (\"KNN\", KNN)])\n",
    "\n",
    "grid = GridSearchCV(pipe, params, scoring=\"accuracy\", cv=StratifiedKFold(3), verbose=1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(grid.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "437486b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('imputer', SimpleImputer()),\n",
      "                ('KNN',\n",
      "                 KNeighborsClassifier(algorithm='ball_tree', leaf_size=8,\n",
      "                                      n_neighbors=7, p=1))])\n",
      "0.39065595995411406\n"
     ]
    }
   ],
   "source": [
    "print(grid.best_estimator_)\n",
    "print(grid.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683b96e1",
   "metadata": {},
   "source": [
    "# RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "43f929ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6115055963413166"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "822028b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 5, 6, 7], dtype=int64),\n",
       " array([2501, 1459,   62,   12,   35,  105, 1298], dtype=int64))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(rf.predict(X_test), return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9a65b102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_examples, train_score, valid_score = learning_curve(rf, X_train, y_train, cv=StratifiedKFold(3))\n",
    "plt.plot(training_examples, np.mean(train_score, axis=1), label=\"train_score\")\n",
    "plt.plot(training_examples, np.mean(valid_score, axis=1), label=\"valid_score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f8e06df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 405 candidates, totalling 1215 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wilso\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "405 fits failed out of a total of 1215.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "405 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\wilso\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\wilso\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 382, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\wilso\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 476, in fit\n",
      "    trees = Parallel(\n",
      "  File \"C:\\Users\\wilso\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1041, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\wilso\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 859, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\wilso\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 777, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\wilso\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\wilso\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\wilso\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\wilso\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\wilso\\anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 117, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"C:\\Users\\wilso\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 189, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\wilso\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 969, in fit\n",
      "    super().fit(\n",
      "  File \"C:\\Users\\wilso\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 317, in fit\n",
      "    raise ValueError(\n",
      "ValueError: Invalid value for max_features. Allowed string values are 'auto', 'sqrt' or 'log2'.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\wilso\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [0.61361214 0.6154125  0.61829349 0.56400822 0.56211749 0.56265747\n",
      " 0.56796931 0.56706902 0.56778903 0.57580136 0.57796194 0.57859225\n",
      " 0.59488683 0.59182605 0.59551712 0.60947079 0.61190154 0.61559256\n",
      " 0.56346804 0.56283794 0.56346809 0.56256777 0.56490824 0.56607873\n",
      " 0.57544143 0.57616174 0.57634173 0.5937167  0.59497711 0.59380689\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.57886218 0.57976225 0.58012266\n",
      " 0.56238749 0.56184727 0.56103685 0.56706892 0.56814929 0.56823916\n",
      " 0.57427109 0.57562137 0.57517124 0.57832169 0.5808427  0.5796725\n",
      " 0.5803026  0.57922227 0.58111302 0.56445845 0.56193763 0.56184768\n",
      " 0.56436841 0.56769926 0.56535862 0.57544124 0.57562169 0.576522\n",
      " 0.57931265 0.58057302 0.57922237        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.61226137 0.61397187 0.61676271 0.56355794 0.56166687 0.56472835\n",
      " 0.56481822 0.56769916 0.56742911 0.57679159 0.57733177 0.57976259\n",
      " 0.5924562  0.5920961  0.59488695 0.61514243 0.61712271 0.617663\n",
      " 0.56085747 0.56373785 0.56202764 0.56562867 0.56715918 0.56742911\n",
      " 0.57688171 0.5755313  0.57688185 0.59479671 0.59317663 0.59497721\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.61361182 0.61631255 0.61577262\n",
      " 0.56400873 0.56238735 0.56409816 0.5675192  0.56904937 0.56877929\n",
      " 0.57589166 0.57832247 0.57679181 0.59182588 0.59182603 0.59182617\n",
      " 0.60974067 0.61433208 0.61739298 0.56427822 0.56643904 0.56661903\n",
      " 0.57202064 0.57256079 0.57238066 0.57598188 0.57949281 0.57877267\n",
      " 0.59407668 0.58813513 0.59002549        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.57940236 0.58102266 0.58102286 0.55887627 0.56355806 0.56463814\n",
      " 0.56976975 0.56715879 0.56832955 0.57571131 0.57661175 0.57715193\n",
      " 0.57895205 0.58237324 0.57949215 0.58399398 0.58426401 0.58246359\n",
      " 0.5657189  0.56553882 0.56562872 0.5697698  0.56796955 0.57202059\n",
      " 0.57706206 0.57805266 0.5779623  0.58363389 0.58183355 0.58111307\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.61496239 0.6184733  0.61721304\n",
      " 0.56211715 0.56130731 0.56499838 0.56895976 0.5675191  0.56895935\n",
      " 0.57670179 0.58012291 0.5796728  0.59056564 0.59200601 0.5904758\n",
      " 0.61541228 0.62027396 0.62009373 0.56346838 0.56670863 0.5657189\n",
      " 0.57040001 0.57076045 0.57130026 0.57697199 0.58030306 0.57814255\n",
      " 0.59092581 0.59218602 0.59029576        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.61748299 0.61514218 0.61505212 0.56121688 0.56472823 0.56319748\n",
      " 0.56760902 0.56949948 0.56850917 0.57544163 0.57904241 0.5794026\n",
      " 0.59056552 0.59101602 0.59065556 0.61028112 0.61586278 0.61496225\n",
      " 0.56769923 0.56814968 0.5677893  0.56958991 0.56995012 0.57076028\n",
      " 0.57742215 0.57931263 0.57868278 0.59416679 0.5913757  0.59299669\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.57778178 0.58192303 0.58210311\n",
      " 0.56283763 0.56049723 0.56373773 0.56652869 0.56886924 0.56796882\n",
      " 0.57796208 0.57634141 0.57688178 0.57985271 0.57904202 0.58201275\n",
      " 0.58309364 0.5858848  0.58498429 0.56913987 0.566979   0.56769928\n",
      " 0.56832972 0.56986018 0.57121036 0.57706203 0.57706189 0.57967289\n",
      " 0.58246342 0.5830036  0.58444424        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.61946354 0.61820311 0.62144416 0.56175733 0.56355832 0.56346787\n",
      " 0.56751891 0.5676089  0.56868944 0.57742176 0.57733204 0.57877216\n",
      " 0.59002559 0.59218619 0.59317648 0.61568231 0.61847328 0.61946366\n",
      " 0.56580878 0.56670907 0.5659891  0.5686894  0.56995002 0.57265064\n",
      " 0.57733191 0.57922283 0.580213   0.59245629 0.59020562 0.59227633\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean_fit_time': array([0.72794477, 1.44751048, 2.19249137, 0.28718146, 0.57310406,\n",
      "       0.84656199, 0.30198185, 0.5977265 , 0.89306974, 0.33562771,\n",
      "       0.66495848, 1.0157148 , 0.37785912, 0.7583789 , 1.15031258,\n",
      "       0.96818209, 2.06176678, 3.1631705 , 0.42598764, 0.84141421,\n",
      "       1.21516792, 0.42808986, 0.8537244 , 1.27877649, 0.49467397,\n",
      "       1.0037291 , 1.46023075, 0.55903109, 1.10826222, 1.69062392,\n",
      "       0.01826914, 0.03292394, 0.04754011, 0.01895078, 0.03324517,\n",
      "       0.04658294, 0.01926796, 0.03424199, 0.04942576, 0.01860404,\n",
      "       0.03242477, 0.05485733, 0.01828432, 0.0315938 , 0.04587777,\n",
      "       0.35339872, 0.6950895 , 1.05254261, 0.30447006, 0.63392925,\n",
      "       0.89273151, 0.3111736 , 0.61315608, 0.92641234, 0.33846807,\n",
      "       0.66922998, 1.02192426, 0.34200454, 0.68355966, 1.02912935,\n",
      "       0.46845007, 0.92132823, 1.41431626, 0.39879529, 0.79932984,\n",
      "       1.20056454, 0.41629918, 0.85407925, 1.25005293, 0.4518319 ,\n",
      "       0.94217912, 1.38925354, 0.46906447, 0.94037636, 1.39727847,\n",
      "       0.01760832, 0.03160961, 0.04489032, 0.01796492, 0.03229904,\n",
      "       0.04670294, 0.01795594, 0.03357689, 0.04648018, 0.0186162 ,\n",
      "       0.03491211, 0.04853702, 0.01894848, 0.03192854, 0.04686912,\n",
      "       0.56480368, 1.12510022, 1.79416776, 0.29950778, 0.60514649,\n",
      "       0.90746609, 0.31553102, 0.64555923, 0.98636015, 0.36622802,\n",
      "       0.8070058 , 1.07257978, 0.40688467, 0.82167665, 1.24375486,\n",
      "       0.77514068, 1.5625128 , 2.3678894 , 0.41021776, 0.83698734,\n",
      "       1.27002549, 0.43030643, 0.87514361, 1.31093534, 0.49234025,\n",
      "       1.05059663, 1.45405006, 0.57671046, 1.10882282, 1.72095998,\n",
      "       0.01696046, 0.03176999, 0.04543193, 0.01762843, 0.03263474,\n",
      "       0.04486728, 0.01761977, 0.03290153, 0.0455567 , 0.01860054,\n",
      "       0.03297742, 0.04588517, 0.01860579, 0.03257879, 0.04538663,\n",
      "       1.70106236, 3.3545049 , 5.09618274, 0.74207767, 1.44553534,\n",
      "       2.16858149, 0.79372613, 1.5427316 , 2.32107782, 0.88644687,\n",
      "       1.72995615, 2.59912411, 1.05216487, 2.06266618, 3.14107482,\n",
      "       2.35789251, 4.75853173, 7.28106777, 1.01483075, 2.0165019 ,\n",
      "       3.08127642, 1.10014121, 2.19086997, 3.2421658 , 1.27238075,\n",
      "       2.48852738, 3.72007767, 1.45547907, 2.9607563 , 4.40757179,\n",
      "       0.01727454, 0.03257147, 0.04756331, 0.01793742, 0.03358873,\n",
      "       0.04622165, 0.01794934, 0.03158514, 0.04520575, 0.01828869,\n",
      "       0.03430311, 0.04621784, 0.01793361, 0.03189882, 0.04606024,\n",
      "       0.83143886, 1.67759116, 2.54789877, 0.70524804, 1.43455593,\n",
      "       2.14401436, 0.74984392, 1.49143116, 2.24658879, 0.80292741,\n",
      "       1.6255482 , 2.46010637, 0.8476158 , 1.67249314, 2.51781329,\n",
      "       1.20069496, 2.4063189 , 3.6304419 , 0.99851791, 2.02950454,\n",
      "       3.00951687, 1.09433993, 2.11174011, 3.27005561, 1.15790033,\n",
      "       2.29515632, 3.44705335, 1.19028211, 2.44118913, 3.62069305,\n",
      "       0.01760666, 0.03223459, 0.04686276, 0.01962598, 0.03426552,\n",
      "       0.04690011, 0.01831102, 0.03289938, 0.04523706, 0.01829839,\n",
      "       0.03323301, 0.04621339, 0.01861644, 0.03224826, 0.04621037,\n",
      "       1.37477477, 2.8133556 , 4.18113979, 0.716314  , 1.4308424 ,\n",
      "       2.14354014, 0.78875597, 1.51695959, 2.31726615, 0.89846889,\n",
      "       1.74284569, 3.00432118, 1.13544424, 2.06207712, 3.14248951,\n",
      "       1.99944496, 4.09609254, 6.07132769, 1.00622153, 2.04132716,\n",
      "       3.08048614, 1.10716446, 2.17887592, 3.28909302, 1.25032926,\n",
      "       2.50967852, 3.72272229, 1.48105319, 2.93265128, 4.45248135,\n",
      "       0.01793917, 0.03292815, 0.04654185, 0.02026947, 0.03226995,\n",
      "       0.0468843 , 0.01893544, 0.03356457, 0.04751563, 0.01797597,\n",
      "       0.04785975, 0.0538528 , 0.01995858, 0.03475046, 0.0478158 ,\n",
      "       1.70638585, 3.4012026 , 5.12968294, 0.71445084, 1.43262506,\n",
      "       2.20878681, 0.838667  , 1.49267785, 2.30009214, 0.89663307,\n",
      "       1.74456231, 2.65039555, 1.01985089, 2.04374965, 3.09568008,\n",
      "       2.37929002, 4.78703181, 7.15136298, 1.01309411, 2.04483708,\n",
      "       3.08126648, 1.10175753, 2.22152026, 3.26954452, 1.26791453,\n",
      "       2.51016013, 3.7409819 , 1.48762194, 2.96705961, 4.37736845,\n",
      "       0.01795371, 0.04952176, 0.05187591, 0.01992114, 0.03524017,\n",
      "       0.0498662 , 0.0188059 , 0.03245099, 0.04619892, 0.01894935,\n",
      "       0.03260279, 0.04869668, 0.01929275, 0.03445641, 0.0492324 ,\n",
      "       0.83817172, 1.67585182, 2.5445799 , 0.70997421, 1.41541823,\n",
      "       2.14835111, 0.75694251, 1.50804965, 2.25954509, 0.93274196,\n",
      "       1.60487843, 2.42882538, 0.87272811, 1.6883173 , 2.53122473,\n",
      "       1.19980899, 2.38604379, 3.61557007, 1.00098697, 2.05273763,\n",
      "       3.01703095, 1.09585333, 2.15555469, 3.20034003, 1.15860168,\n",
      "       2.29484256, 3.49582982, 1.1909682 , 2.39394355, 3.61559073,\n",
      "       0.01767707, 0.03192814, 0.04585449, 0.01829394, 0.03322395,\n",
      "       0.04788431, 0.01797795, 0.0345784 , 0.04688183, 0.01828257,\n",
      "       0.03291233, 0.04920745, 0.01960564, 0.03354732, 0.04652301,\n",
      "       1.40350223, 2.84308616, 4.17999307, 0.70979253, 1.43261305,\n",
      "       2.14379025, 0.78895195, 1.54211617, 2.29762634, 0.90502238,\n",
      "       1.74576068, 2.64552251, 1.02035085, 2.07091427, 3.0751605 ,\n",
      "       1.99580741, 4.03030578, 6.02801069, 1.02996699, 2.04381235,\n",
      "       3.04329658, 1.10202352, 2.18977753, 3.31353903, 1.24821846,\n",
      "       2.50862908, 3.70433187, 1.48105955, 2.91913978, 4.41897178,\n",
      "       0.01893751, 0.03163354, 0.04589399, 0.01786733, 0.03158379,\n",
      "       0.04720561, 0.01830037, 0.03309433, 0.04722627, 0.01861628,\n",
      "       0.0322597 , 0.04688629, 0.01795141, 0.0319225 , 0.046863  ]), 'std_fit_time': array([6.08839522e-03, 1.18410813e-02, 4.36317688e-02, 4.13909184e-03,\n",
      "       4.08450068e-03, 3.28768468e-03, 3.73171246e-03, 4.54934640e-03,\n",
      "       1.00383297e-02, 4.31601920e-03, 1.63577458e-03, 2.45792674e-02,\n",
      "       2.26269774e-03, 9.44229711e-03, 1.08047822e-02, 2.53106889e-03,\n",
      "       6.60647726e-02, 3.56127700e-02, 1.61622879e-02, 2.67831137e-02,\n",
      "       3.38338018e-03, 1.06108275e-02, 1.29198002e-02, 4.90667237e-03,\n",
      "       2.05810560e-02, 3.61108677e-02, 2.15939974e-02, 5.43302798e-03,\n",
      "       3.02646493e-03, 2.97004863e-02, 4.53283500e-04, 1.69495321e-05,\n",
      "       4.69628294e-04, 5.61957980e-07, 4.43835523e-04, 5.05753528e-04,\n",
      "       9.33204985e-04, 4.70302644e-04, 1.05205515e-03, 4.63342735e-04,\n",
      "       3.90320188e-04, 1.00265990e-02, 9.40212023e-04, 4.78679713e-04,\n",
      "       1.47400196e-06, 4.64653421e-03, 4.12376309e-03, 3.64792477e-03,\n",
      "       1.24711353e-02, 4.54106393e-02, 4.07465609e-03, 4.35792778e-03,\n",
      "       2.76121054e-03, 3.31964205e-03, 1.04340327e-02, 9.74579840e-03,\n",
      "       3.51040819e-02, 1.46956545e-03, 2.06785453e-03, 4.83085110e-03,\n",
      "       8.49798678e-03, 9.81447715e-03, 3.17092140e-02, 1.33927428e-03,\n",
      "       6.03934615e-03, 5.62151145e-03, 1.67663863e-03, 3.53811279e-02,\n",
      "       1.49724081e-02, 4.79989198e-03, 5.57842887e-02, 1.11949584e-01,\n",
      "       2.65142565e-02, 2.15663506e-02, 4.99239976e-03, 4.62180112e-04,\n",
      "       8.96188881e-04, 8.16046959e-04, 3.64530148e-05, 8.39872844e-04,\n",
      "       6.77066574e-04, 3.13538310e-05, 4.70134086e-04, 1.01409892e-03,\n",
      "       4.69853561e-04, 1.77713193e-03, 1.24515350e-03, 8.16340395e-04,\n",
      "       7.96848989e-04, 7.84476167e-04, 2.84715155e-03, 9.07207350e-03,\n",
      "       9.58661927e-02, 1.65787272e-03, 3.39976861e-03, 3.90472782e-03,\n",
      "       2.43752799e-03, 1.17930169e-02, 3.01264702e-02, 1.26914717e-02,\n",
      "       4.85945178e-02, 1.45231034e-02, 4.55053124e-03, 6.46770300e-03,\n",
      "       3.01736610e-02, 1.89905061e-03, 1.24888299e-02, 4.45365825e-02,\n",
      "       9.79831898e-03, 6.25446518e-02, 5.34390046e-02, 2.67928233e-03,\n",
      "       2.54050596e-02, 1.64041254e-02, 3.08560159e-03, 7.29894480e-02,\n",
      "       2.87059844e-02, 2.79472639e-02, 1.05935417e-02, 4.51826597e-02,\n",
      "       3.01039584e-05, 2.56594740e-04, 3.62582239e-04, 4.83214763e-04,\n",
      "       1.01473324e-03, 1.60822124e-05, 9.16949142e-04, 8.30684363e-04,\n",
      "       4.78957561e-04, 4.58695715e-04, 7.50437172e-04, 1.95225205e-05,\n",
      "       9.24610373e-04, 9.39481371e-04, 6.88143271e-04, 2.59888442e-02,\n",
      "       3.50760915e-03, 2.02934020e-02, 2.83409801e-02, 7.76249841e-03,\n",
      "       4.38676897e-03, 3.85842679e-02, 1.67668166e-02, 1.81361635e-02,\n",
      "       3.54941117e-02, 1.11665427e-02, 1.10254907e-02, 3.62126365e-02,\n",
      "       1.37502966e-02, 1.17516486e-01, 7.30800092e-03, 3.48130039e-02,\n",
      "       1.46770372e-01, 2.77921515e-03, 8.50695871e-03, 2.99541795e-02,\n",
      "       1.14877794e-02, 3.56251054e-02, 7.59678305e-03, 4.60878907e-02,\n",
      "       2.12171474e-02, 3.82778059e-02, 8.83561777e-03, 4.78350417e-02,\n",
      "       3.52827397e-02, 4.60914743e-04, 1.23285179e-03, 1.24125142e-03,\n",
      "       1.52972353e-05, 1.86446419e-03, 9.23802729e-04, 8.41366318e-04,\n",
      "       9.43808433e-04, 4.75350858e-04, 4.69809939e-04, 1.96730251e-03,\n",
      "       1.26429246e-03, 1.89248893e-05, 7.43145607e-06, 1.97189270e-03,\n",
      "       1.05209604e-02, 1.81513772e-02, 5.33883733e-02, 7.82795249e-03,\n",
      "       5.99416269e-02, 4.78972934e-02, 6.65801176e-03, 1.13665380e-02,\n",
      "       1.57518942e-02, 7.17451927e-03, 2.01074250e-02, 3.91686103e-02,\n",
      "       1.46937961e-02, 1.08973917e-02, 2.16114791e-02, 6.33512457e-03,\n",
      "       1.37665608e-02, 3.42555882e-02, 6.74388993e-03, 3.53101314e-02,\n",
      "       3.24788040e-02, 1.57788222e-02, 5.68685435e-03, 7.62989322e-02,\n",
      "       3.95766871e-03, 4.38265612e-03, 2.67795104e-02, 1.23668353e-02,\n",
      "       4.02227781e-02, 3.26874161e-02, 4.60275793e-04, 4.52207597e-04,\n",
      "       1.71399948e-05, 1.69028830e-03, 4.78863503e-04, 1.85347339e-05,\n",
      "       4.52497987e-04, 1.79379945e-05, 4.82939338e-04, 4.62185387e-04,\n",
      "       1.88315296e-03, 4.68850040e-04, 4.69854771e-04, 9.38357516e-04,\n",
      "       9.40436806e-04, 5.78947778e-03, 3.39710865e-02, 2.52834674e-02,\n",
      "       1.41598885e-03, 6.85640202e-03, 1.10577528e-02, 3.23585244e-02,\n",
      "       1.13441787e-02, 3.62978363e-02, 4.46510985e-02, 1.40888145e-02,\n",
      "       2.91422513e-01, 3.19354024e-02, 9.18422253e-02, 6.69594498e-02,\n",
      "       4.26016342e-03, 1.92299282e-01, 7.49466247e-02, 5.56934812e-04,\n",
      "       2.88259571e-02, 4.26843981e-02, 2.45171454e-02, 7.80964766e-03,\n",
      "       3.97238375e-02, 3.81545531e-03, 3.06839056e-02, 4.12311740e-02,\n",
      "       1.24456882e-02, 8.81280861e-03, 1.01890721e-01, 1.63687083e-05,\n",
      "       8.15636111e-04, 4.95460208e-04, 1.25420701e-03, 4.80216805e-04,\n",
      "       1.93877946e-05, 8.12243007e-04, 9.47561962e-04, 4.79531346e-04,\n",
      "       3.55269462e-05, 2.11406344e-02, 1.43205575e-03, 1.73083058e-05,\n",
      "       6.42973343e-04, 1.02554543e-03, 8.14509160e-03, 2.88413114e-02,\n",
      "       4.13609958e-02, 2.97440514e-03, 9.44975520e-03, 2.80317617e-02,\n",
      "       8.63471836e-02, 5.81774019e-02, 2.24925712e-02, 2.93610524e-02,\n",
      "       1.91210434e-03, 2.32901297e-02, 1.45786394e-02, 5.23473776e-03,\n",
      "       3.77691215e-02, 1.09702933e-02, 5.56827771e-02, 4.82887151e-02,\n",
      "       8.32620483e-03, 5.18630672e-03, 5.63068429e-02, 3.05073931e-02,\n",
      "       1.57237525e-02, 1.92448632e-02, 2.92288495e-02, 9.89739495e-03,\n",
      "       3.45246474e-02, 4.48184756e-03, 3.96153537e-02, 1.17620843e-02,\n",
      "       1.70079347e-06, 1.55707348e-02, 1.64652993e-03, 1.80499644e-05,\n",
      "       4.69516710e-04, 8.14393406e-04, 2.43447228e-04, 3.91773906e-04,\n",
      "       4.52602004e-04, 2.97360213e-07, 9.23690583e-04, 1.91323947e-03,\n",
      "       4.61338301e-04, 7.08809957e-04, 8.65190163e-04, 5.74619294e-03,\n",
      "       2.33739605e-02, 1.97548358e-02, 2.98454718e-03, 2.27322341e-02,\n",
      "       3.20127595e-02, 3.23326662e-03, 8.31005665e-03, 2.24319824e-02,\n",
      "       7.84597770e-02, 7.11577644e-03, 1.64400130e-02, 2.87257089e-02,\n",
      "       1.16353282e-02, 4.20947040e-02, 1.66896505e-02, 1.50720023e-02,\n",
      "       5.99880078e-02, 2.00711159e-02, 4.46842860e-02, 2.16970689e-02,\n",
      "       4.36089849e-02, 2.57494399e-02, 2.41312068e-02, 1.25572971e-02,\n",
      "       2.20809349e-02, 1.95288289e-02, 5.33383674e-03, 3.81791454e-02,\n",
      "       4.62368090e-02, 8.82113323e-04, 1.84886738e-05, 1.65780462e-05,\n",
      "       4.59729728e-04, 4.57596597e-04, 8.27674309e-04, 1.65627999e-05,\n",
      "       1.69298865e-03, 8.07689856e-04, 4.43665580e-04, 1.12391596e-07,\n",
      "       1.25019093e-03, 9.20826404e-04, 5.13629741e-04, 3.64418740e-04,\n",
      "       5.81805094e-03, 3.78188715e-02, 5.93858427e-02, 3.56508035e-03,\n",
      "       9.52086307e-03, 2.50663642e-02, 3.97319425e-02, 1.33554146e-02,\n",
      "       3.36197240e-03, 1.87593091e-02, 1.53088172e-02, 3.81501351e-02,\n",
      "       8.38681362e-03, 9.44653696e-03, 1.85031375e-02, 1.40700617e-02,\n",
      "       2.91325687e-02, 4.43422507e-02, 6.82274958e-03, 2.87476522e-02,\n",
      "       2.76368847e-02, 4.00625578e-02, 1.17349438e-02, 2.75240864e-02,\n",
      "       1.30070911e-02, 4.01445551e-02, 1.58432558e-02, 3.78351115e-02,\n",
      "       1.99378704e-02, 5.15418244e-02, 1.39208280e-03, 3.96798637e-04,\n",
      "       8.27086945e-04, 1.48804648e-04, 4.71542568e-04, 2.02990073e-03,\n",
      "       4.35467356e-04, 6.05573471e-04, 4.96433718e-04, 9.64970256e-04,\n",
      "       4.62331720e-04, 1.39929233e-03, 4.89903609e-07, 2.25557361e-05,\n",
      "       8.13868546e-04]), 'mean_score_time': array([0.03791134, 0.07447108, 0.11176737, 0.01769837, 0.03355225,\n",
      "       0.04954648, 0.01760372, 0.0339187 , 0.0497152 , 0.01863988,\n",
      "       0.03556752, 0.05226032, 0.01992186, 0.03889354, 0.05784241,\n",
      "       0.03692134, 0.07949289, 0.11736997, 0.01861548, 0.03527625,\n",
      "       0.05368972, 0.02043621, 0.03623533, 0.05427686, 0.01961335,\n",
      "       0.03956827, 0.05750148, 0.02195326, 0.04220955, 0.0624845 ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.02127695, 0.04056009, 0.06053344, 0.01862979, 0.03540238,\n",
      "       0.05531685, 0.0189627 , 0.03556967, 0.05424086, 0.01995945,\n",
      "       0.03990881, 0.05668696, 0.02194111, 0.04157845, 0.06035233,\n",
      "       0.02127369, 0.0409019 , 0.06149062, 0.01896238, 0.03557253,\n",
      "       0.05341633, 0.01861572, 0.03709976, 0.0530657 , 0.01998631,\n",
      "       0.04253292, 0.05386265, 0.02095668, 0.04070266, 0.06151676,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.03092758, 0.06217845, 0.09128912, 0.01859323, 0.03590457,\n",
      "       0.052876  , 0.01960969, 0.03622532, 0.05519358, 0.01930102,\n",
      "       0.04811263, 0.05686116, 0.02196654, 0.0415442 , 0.06316257,\n",
      "       0.03092082, 0.05918829, 0.09125781, 0.01862804, 0.03723478,\n",
      "       0.05284834, 0.01861699, 0.0385627 , 0.05681594, 0.01992551,\n",
      "       0.0408895 , 0.05950451, 0.0216097 , 0.04155731, 0.06483912,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.03925371, 0.0783445 , 0.11602426, 0.01961239, 0.03624916,\n",
      "       0.05368376, 0.01961517, 0.03655601, 0.0545458 , 0.02159731,\n",
      "       0.03855109, 0.05849608, 0.02228626, 0.04289707, 0.06446894,\n",
      "       0.03891985, 0.07837224, 0.11507201, 0.01827637, 0.03623692,\n",
      "       0.05353713, 0.01884842, 0.03693763, 0.0539089 , 0.02061288,\n",
      "       0.03857549, 0.05717039, 0.02160891, 0.04421401, 0.06374598,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.02260335, 0.04189801, 0.06281932, 0.01847569, 0.03557285,\n",
      "       0.05320287, 0.01925675, 0.03755601, 0.05474114, 0.0202752 ,\n",
      "       0.03856309, 0.058707  , 0.02094499, 0.04158298, 0.06239549,\n",
      "       0.02199324, 0.04206491, 0.06147989, 0.01828551, 0.03688979,\n",
      "       0.0555083 , 0.02027853, 0.03723963, 0.05537264, 0.0216074 ,\n",
      "       0.03924115, 0.05835366, 0.0216078 , 0.04189388, 0.06216749,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.03226328, 0.06316392, 0.09552224, 0.01930483, 0.03690275,\n",
      "       0.05284595, 0.0198462 , 0.03689249, 0.05522458, 0.02127862,\n",
      "       0.03954156, 0.0678366 , 0.02558263, 0.04290334, 0.06482855,\n",
      "       0.03360184, 0.06316439, 0.09555403, 0.01861199, 0.03616683,\n",
      "       0.05851094, 0.01903844, 0.03871489, 0.05452164, 0.01994594,\n",
      "       0.03957367, 0.05815482, 0.02227505, 0.04685473, 0.07066083,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.0418756 , 0.08151841, 0.12003024, 0.02113636, 0.03893479,\n",
      "       0.05696535, 0.02193809, 0.03906592, 0.05871503, 0.02161082,\n",
      "       0.04321281, 0.06284388, 0.02356354, 0.04620043, 0.068319  ,\n",
      "       0.04006362, 0.08022881, 0.11652088, 0.01993092, 0.03923551,\n",
      "       0.05782175, 0.02160414, 0.05383317, 0.05885593, 0.02194047,\n",
      "       0.04222067, 0.06417076, 0.02426998, 0.04985754, 0.06714209,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.02260367, 0.04505221, 0.06616282, 0.02063314, 0.039229  ,\n",
      "       0.05787245, 0.02092123, 0.03989132, 0.05983424, 0.02526999,\n",
      "       0.04110297, 0.06182249, 0.02327077, 0.04489088, 0.06672208,\n",
      "       0.02292752, 0.04488031, 0.06690518, 0.0202802 , 0.0387342 ,\n",
      "       0.05716689, 0.02093101, 0.03955499, 0.07230735, 0.0216217 ,\n",
      "       0.04190127, 0.06268191, 0.02596641, 0.04352697, 0.06616012,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.03292497, 0.06413674, 0.096071  , 0.01994578, 0.03871846,\n",
      "       0.05785831, 0.01994793, 0.03955205, 0.06017319, 0.0229276 ,\n",
      "       0.04190445, 0.06215946, 0.02328451, 0.04578257, 0.06714741,\n",
      "       0.03670653, 0.06574957, 0.09793353, 0.02094324, 0.03975471,\n",
      "       0.05783176, 0.02094316, 0.041888  , 0.05882724, 0.02160343,\n",
      "       0.04164322, 0.06133413, 0.02327005, 0.04623445, 0.0687867 ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ]), 'std_score_time': array([1.77578722e-05, 9.41336220e-04, 1.54997641e-03, 1.80366207e-03,\n",
      "       5.05992919e-04, 9.57913573e-04, 4.83061590e-04, 3.30994681e-05,\n",
      "       8.42606302e-04, 9.21463294e-04, 4.65493333e-04, 4.31395842e-04,\n",
      "       1.80877156e-05, 2.83803802e-05, 1.25153985e-06, 2.35293356e-05,\n",
      "       1.69713419e-03, 9.03651546e-04, 4.72213381e-04, 1.24245607e-03,\n",
      "       1.02636096e-03, 3.17926344e-03, 4.70360280e-04, 8.16123996e-04,\n",
      "       9.41168212e-04, 1.22344875e-03, 4.86655650e-04, 3.39985971e-05,\n",
      "       1.22753454e-03, 1.23238395e-03, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 4.70021695e-04, 4.71091074e-04, 4.66159655e-04,\n",
      "       4.52489236e-04, 4.09094803e-04, 3.57040302e-03, 1.61519667e-05,\n",
      "       9.39202193e-04, 9.76965957e-04, 8.28556830e-04, 1.39981375e-03,\n",
      "       6.45382031e-04, 8.15074920e-04, 1.70037390e-03, 1.06948687e-03,\n",
      "       4.72171582e-04, 1.84688497e-05, 4.61539219e-04, 1.58579723e-05,\n",
      "       4.69515983e-04, 4.09551090e-04, 4.69522198e-04, 2.05850951e-03,\n",
      "       1.05886377e-03, 7.98232556e-04, 6.64799151e-03, 8.12084907e-04,\n",
      "       1.42817360e-03, 2.87647468e-04, 1.86187918e-03, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 8.29900664e-04, 9.24255493e-04,\n",
      "       2.50155709e-03, 4.78094562e-04, 4.05233662e-07, 8.03132392e-04,\n",
      "       1.70862724e-03, 9.31373627e-04, 1.23457607e-03, 4.43328661e-04,\n",
      "       1.03426168e-02, 8.29728535e-04, 8.13321930e-04, 4.87049069e-04,\n",
      "       2.62002993e-03, 2.84996368e-06, 1.68035699e-03, 1.90591235e-03,\n",
      "       4.79763009e-04, 3.29251181e-03, 1.61431895e-03, 4.73451849e-04,\n",
      "       9.39605358e-04, 1.39798913e-03, 1.81744544e-05, 3.03756872e-05,\n",
      "       2.04529812e-03, 9.39459892e-04, 4.67324297e-04, 4.32529475e-03,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 4.55354562e-04,\n",
      "       1.46756463e-03, 9.65101760e-04, 1.24498469e-03, 9.19983453e-04,\n",
      "       8.33208114e-04, 4.68841634e-04, 4.88229132e-04, 2.03661321e-03,\n",
      "       9.47962511e-04, 4.63484287e-04, 4.66453925e-04, 4.85139974e-04,\n",
      "       1.66350953e-05, 2.34353422e-03, 1.97156773e-05, 8.18829117e-04,\n",
      "       5.62479539e-04, 4.79347667e-04, 1.24485617e-03, 1.69097815e-03,\n",
      "       6.67461687e-04, 1.57348234e-06, 8.01416320e-04, 4.68279595e-04,\n",
      "       1.24738973e-03, 4.51651177e-04, 4.69740686e-04, 1.69501459e-03,\n",
      "       8.28314801e-04, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       1.69708955e-03, 1.40404440e-03, 2.16527851e-03, 4.37848968e-04,\n",
      "       9.39988570e-04, 9.48582068e-04, 4.88289567e-04, 2.33690231e-03,\n",
      "       1.50482696e-03, 4.61536099e-04, 9.40661589e-04, 2.46798730e-03,\n",
      "       5.61957980e-07, 4.51916629e-04, 3.51749930e-04, 7.51884716e-04,\n",
      "       6.21828198e-04, 4.78543936e-04, 4.70527507e-04, 3.23072570e-05,\n",
      "       1.70737429e-03, 1.25371943e-03, 4.74528699e-04, 4.03625935e-04,\n",
      "       1.69721214e-03, 9.47288238e-04, 1.07140451e-03, 4.68956207e-04,\n",
      "       8.10500031e-04, 4.96123202e-04, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 4.58831626e-04, 1.24434675e-03, 1.56453685e-03,\n",
      "       4.76941508e-04, 8.11960233e-04, 1.85456351e-05, 8.28444241e-04,\n",
      "       7.97488214e-04, 1.22728638e-03, 1.24126901e-03, 9.28942596e-04,\n",
      "       8.24833476e-03, 4.79063967e-04, 3.25039790e-03, 1.61283471e-03,\n",
      "       3.09028308e-03, 9.42625120e-04, 1.64373167e-03, 4.68909417e-04,\n",
      "       5.56062746e-04, 5.17021963e-03, 1.28094029e-04, 3.27409771e-03,\n",
      "       4.44517458e-04, 3.04718055e-05, 1.22656728e-03, 4.89073202e-04,\n",
      "       4.71064286e-04, 4.94665001e-03, 2.29905072e-03, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 8.14578490e-04, 1.03013505e-03,\n",
      "       4.32559306e-04, 1.41185840e-03, 8.13133297e-04, 8.31078424e-04,\n",
      "       2.14953236e-03, 2.23966326e-03, 2.06776988e-04, 4.71258324e-04,\n",
      "       1.69555208e-03, 8.13882049e-04, 8.56070814e-04, 4.77821698e-04,\n",
      "       7.10148227e-04, 2.41080052e-04, 8.13710444e-04, 3.47868245e-03,\n",
      "       8.25245226e-04, 1.23671478e-03, 1.67380484e-05, 1.68212728e-03,\n",
      "       1.58683670e-02, 1.64789392e-03, 8.13031150e-04, 1.88070479e-03,\n",
      "       4.00358264e-03, 4.44570652e-04, 5.09000207e-03, 1.25703786e-03,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 4.65782668e-04,\n",
      "       8.47471471e-04, 8.50540250e-04, 9.24760531e-04, 1.24379406e-03,\n",
      "       8.15166042e-04, 1.74127194e-05, 8.13030715e-04, 2.14489698e-03,\n",
      "       4.69971301e-03, 2.99524173e-04, 1.62877381e-03, 4.70753015e-04,\n",
      "       3.38873243e-05, 1.99814405e-03, 1.68587394e-05, 8.15564976e-04,\n",
      "       1.50931790e-03, 4.68788621e-04, 2.29784639e-04, 4.51589433e-04,\n",
      "       1.87195280e-05, 4.52703680e-04, 1.97660014e-02, 4.54179488e-04,\n",
      "       7.67535787e-04, 8.40530175e-04, 4.23120646e-03, 1.22881664e-03,\n",
      "       4.74679769e-04, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       1.63014314e-05, 4.60824609e-04, 4.48339825e-04, 2.78268518e-06,\n",
      "       2.23439383e-04, 1.74836679e-05, 1.40826266e-06, 4.83059119e-04,\n",
      "       9.39591988e-04, 1.62946391e-03, 2.68832146e-05, 4.81863957e-04,\n",
      "       4.85308825e-04, 8.22215014e-04, 9.34308931e-04, 4.09220509e-03,\n",
      "       1.09661201e-04, 1.98861678e-03, 2.79627038e-06, 1.23882310e-03,\n",
      "       1.39034041e-03, 7.83539170e-04, 1.64101210e-03, 1.36304901e-03,\n",
      "       4.87668534e-04, 3.47515033e-04, 3.92886667e-04, 4.71261219e-04,\n",
      "       1.25838329e-03, 1.28351492e-03, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00]), 'param_RF__criterion': masked_array(data=['gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
      "                   'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
      "                   'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
      "                   'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
      "                   'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
      "                   'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
      "                   'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
      "                   'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
      "                   'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
      "                   'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
      "                   'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
      "                   'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
      "                   'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
      "                   'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
      "                   'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
      "                   'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
      "                   'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
      "                   'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
      "                   'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
      "                   'gini', 'gini', 'log_loss', 'log_loss', 'log_loss',\n",
      "                   'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
      "                   'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
      "                   'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
      "                   'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
      "                   'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
      "                   'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
      "                   'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
      "                   'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
      "                   'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
      "                   'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
      "                   'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
      "                   'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
      "                   'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
      "                   'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
      "                   'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
      "                   'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
      "                   'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
      "                   'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
      "                   'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
      "                   'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
      "                   'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
      "                   'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
      "                   'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
      "                   'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
      "                   'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
      "                   'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
      "                   'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
      "                   'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
      "                   'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
      "                   'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
      "                   'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
      "                   'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
      "                   'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy'],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_RF__max_depth': masked_array(data=[None, None, None, None, None, None, None, None, None,\n",
      "                   None, None, None, None, None, None, None, None, None,\n",
      "                   None, None, None, None, None, None, None, None, None,\n",
      "                   None, None, None, None, None, None, None, None, None,\n",
      "                   None, None, None, None, None, None, None, None, None,\n",
      "                   5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "                   5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "                   5, 5, 5, 5, 5, 5, 5, 5, 5, 10, 10, 10, 10, 10, 10, 10,\n",
      "                   10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
      "                   10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
      "                   10, 10, 10, 10, 10, 10, 10, 10, 10, 10, None, None,\n",
      "                   None, None, None, None, None, None, None, None, None,\n",
      "                   None, None, None, None, None, None, None, None, None,\n",
      "                   None, None, None, None, None, None, None, None, None,\n",
      "                   None, None, None, None, None, None, None, None, None,\n",
      "                   None, None, None, None, None, None, None, 5, 5, 5, 5,\n",
      "                   5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "                   5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "                   5, 5, 5, 5, 5, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
      "                   10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
      "                   10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
      "                   10, 10, 10, 10, 10, 10, 10, None, None, None, None,\n",
      "                   None, None, None, None, None, None, None, None, None,\n",
      "                   None, None, None, None, None, None, None, None, None,\n",
      "                   None, None, None, None, None, None, None, None, None,\n",
      "                   None, None, None, None, None, None, None, None, None,\n",
      "                   None, None, None, None, None, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "                   5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "                   5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "                   5, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
      "                   10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
      "                   10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
      "                   10, 10, 10, 10],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_RF__max_features': masked_array(data=['sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
      "                   'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
      "                   'sqrt', 'log2', 'log2', 'log2', 'log2', 'log2', 'log2',\n",
      "                   'log2', 'log2', 'log2', 'log2', 'log2', 'log2', 'log2',\n",
      "                   'log2', 'log2', 'None', 'None', 'None', 'None', 'None',\n",
      "                   'None', 'None', 'None', 'None', 'None', 'None', 'None',\n",
      "                   'None', 'None', 'None', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
      "                   'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
      "                   'sqrt', 'sqrt', 'sqrt', 'sqrt', 'log2', 'log2', 'log2',\n",
      "                   'log2', 'log2', 'log2', 'log2', 'log2', 'log2', 'log2',\n",
      "                   'log2', 'log2', 'log2', 'log2', 'log2', 'None', 'None',\n",
      "                   'None', 'None', 'None', 'None', 'None', 'None', 'None',\n",
      "                   'None', 'None', 'None', 'None', 'None', 'None', 'sqrt',\n",
      "                   'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
      "                   'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
      "                   'log2', 'log2', 'log2', 'log2', 'log2', 'log2', 'log2',\n",
      "                   'log2', 'log2', 'log2', 'log2', 'log2', 'log2', 'log2',\n",
      "                   'log2', 'None', 'None', 'None', 'None', 'None', 'None',\n",
      "                   'None', 'None', 'None', 'None', 'None', 'None', 'None',\n",
      "                   'None', 'None', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
      "                   'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
      "                   'sqrt', 'sqrt', 'sqrt', 'log2', 'log2', 'log2', 'log2',\n",
      "                   'log2', 'log2', 'log2', 'log2', 'log2', 'log2', 'log2',\n",
      "                   'log2', 'log2', 'log2', 'log2', 'None', 'None', 'None',\n",
      "                   'None', 'None', 'None', 'None', 'None', 'None', 'None',\n",
      "                   'None', 'None', 'None', 'None', 'None', 'sqrt', 'sqrt',\n",
      "                   'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
      "                   'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'log2',\n",
      "                   'log2', 'log2', 'log2', 'log2', 'log2', 'log2', 'log2',\n",
      "                   'log2', 'log2', 'log2', 'log2', 'log2', 'log2', 'log2',\n",
      "                   'None', 'None', 'None', 'None', 'None', 'None', 'None',\n",
      "                   'None', 'None', 'None', 'None', 'None', 'None', 'None',\n",
      "                   'None', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
      "                   'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
      "                   'sqrt', 'sqrt', 'log2', 'log2', 'log2', 'log2', 'log2',\n",
      "                   'log2', 'log2', 'log2', 'log2', 'log2', 'log2', 'log2',\n",
      "                   'log2', 'log2', 'log2', 'None', 'None', 'None', 'None',\n",
      "                   'None', 'None', 'None', 'None', 'None', 'None', 'None',\n",
      "                   'None', 'None', 'None', 'None', 'sqrt', 'sqrt', 'sqrt',\n",
      "                   'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
      "                   'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'log2', 'log2',\n",
      "                   'log2', 'log2', 'log2', 'log2', 'log2', 'log2', 'log2',\n",
      "                   'log2', 'log2', 'log2', 'log2', 'log2', 'log2', 'None',\n",
      "                   'None', 'None', 'None', 'None', 'None', 'None', 'None',\n",
      "                   'None', 'None', 'None', 'None', 'None', 'None', 'None',\n",
      "                   'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
      "                   'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
      "                   'sqrt', 'log2', 'log2', 'log2', 'log2', 'log2', 'log2',\n",
      "                   'log2', 'log2', 'log2', 'log2', 'log2', 'log2', 'log2',\n",
      "                   'log2', 'log2', 'None', 'None', 'None', 'None', 'None',\n",
      "                   'None', 'None', 'None', 'None', 'None', 'None', 'None',\n",
      "                   'None', 'None', 'None', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
      "                   'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
      "                   'sqrt', 'sqrt', 'sqrt', 'sqrt', 'log2', 'log2', 'log2',\n",
      "                   'log2', 'log2', 'log2', 'log2', 'log2', 'log2', 'log2',\n",
      "                   'log2', 'log2', 'log2', 'log2', 'log2', 'None', 'None',\n",
      "                   'None', 'None', 'None', 'None', 'None', 'None', 'None',\n",
      "                   'None', 'None', 'None', 'None', 'None', 'None'],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_RF__max_leaf_nodes': masked_array(data=[None, None, None, 8, 8, 8, 10, 10, 10, 16, 16, 16, 32,\n",
      "                   32, 32, None, None, None, 8, 8, 8, 10, 10, 10, 16, 16,\n",
      "                   16, 32, 32, 32, None, None, None, 8, 8, 8, 10, 10, 10,\n",
      "                   16, 16, 16, 32, 32, 32, None, None, None, 8, 8, 8, 10,\n",
      "                   10, 10, 16, 16, 16, 32, 32, 32, None, None, None, 8, 8,\n",
      "                   8, 10, 10, 10, 16, 16, 16, 32, 32, 32, None, None,\n",
      "                   None, 8, 8, 8, 10, 10, 10, 16, 16, 16, 32, 32, 32,\n",
      "                   None, None, None, 8, 8, 8, 10, 10, 10, 16, 16, 16, 32,\n",
      "                   32, 32, None, None, None, 8, 8, 8, 10, 10, 10, 16, 16,\n",
      "                   16, 32, 32, 32, None, None, None, 8, 8, 8, 10, 10, 10,\n",
      "                   16, 16, 16, 32, 32, 32, None, None, None, 8, 8, 8, 10,\n",
      "                   10, 10, 16, 16, 16, 32, 32, 32, None, None, None, 8, 8,\n",
      "                   8, 10, 10, 10, 16, 16, 16, 32, 32, 32, None, None,\n",
      "                   None, 8, 8, 8, 10, 10, 10, 16, 16, 16, 32, 32, 32,\n",
      "                   None, None, None, 8, 8, 8, 10, 10, 10, 16, 16, 16, 32,\n",
      "                   32, 32, None, None, None, 8, 8, 8, 10, 10, 10, 16, 16,\n",
      "                   16, 32, 32, 32, None, None, None, 8, 8, 8, 10, 10, 10,\n",
      "                   16, 16, 16, 32, 32, 32, None, None, None, 8, 8, 8, 10,\n",
      "                   10, 10, 16, 16, 16, 32, 32, 32, None, None, None, 8, 8,\n",
      "                   8, 10, 10, 10, 16, 16, 16, 32, 32, 32, None, None,\n",
      "                   None, 8, 8, 8, 10, 10, 10, 16, 16, 16, 32, 32, 32,\n",
      "                   None, None, None, 8, 8, 8, 10, 10, 10, 16, 16, 16, 32,\n",
      "                   32, 32, None, None, None, 8, 8, 8, 10, 10, 10, 16, 16,\n",
      "                   16, 32, 32, 32, None, None, None, 8, 8, 8, 10, 10, 10,\n",
      "                   16, 16, 16, 32, 32, 32, None, None, None, 8, 8, 8, 10,\n",
      "                   10, 10, 16, 16, 16, 32, 32, 32, None, None, None, 8, 8,\n",
      "                   8, 10, 10, 10, 16, 16, 16, 32, 32, 32, None, None,\n",
      "                   None, 8, 8, 8, 10, 10, 10, 16, 16, 16, 32, 32, 32,\n",
      "                   None, None, None, 8, 8, 8, 10, 10, 10, 16, 16, 16, 32,\n",
      "                   32, 32, None, None, None, 8, 8, 8, 10, 10, 10, 16, 16,\n",
      "                   16, 32, 32, 32, None, None, None, 8, 8, 8, 10, 10, 10,\n",
      "                   16, 16, 16, 32, 32, 32],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_RF__n_estimators': masked_array(data=[50, 100, 150, 50, 100, 150, 50, 100, 150, 50, 100, 150,\n",
      "                   50, 100, 150, 50, 100, 150, 50, 100, 150, 50, 100, 150,\n",
      "                   50, 100, 150, 50, 100, 150, 50, 100, 150, 50, 100, 150,\n",
      "                   50, 100, 150, 50, 100, 150, 50, 100, 150, 50, 100, 150,\n",
      "                   50, 100, 150, 50, 100, 150, 50, 100, 150, 50, 100, 150,\n",
      "                   50, 100, 150, 50, 100, 150, 50, 100, 150, 50, 100, 150,\n",
      "                   50, 100, 150, 50, 100, 150, 50, 100, 150, 50, 100, 150,\n",
      "                   50, 100, 150, 50, 100, 150, 50, 100, 150, 50, 100, 150,\n",
      "                   50, 100, 150, 50, 100, 150, 50, 100, 150, 50, 100, 150,\n",
      "                   50, 100, 150, 50, 100, 150, 50, 100, 150, 50, 100, 150,\n",
      "                   50, 100, 150, 50, 100, 150, 50, 100, 150, 50, 100, 150,\n",
      "                   50, 100, 150, 50, 100, 150, 50, 100, 150, 50, 100, 150,\n",
      "                   50, 100, 150, 50, 100, 150, 50, 100, 150, 50, 100, 150,\n",
      "                   50, 100, 150, 50, 100, 150, 50, 100, 150, 50, 100, 150,\n",
      "                   50, 100, 150, 50, 100, 150, 50, 100, 150, 50, 100, 150,\n",
      "                   50, 100, 150, 50, 100, 150, 50, 100, 150, 50, 100, 150,\n",
      "                   50, 100, 150, 50, 100, 150, 50, 100, 150, 50, 100, 150,\n",
      "                   50, 100, 150, 50, 100, 150, 50, 100, 150, 50, 100, 150,\n",
      "                   50, 100, 150, 50, 100, 150, 50, 100, 150, 50, 100, 150,\n",
      "                   50, 100, 150, 50, 100, 150, 50, 100, 150, 50, 100, 150,\n",
      "                   50, 100, 150, 50, 100, 150, 50, 100, 150, 50, 100, 150,\n",
      "                   50, 100, 150, 50, 100, 150, 50, 100, 150, 50, 100, 150,\n",
      "                   50, 100, 150, 50, 100, 150, 50, 100, 150, 50, 100, 150,\n",
      "                   50, 100, 150, 50, 100, 150, 50, 100, 150, 50, 100, 150,\n",
      "                   50, 100, 150, 50, 100, 150, 50, 100, 150, 50, 100, 150,\n",
      "                   50, 100, 150, 50, 100, 150, 50, 100, 150, 50, 100, 150,\n",
      "                   50, 100, 150, 50, 100, 150, 50, 100, 150, 50, 100, 150,\n",
      "                   50, 100, 150, 50, 100, 150, 50, 100, 150, 50, 100, 150,\n",
      "                   50, 100, 150, 50, 100, 150, 50, 100, 150, 50, 100, 150,\n",
      "                   50, 100, 150, 50, 100, 150, 50, 100, 150, 50, 100, 150,\n",
      "                   50, 100, 150, 50, 100, 150, 50, 100, 150, 50, 100, 150,\n",
      "                   50, 100, 150, 50, 100, 150, 50, 100, 150, 50, 100, 150,\n",
      "                   50, 100, 150, 50, 100, 150, 50, 100, 150, 50, 100, 150,\n",
      "                   50, 100, 150, 50, 100, 150, 50, 100, 150],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'RF__criterion': 'gini', 'RF__max_depth': None, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 50}, {'RF__criterion': 'gini', 'RF__max_depth': None, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 100}, {'RF__criterion': 'gini', 'RF__max_depth': None, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 150}, {'RF__criterion': 'gini', 'RF__max_depth': None, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 50}, {'RF__criterion': 'gini', 'RF__max_depth': None, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 100}, {'RF__criterion': 'gini', 'RF__max_depth': None, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 150}, {'RF__criterion': 'gini', 'RF__max_depth': None, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 50}, {'RF__criterion': 'gini', 'RF__max_depth': None, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 100}, {'RF__criterion': 'gini', 'RF__max_depth': None, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 150}, {'RF__criterion': 'gini', 'RF__max_depth': None, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 50}, {'RF__criterion': 'gini', 'RF__max_depth': None, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 100}, {'RF__criterion': 'gini', 'RF__max_depth': None, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 150}, {'RF__criterion': 'gini', 'RF__max_depth': None, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 50}, {'RF__criterion': 'gini', 'RF__max_depth': None, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 100}, {'RF__criterion': 'gini', 'RF__max_depth': None, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 150}, {'RF__criterion': 'gini', 'RF__max_depth': None, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 50}, {'RF__criterion': 'gini', 'RF__max_depth': None, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 100}, {'RF__criterion': 'gini', 'RF__max_depth': None, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 150}, {'RF__criterion': 'gini', 'RF__max_depth': None, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 50}, {'RF__criterion': 'gini', 'RF__max_depth': None, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 100}, {'RF__criterion': 'gini', 'RF__max_depth': None, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 150}, {'RF__criterion': 'gini', 'RF__max_depth': None, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 50}, {'RF__criterion': 'gini', 'RF__max_depth': None, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 100}, {'RF__criterion': 'gini', 'RF__max_depth': None, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 150}, {'RF__criterion': 'gini', 'RF__max_depth': None, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 50}, {'RF__criterion': 'gini', 'RF__max_depth': None, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 100}, {'RF__criterion': 'gini', 'RF__max_depth': None, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 150}, {'RF__criterion': 'gini', 'RF__max_depth': None, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 50}, {'RF__criterion': 'gini', 'RF__max_depth': None, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 100}, {'RF__criterion': 'gini', 'RF__max_depth': None, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 150}, {'RF__criterion': 'gini', 'RF__max_depth': None, 'RF__max_features': 'None', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 50}, {'RF__criterion': 'gini', 'RF__max_depth': None, 'RF__max_features': 'None', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 100}, {'RF__criterion': 'gini', 'RF__max_depth': None, 'RF__max_features': 'None', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 150}, {'RF__criterion': 'gini', 'RF__max_depth': None, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 50}, {'RF__criterion': 'gini', 'RF__max_depth': None, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 100}, {'RF__criterion': 'gini', 'RF__max_depth': None, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 150}, {'RF__criterion': 'gini', 'RF__max_depth': None, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 50}, {'RF__criterion': 'gini', 'RF__max_depth': None, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 100}, {'RF__criterion': 'gini', 'RF__max_depth': None, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 150}, {'RF__criterion': 'gini', 'RF__max_depth': None, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 50}, {'RF__criterion': 'gini', 'RF__max_depth': None, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 100}, {'RF__criterion': 'gini', 'RF__max_depth': None, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 150}, {'RF__criterion': 'gini', 'RF__max_depth': None, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 50}, {'RF__criterion': 'gini', 'RF__max_depth': None, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 100}, {'RF__criterion': 'gini', 'RF__max_depth': None, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 150}, {'RF__criterion': 'gini', 'RF__max_depth': 5, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 50}, {'RF__criterion': 'gini', 'RF__max_depth': 5, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 100}, {'RF__criterion': 'gini', 'RF__max_depth': 5, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 150}, {'RF__criterion': 'gini', 'RF__max_depth': 5, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 50}, {'RF__criterion': 'gini', 'RF__max_depth': 5, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 100}, {'RF__criterion': 'gini', 'RF__max_depth': 5, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 150}, {'RF__criterion': 'gini', 'RF__max_depth': 5, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 50}, {'RF__criterion': 'gini', 'RF__max_depth': 5, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 100}, {'RF__criterion': 'gini', 'RF__max_depth': 5, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 150}, {'RF__criterion': 'gini', 'RF__max_depth': 5, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 50}, {'RF__criterion': 'gini', 'RF__max_depth': 5, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 100}, {'RF__criterion': 'gini', 'RF__max_depth': 5, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 150}, {'RF__criterion': 'gini', 'RF__max_depth': 5, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 50}, {'RF__criterion': 'gini', 'RF__max_depth': 5, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 100}, {'RF__criterion': 'gini', 'RF__max_depth': 5, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 150}, {'RF__criterion': 'gini', 'RF__max_depth': 5, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 50}, {'RF__criterion': 'gini', 'RF__max_depth': 5, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 100}, {'RF__criterion': 'gini', 'RF__max_depth': 5, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 150}, {'RF__criterion': 'gini', 'RF__max_depth': 5, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 50}, {'RF__criterion': 'gini', 'RF__max_depth': 5, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 100}, {'RF__criterion': 'gini', 'RF__max_depth': 5, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 150}, {'RF__criterion': 'gini', 'RF__max_depth': 5, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 50}, {'RF__criterion': 'gini', 'RF__max_depth': 5, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 100}, {'RF__criterion': 'gini', 'RF__max_depth': 5, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 150}, {'RF__criterion': 'gini', 'RF__max_depth': 5, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 50}, {'RF__criterion': 'gini', 'RF__max_depth': 5, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 100}, {'RF__criterion': 'gini', 'RF__max_depth': 5, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 150}, {'RF__criterion': 'gini', 'RF__max_depth': 5, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 50}, {'RF__criterion': 'gini', 'RF__max_depth': 5, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 100}, {'RF__criterion': 'gini', 'RF__max_depth': 5, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 150}, {'RF__criterion': 'gini', 'RF__max_depth': 5, 'RF__max_features': 'None', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 50}, {'RF__criterion': 'gini', 'RF__max_depth': 5, 'RF__max_features': 'None', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 100}, {'RF__criterion': 'gini', 'RF__max_depth': 5, 'RF__max_features': 'None', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 150}, {'RF__criterion': 'gini', 'RF__max_depth': 5, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 50}, {'RF__criterion': 'gini', 'RF__max_depth': 5, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 100}, {'RF__criterion': 'gini', 'RF__max_depth': 5, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 150}, {'RF__criterion': 'gini', 'RF__max_depth': 5, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 50}, {'RF__criterion': 'gini', 'RF__max_depth': 5, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 100}, {'RF__criterion': 'gini', 'RF__max_depth': 5, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 150}, {'RF__criterion': 'gini', 'RF__max_depth': 5, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 50}, {'RF__criterion': 'gini', 'RF__max_depth': 5, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 100}, {'RF__criterion': 'gini', 'RF__max_depth': 5, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 150}, {'RF__criterion': 'gini', 'RF__max_depth': 5, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 50}, {'RF__criterion': 'gini', 'RF__max_depth': 5, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 100}, {'RF__criterion': 'gini', 'RF__max_depth': 5, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 150}, {'RF__criterion': 'gini', 'RF__max_depth': 10, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 50}, {'RF__criterion': 'gini', 'RF__max_depth': 10, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 100}, {'RF__criterion': 'gini', 'RF__max_depth': 10, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 150}, {'RF__criterion': 'gini', 'RF__max_depth': 10, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 50}, {'RF__criterion': 'gini', 'RF__max_depth': 10, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 100}, {'RF__criterion': 'gini', 'RF__max_depth': 10, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 150}, {'RF__criterion': 'gini', 'RF__max_depth': 10, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 50}, {'RF__criterion': 'gini', 'RF__max_depth': 10, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 100}, {'RF__criterion': 'gini', 'RF__max_depth': 10, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 150}, {'RF__criterion': 'gini', 'RF__max_depth': 10, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 50}, {'RF__criterion': 'gini', 'RF__max_depth': 10, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 100}, {'RF__criterion': 'gini', 'RF__max_depth': 10, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 150}, {'RF__criterion': 'gini', 'RF__max_depth': 10, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 50}, {'RF__criterion': 'gini', 'RF__max_depth': 10, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 100}, {'RF__criterion': 'gini', 'RF__max_depth': 10, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 150}, {'RF__criterion': 'gini', 'RF__max_depth': 10, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 50}, {'RF__criterion': 'gini', 'RF__max_depth': 10, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 100}, {'RF__criterion': 'gini', 'RF__max_depth': 10, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 150}, {'RF__criterion': 'gini', 'RF__max_depth': 10, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 50}, {'RF__criterion': 'gini', 'RF__max_depth': 10, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 100}, {'RF__criterion': 'gini', 'RF__max_depth': 10, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 150}, {'RF__criterion': 'gini', 'RF__max_depth': 10, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 50}, {'RF__criterion': 'gini', 'RF__max_depth': 10, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 100}, {'RF__criterion': 'gini', 'RF__max_depth': 10, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 150}, {'RF__criterion': 'gini', 'RF__max_depth': 10, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 50}, {'RF__criterion': 'gini', 'RF__max_depth': 10, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 100}, {'RF__criterion': 'gini', 'RF__max_depth': 10, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 150}, {'RF__criterion': 'gini', 'RF__max_depth': 10, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 50}, {'RF__criterion': 'gini', 'RF__max_depth': 10, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 100}, {'RF__criterion': 'gini', 'RF__max_depth': 10, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 150}, {'RF__criterion': 'gini', 'RF__max_depth': 10, 'RF__max_features': 'None', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 50}, {'RF__criterion': 'gini', 'RF__max_depth': 10, 'RF__max_features': 'None', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 100}, {'RF__criterion': 'gini', 'RF__max_depth': 10, 'RF__max_features': 'None', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 150}, {'RF__criterion': 'gini', 'RF__max_depth': 10, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 50}, {'RF__criterion': 'gini', 'RF__max_depth': 10, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 100}, {'RF__criterion': 'gini', 'RF__max_depth': 10, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 150}, {'RF__criterion': 'gini', 'RF__max_depth': 10, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 50}, {'RF__criterion': 'gini', 'RF__max_depth': 10, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 100}, {'RF__criterion': 'gini', 'RF__max_depth': 10, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 150}, {'RF__criterion': 'gini', 'RF__max_depth': 10, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 50}, {'RF__criterion': 'gini', 'RF__max_depth': 10, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 100}, {'RF__criterion': 'gini', 'RF__max_depth': 10, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 150}, {'RF__criterion': 'gini', 'RF__max_depth': 10, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 50}, {'RF__criterion': 'gini', 'RF__max_depth': 10, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 100}, {'RF__criterion': 'gini', 'RF__max_depth': 10, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 150}, {'RF__criterion': 'log_loss', 'RF__max_depth': None, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 50}, {'RF__criterion': 'log_loss', 'RF__max_depth': None, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 100}, {'RF__criterion': 'log_loss', 'RF__max_depth': None, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 150}, {'RF__criterion': 'log_loss', 'RF__max_depth': None, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 50}, {'RF__criterion': 'log_loss', 'RF__max_depth': None, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 100}, {'RF__criterion': 'log_loss', 'RF__max_depth': None, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 150}, {'RF__criterion': 'log_loss', 'RF__max_depth': None, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 50}, {'RF__criterion': 'log_loss', 'RF__max_depth': None, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 100}, {'RF__criterion': 'log_loss', 'RF__max_depth': None, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 150}, {'RF__criterion': 'log_loss', 'RF__max_depth': None, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 50}, {'RF__criterion': 'log_loss', 'RF__max_depth': None, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 100}, {'RF__criterion': 'log_loss', 'RF__max_depth': None, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 150}, {'RF__criterion': 'log_loss', 'RF__max_depth': None, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 50}, {'RF__criterion': 'log_loss', 'RF__max_depth': None, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 100}, {'RF__criterion': 'log_loss', 'RF__max_depth': None, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 150}, {'RF__criterion': 'log_loss', 'RF__max_depth': None, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 50}, {'RF__criterion': 'log_loss', 'RF__max_depth': None, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 100}, {'RF__criterion': 'log_loss', 'RF__max_depth': None, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 150}, {'RF__criterion': 'log_loss', 'RF__max_depth': None, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 50}, {'RF__criterion': 'log_loss', 'RF__max_depth': None, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 100}, {'RF__criterion': 'log_loss', 'RF__max_depth': None, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 150}, {'RF__criterion': 'log_loss', 'RF__max_depth': None, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 50}, {'RF__criterion': 'log_loss', 'RF__max_depth': None, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 100}, {'RF__criterion': 'log_loss', 'RF__max_depth': None, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 150}, {'RF__criterion': 'log_loss', 'RF__max_depth': None, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 50}, {'RF__criterion': 'log_loss', 'RF__max_depth': None, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 100}, {'RF__criterion': 'log_loss', 'RF__max_depth': None, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 150}, {'RF__criterion': 'log_loss', 'RF__max_depth': None, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 50}, {'RF__criterion': 'log_loss', 'RF__max_depth': None, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 100}, {'RF__criterion': 'log_loss', 'RF__max_depth': None, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 150}, {'RF__criterion': 'log_loss', 'RF__max_depth': None, 'RF__max_features': 'None', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 50}, {'RF__criterion': 'log_loss', 'RF__max_depth': None, 'RF__max_features': 'None', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 100}, {'RF__criterion': 'log_loss', 'RF__max_depth': None, 'RF__max_features': 'None', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 150}, {'RF__criterion': 'log_loss', 'RF__max_depth': None, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 50}, {'RF__criterion': 'log_loss', 'RF__max_depth': None, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 100}, {'RF__criterion': 'log_loss', 'RF__max_depth': None, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 150}, {'RF__criterion': 'log_loss', 'RF__max_depth': None, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 50}, {'RF__criterion': 'log_loss', 'RF__max_depth': None, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 100}, {'RF__criterion': 'log_loss', 'RF__max_depth': None, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 150}, {'RF__criterion': 'log_loss', 'RF__max_depth': None, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 50}, {'RF__criterion': 'log_loss', 'RF__max_depth': None, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 100}, {'RF__criterion': 'log_loss', 'RF__max_depth': None, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 150}, {'RF__criterion': 'log_loss', 'RF__max_depth': None, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 50}, {'RF__criterion': 'log_loss', 'RF__max_depth': None, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 100}, {'RF__criterion': 'log_loss', 'RF__max_depth': None, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 150}, {'RF__criterion': 'log_loss', 'RF__max_depth': 5, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 50}, {'RF__criterion': 'log_loss', 'RF__max_depth': 5, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 100}, {'RF__criterion': 'log_loss', 'RF__max_depth': 5, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 150}, {'RF__criterion': 'log_loss', 'RF__max_depth': 5, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 50}, {'RF__criterion': 'log_loss', 'RF__max_depth': 5, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 100}, {'RF__criterion': 'log_loss', 'RF__max_depth': 5, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 150}, {'RF__criterion': 'log_loss', 'RF__max_depth': 5, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 50}, {'RF__criterion': 'log_loss', 'RF__max_depth': 5, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 100}, {'RF__criterion': 'log_loss', 'RF__max_depth': 5, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 150}, {'RF__criterion': 'log_loss', 'RF__max_depth': 5, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 50}, {'RF__criterion': 'log_loss', 'RF__max_depth': 5, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 100}, {'RF__criterion': 'log_loss', 'RF__max_depth': 5, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 150}, {'RF__criterion': 'log_loss', 'RF__max_depth': 5, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 50}, {'RF__criterion': 'log_loss', 'RF__max_depth': 5, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 100}, {'RF__criterion': 'log_loss', 'RF__max_depth': 5, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 150}, {'RF__criterion': 'log_loss', 'RF__max_depth': 5, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 50}, {'RF__criterion': 'log_loss', 'RF__max_depth': 5, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 100}, {'RF__criterion': 'log_loss', 'RF__max_depth': 5, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 150}, {'RF__criterion': 'log_loss', 'RF__max_depth': 5, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 50}, {'RF__criterion': 'log_loss', 'RF__max_depth': 5, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 100}, {'RF__criterion': 'log_loss', 'RF__max_depth': 5, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 150}, {'RF__criterion': 'log_loss', 'RF__max_depth': 5, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 50}, {'RF__criterion': 'log_loss', 'RF__max_depth': 5, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 100}, {'RF__criterion': 'log_loss', 'RF__max_depth': 5, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 150}, {'RF__criterion': 'log_loss', 'RF__max_depth': 5, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 50}, {'RF__criterion': 'log_loss', 'RF__max_depth': 5, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 100}, {'RF__criterion': 'log_loss', 'RF__max_depth': 5, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 150}, {'RF__criterion': 'log_loss', 'RF__max_depth': 5, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 50}, {'RF__criterion': 'log_loss', 'RF__max_depth': 5, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 100}, {'RF__criterion': 'log_loss', 'RF__max_depth': 5, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 150}, {'RF__criterion': 'log_loss', 'RF__max_depth': 5, 'RF__max_features': 'None', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 50}, {'RF__criterion': 'log_loss', 'RF__max_depth': 5, 'RF__max_features': 'None', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 100}, {'RF__criterion': 'log_loss', 'RF__max_depth': 5, 'RF__max_features': 'None', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 150}, {'RF__criterion': 'log_loss', 'RF__max_depth': 5, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 50}, {'RF__criterion': 'log_loss', 'RF__max_depth': 5, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 100}, {'RF__criterion': 'log_loss', 'RF__max_depth': 5, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 150}, {'RF__criterion': 'log_loss', 'RF__max_depth': 5, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 50}, {'RF__criterion': 'log_loss', 'RF__max_depth': 5, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 100}, {'RF__criterion': 'log_loss', 'RF__max_depth': 5, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 150}, {'RF__criterion': 'log_loss', 'RF__max_depth': 5, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 50}, {'RF__criterion': 'log_loss', 'RF__max_depth': 5, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 100}, {'RF__criterion': 'log_loss', 'RF__max_depth': 5, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 150}, {'RF__criterion': 'log_loss', 'RF__max_depth': 5, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 50}, {'RF__criterion': 'log_loss', 'RF__max_depth': 5, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 100}, {'RF__criterion': 'log_loss', 'RF__max_depth': 5, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 150}, {'RF__criterion': 'log_loss', 'RF__max_depth': 10, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 50}, {'RF__criterion': 'log_loss', 'RF__max_depth': 10, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 100}, {'RF__criterion': 'log_loss', 'RF__max_depth': 10, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 150}, {'RF__criterion': 'log_loss', 'RF__max_depth': 10, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 50}, {'RF__criterion': 'log_loss', 'RF__max_depth': 10, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 100}, {'RF__criterion': 'log_loss', 'RF__max_depth': 10, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 150}, {'RF__criterion': 'log_loss', 'RF__max_depth': 10, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 50}, {'RF__criterion': 'log_loss', 'RF__max_depth': 10, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 100}, {'RF__criterion': 'log_loss', 'RF__max_depth': 10, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 150}, {'RF__criterion': 'log_loss', 'RF__max_depth': 10, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 50}, {'RF__criterion': 'log_loss', 'RF__max_depth': 10, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 100}, {'RF__criterion': 'log_loss', 'RF__max_depth': 10, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 150}, {'RF__criterion': 'log_loss', 'RF__max_depth': 10, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 50}, {'RF__criterion': 'log_loss', 'RF__max_depth': 10, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 100}, {'RF__criterion': 'log_loss', 'RF__max_depth': 10, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 150}, {'RF__criterion': 'log_loss', 'RF__max_depth': 10, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 50}, {'RF__criterion': 'log_loss', 'RF__max_depth': 10, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 100}, {'RF__criterion': 'log_loss', 'RF__max_depth': 10, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 150}, {'RF__criterion': 'log_loss', 'RF__max_depth': 10, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 50}, {'RF__criterion': 'log_loss', 'RF__max_depth': 10, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 100}, {'RF__criterion': 'log_loss', 'RF__max_depth': 10, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 150}, {'RF__criterion': 'log_loss', 'RF__max_depth': 10, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 50}, {'RF__criterion': 'log_loss', 'RF__max_depth': 10, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 100}, {'RF__criterion': 'log_loss', 'RF__max_depth': 10, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 150}, {'RF__criterion': 'log_loss', 'RF__max_depth': 10, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 50}, {'RF__criterion': 'log_loss', 'RF__max_depth': 10, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 100}, {'RF__criterion': 'log_loss', 'RF__max_depth': 10, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 150}, {'RF__criterion': 'log_loss', 'RF__max_depth': 10, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 50}, {'RF__criterion': 'log_loss', 'RF__max_depth': 10, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 100}, {'RF__criterion': 'log_loss', 'RF__max_depth': 10, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 150}, {'RF__criterion': 'log_loss', 'RF__max_depth': 10, 'RF__max_features': 'None', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 50}, {'RF__criterion': 'log_loss', 'RF__max_depth': 10, 'RF__max_features': 'None', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 100}, {'RF__criterion': 'log_loss', 'RF__max_depth': 10, 'RF__max_features': 'None', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 150}, {'RF__criterion': 'log_loss', 'RF__max_depth': 10, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 50}, {'RF__criterion': 'log_loss', 'RF__max_depth': 10, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 100}, {'RF__criterion': 'log_loss', 'RF__max_depth': 10, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 150}, {'RF__criterion': 'log_loss', 'RF__max_depth': 10, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 50}, {'RF__criterion': 'log_loss', 'RF__max_depth': 10, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 100}, {'RF__criterion': 'log_loss', 'RF__max_depth': 10, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 150}, {'RF__criterion': 'log_loss', 'RF__max_depth': 10, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 50}, {'RF__criterion': 'log_loss', 'RF__max_depth': 10, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 100}, {'RF__criterion': 'log_loss', 'RF__max_depth': 10, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 150}, {'RF__criterion': 'log_loss', 'RF__max_depth': 10, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 50}, {'RF__criterion': 'log_loss', 'RF__max_depth': 10, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 100}, {'RF__criterion': 'log_loss', 'RF__max_depth': 10, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 150}, {'RF__criterion': 'entropy', 'RF__max_depth': None, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 50}, {'RF__criterion': 'entropy', 'RF__max_depth': None, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 100}, {'RF__criterion': 'entropy', 'RF__max_depth': None, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 150}, {'RF__criterion': 'entropy', 'RF__max_depth': None, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 50}, {'RF__criterion': 'entropy', 'RF__max_depth': None, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 100}, {'RF__criterion': 'entropy', 'RF__max_depth': None, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 150}, {'RF__criterion': 'entropy', 'RF__max_depth': None, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 50}, {'RF__criterion': 'entropy', 'RF__max_depth': None, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 100}, {'RF__criterion': 'entropy', 'RF__max_depth': None, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 150}, {'RF__criterion': 'entropy', 'RF__max_depth': None, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 50}, {'RF__criterion': 'entropy', 'RF__max_depth': None, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 100}, {'RF__criterion': 'entropy', 'RF__max_depth': None, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 150}, {'RF__criterion': 'entropy', 'RF__max_depth': None, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 50}, {'RF__criterion': 'entropy', 'RF__max_depth': None, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 100}, {'RF__criterion': 'entropy', 'RF__max_depth': None, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 150}, {'RF__criterion': 'entropy', 'RF__max_depth': None, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 50}, {'RF__criterion': 'entropy', 'RF__max_depth': None, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 100}, {'RF__criterion': 'entropy', 'RF__max_depth': None, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 150}, {'RF__criterion': 'entropy', 'RF__max_depth': None, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 50}, {'RF__criterion': 'entropy', 'RF__max_depth': None, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 100}, {'RF__criterion': 'entropy', 'RF__max_depth': None, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 150}, {'RF__criterion': 'entropy', 'RF__max_depth': None, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 50}, {'RF__criterion': 'entropy', 'RF__max_depth': None, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 100}, {'RF__criterion': 'entropy', 'RF__max_depth': None, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 150}, {'RF__criterion': 'entropy', 'RF__max_depth': None, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 50}, {'RF__criterion': 'entropy', 'RF__max_depth': None, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 100}, {'RF__criterion': 'entropy', 'RF__max_depth': None, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 150}, {'RF__criterion': 'entropy', 'RF__max_depth': None, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 50}, {'RF__criterion': 'entropy', 'RF__max_depth': None, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 100}, {'RF__criterion': 'entropy', 'RF__max_depth': None, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 150}, {'RF__criterion': 'entropy', 'RF__max_depth': None, 'RF__max_features': 'None', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 50}, {'RF__criterion': 'entropy', 'RF__max_depth': None, 'RF__max_features': 'None', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 100}, {'RF__criterion': 'entropy', 'RF__max_depth': None, 'RF__max_features': 'None', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 150}, {'RF__criterion': 'entropy', 'RF__max_depth': None, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 50}, {'RF__criterion': 'entropy', 'RF__max_depth': None, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 100}, {'RF__criterion': 'entropy', 'RF__max_depth': None, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 150}, {'RF__criterion': 'entropy', 'RF__max_depth': None, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 50}, {'RF__criterion': 'entropy', 'RF__max_depth': None, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 100}, {'RF__criterion': 'entropy', 'RF__max_depth': None, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 150}, {'RF__criterion': 'entropy', 'RF__max_depth': None, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 50}, {'RF__criterion': 'entropy', 'RF__max_depth': None, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 100}, {'RF__criterion': 'entropy', 'RF__max_depth': None, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 150}, {'RF__criterion': 'entropy', 'RF__max_depth': None, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 50}, {'RF__criterion': 'entropy', 'RF__max_depth': None, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 100}, {'RF__criterion': 'entropy', 'RF__max_depth': None, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 150}, {'RF__criterion': 'entropy', 'RF__max_depth': 5, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 50}, {'RF__criterion': 'entropy', 'RF__max_depth': 5, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 100}, {'RF__criterion': 'entropy', 'RF__max_depth': 5, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 150}, {'RF__criterion': 'entropy', 'RF__max_depth': 5, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 50}, {'RF__criterion': 'entropy', 'RF__max_depth': 5, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 100}, {'RF__criterion': 'entropy', 'RF__max_depth': 5, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 150}, {'RF__criterion': 'entropy', 'RF__max_depth': 5, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 50}, {'RF__criterion': 'entropy', 'RF__max_depth': 5, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 100}, {'RF__criterion': 'entropy', 'RF__max_depth': 5, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 150}, {'RF__criterion': 'entropy', 'RF__max_depth': 5, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 50}, {'RF__criterion': 'entropy', 'RF__max_depth': 5, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 100}, {'RF__criterion': 'entropy', 'RF__max_depth': 5, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 150}, {'RF__criterion': 'entropy', 'RF__max_depth': 5, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 50}, {'RF__criterion': 'entropy', 'RF__max_depth': 5, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 100}, {'RF__criterion': 'entropy', 'RF__max_depth': 5, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 150}, {'RF__criterion': 'entropy', 'RF__max_depth': 5, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 50}, {'RF__criterion': 'entropy', 'RF__max_depth': 5, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 100}, {'RF__criterion': 'entropy', 'RF__max_depth': 5, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 150}, {'RF__criterion': 'entropy', 'RF__max_depth': 5, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 50}, {'RF__criterion': 'entropy', 'RF__max_depth': 5, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 100}, {'RF__criterion': 'entropy', 'RF__max_depth': 5, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 150}, {'RF__criterion': 'entropy', 'RF__max_depth': 5, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 50}, {'RF__criterion': 'entropy', 'RF__max_depth': 5, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 100}, {'RF__criterion': 'entropy', 'RF__max_depth': 5, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 150}, {'RF__criterion': 'entropy', 'RF__max_depth': 5, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 50}, {'RF__criterion': 'entropy', 'RF__max_depth': 5, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 100}, {'RF__criterion': 'entropy', 'RF__max_depth': 5, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 150}, {'RF__criterion': 'entropy', 'RF__max_depth': 5, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 50}, {'RF__criterion': 'entropy', 'RF__max_depth': 5, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 100}, {'RF__criterion': 'entropy', 'RF__max_depth': 5, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 150}, {'RF__criterion': 'entropy', 'RF__max_depth': 5, 'RF__max_features': 'None', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 50}, {'RF__criterion': 'entropy', 'RF__max_depth': 5, 'RF__max_features': 'None', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 100}, {'RF__criterion': 'entropy', 'RF__max_depth': 5, 'RF__max_features': 'None', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 150}, {'RF__criterion': 'entropy', 'RF__max_depth': 5, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 50}, {'RF__criterion': 'entropy', 'RF__max_depth': 5, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 100}, {'RF__criterion': 'entropy', 'RF__max_depth': 5, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 150}, {'RF__criterion': 'entropy', 'RF__max_depth': 5, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 50}, {'RF__criterion': 'entropy', 'RF__max_depth': 5, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 100}, {'RF__criterion': 'entropy', 'RF__max_depth': 5, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 150}, {'RF__criterion': 'entropy', 'RF__max_depth': 5, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 50}, {'RF__criterion': 'entropy', 'RF__max_depth': 5, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 100}, {'RF__criterion': 'entropy', 'RF__max_depth': 5, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 150}, {'RF__criterion': 'entropy', 'RF__max_depth': 5, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 50}, {'RF__criterion': 'entropy', 'RF__max_depth': 5, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 100}, {'RF__criterion': 'entropy', 'RF__max_depth': 5, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 150}, {'RF__criterion': 'entropy', 'RF__max_depth': 10, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 50}, {'RF__criterion': 'entropy', 'RF__max_depth': 10, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 100}, {'RF__criterion': 'entropy', 'RF__max_depth': 10, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 150}, {'RF__criterion': 'entropy', 'RF__max_depth': 10, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 50}, {'RF__criterion': 'entropy', 'RF__max_depth': 10, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 100}, {'RF__criterion': 'entropy', 'RF__max_depth': 10, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 150}, {'RF__criterion': 'entropy', 'RF__max_depth': 10, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 50}, {'RF__criterion': 'entropy', 'RF__max_depth': 10, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 100}, {'RF__criterion': 'entropy', 'RF__max_depth': 10, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 150}, {'RF__criterion': 'entropy', 'RF__max_depth': 10, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 50}, {'RF__criterion': 'entropy', 'RF__max_depth': 10, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 100}, {'RF__criterion': 'entropy', 'RF__max_depth': 10, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 150}, {'RF__criterion': 'entropy', 'RF__max_depth': 10, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 50}, {'RF__criterion': 'entropy', 'RF__max_depth': 10, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 100}, {'RF__criterion': 'entropy', 'RF__max_depth': 10, 'RF__max_features': 'sqrt', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 150}, {'RF__criterion': 'entropy', 'RF__max_depth': 10, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 50}, {'RF__criterion': 'entropy', 'RF__max_depth': 10, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 100}, {'RF__criterion': 'entropy', 'RF__max_depth': 10, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 150}, {'RF__criterion': 'entropy', 'RF__max_depth': 10, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 50}, {'RF__criterion': 'entropy', 'RF__max_depth': 10, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 100}, {'RF__criterion': 'entropy', 'RF__max_depth': 10, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 150}, {'RF__criterion': 'entropy', 'RF__max_depth': 10, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 50}, {'RF__criterion': 'entropy', 'RF__max_depth': 10, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 100}, {'RF__criterion': 'entropy', 'RF__max_depth': 10, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 150}, {'RF__criterion': 'entropy', 'RF__max_depth': 10, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 50}, {'RF__criterion': 'entropy', 'RF__max_depth': 10, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 100}, {'RF__criterion': 'entropy', 'RF__max_depth': 10, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 150}, {'RF__criterion': 'entropy', 'RF__max_depth': 10, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 50}, {'RF__criterion': 'entropy', 'RF__max_depth': 10, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 100}, {'RF__criterion': 'entropy', 'RF__max_depth': 10, 'RF__max_features': 'log2', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 150}, {'RF__criterion': 'entropy', 'RF__max_depth': 10, 'RF__max_features': 'None', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 50}, {'RF__criterion': 'entropy', 'RF__max_depth': 10, 'RF__max_features': 'None', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 100}, {'RF__criterion': 'entropy', 'RF__max_depth': 10, 'RF__max_features': 'None', 'RF__max_leaf_nodes': None, 'RF__n_estimators': 150}, {'RF__criterion': 'entropy', 'RF__max_depth': 10, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 50}, {'RF__criterion': 'entropy', 'RF__max_depth': 10, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 100}, {'RF__criterion': 'entropy', 'RF__max_depth': 10, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 8, 'RF__n_estimators': 150}, {'RF__criterion': 'entropy', 'RF__max_depth': 10, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 50}, {'RF__criterion': 'entropy', 'RF__max_depth': 10, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 100}, {'RF__criterion': 'entropy', 'RF__max_depth': 10, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 10, 'RF__n_estimators': 150}, {'RF__criterion': 'entropy', 'RF__max_depth': 10, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 50}, {'RF__criterion': 'entropy', 'RF__max_depth': 10, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 100}, {'RF__criterion': 'entropy', 'RF__max_depth': 10, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 16, 'RF__n_estimators': 150}, {'RF__criterion': 'entropy', 'RF__max_depth': 10, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 50}, {'RF__criterion': 'entropy', 'RF__max_depth': 10, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 100}, {'RF__criterion': 'entropy', 'RF__max_depth': 10, 'RF__max_features': 'None', 'RF__max_leaf_nodes': 32, 'RF__n_estimators': 150}], 'split0_test_score': array([0.61382663, 0.61409668, 0.61760735, 0.55981637, 0.56008642,\n",
      "       0.56224683, 0.56764785, 0.56413719, 0.5679179 , 0.57493924,\n",
      "       0.57953011, 0.57709965, 0.59114232, 0.58952201, 0.59492304,\n",
      "       0.60815555, 0.61112611, 0.61706724, 0.56278693, 0.55792601,\n",
      "       0.55954631, 0.56143667, 0.56332703, 0.56440724, 0.57142857,\n",
      "       0.57601944, 0.57493924, 0.59060221, 0.59033216, 0.58979206,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "       0.57926006, 0.57844991, 0.57817985, 0.55819606, 0.56143667,\n",
      "       0.56089657, 0.5668377 , 0.56521739, 0.56899811, 0.57223873,\n",
      "       0.5762895 , 0.57331893, 0.5779098 , 0.57871996, 0.57953011,\n",
      "       0.5779098 , 0.57763975, 0.58115042, 0.55981637, 0.56035647,\n",
      "       0.55981637, 0.55846611, 0.56548744, 0.56251688, 0.5762895 ,\n",
      "       0.57439914, 0.57385903, 0.57655955, 0.57817985, 0.57709965,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "       0.6178774 , 0.62003781, 0.61976776, 0.56116662, 0.56467729,\n",
      "       0.56521739, 0.56710775, 0.56521739, 0.56467729, 0.57655955,\n",
      "       0.57709965, 0.57844991, 0.59114232, 0.59033216, 0.59033216,\n",
      "       0.6184175 , 0.62219822, 0.62300837, 0.5557656 , 0.56062652,\n",
      "       0.56062652, 0.56197678, 0.56332703, 0.56467729, 0.57520929,\n",
      "       0.57385903, 0.5762895 , 0.59573319, 0.59087227, 0.59195247,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "       0.61922765, 0.62003781, 0.61868755, 0.56116662, 0.56440724,\n",
      "       0.56170672, 0.5662976 , 0.57034837, 0.56980826, 0.57250878,\n",
      "       0.57412908, 0.57520929, 0.58898191, 0.58817175, 0.5879017 ,\n",
      "       0.61490683, 0.61679719, 0.62003781, 0.56440724, 0.56197678,\n",
      "       0.56413719, 0.56926816, 0.57034837, 0.56818796, 0.57331893,\n",
      "       0.57926006, 0.57520929, 0.59141237, 0.5873616 , 0.5868215 ,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "       0.57899001, 0.58250068, 0.58007021, 0.56359708, 0.56224683,\n",
      "       0.56575749, 0.56710775, 0.56656765, 0.56440724, 0.57493924,\n",
      "       0.57763975, 0.57655955, 0.58034026, 0.58196057, 0.58142047,\n",
      "       0.58169052, 0.58088037, 0.57899001, 0.56170672, 0.56278693,\n",
      "       0.56089657, 0.56710775, 0.56467729, 0.57142857, 0.57574939,\n",
      "       0.57466919, 0.57547934, 0.57953011, 0.57899001, 0.57980016,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "       0.61868755, 0.62300837, 0.62030786, 0.56413719, 0.56062652,\n",
      "       0.56494734, 0.57007831, 0.5679179 , 0.56980826, 0.57196867,\n",
      "       0.57844991, 0.5762895 , 0.58709155, 0.58763165, 0.58763165,\n",
      "       0.61733729, 0.62462868, 0.62570888, 0.56062652, 0.56602755,\n",
      "       0.56386713, 0.57088847, 0.56899811, 0.56926816, 0.5768296 ,\n",
      "       0.57763975, 0.57547934, 0.58547124, 0.5868215 , 0.58763165,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "       0.62057791, 0.61733729, 0.61733729, 0.56170672, 0.56332703,\n",
      "       0.56305698, 0.5673778 , 0.56818796, 0.56926816, 0.57601944,\n",
      "       0.57709965, 0.5773697 , 0.58952201, 0.58628139, 0.58817175,\n",
      "       0.61031596, 0.61463678, 0.61733729, 0.5668377 , 0.56359708,\n",
      "       0.56521739, 0.5662976 , 0.57061842, 0.56953821, 0.57385903,\n",
      "       0.57466919, 0.57466919, 0.58952201, 0.5879017 , 0.58520119,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "       0.57980016, 0.58223062, 0.58304078, 0.56413719, 0.55846611,\n",
      "       0.56332703, 0.56521739, 0.56953821, 0.56845801, 0.5768296 ,\n",
      "       0.57331893, 0.57601944, 0.58061032, 0.57817985, 0.58412098,\n",
      "       0.58196057, 0.58250068, 0.58034026, 0.56872806, 0.5668377 ,\n",
      "       0.56521739, 0.56764785, 0.56764785, 0.56872806, 0.57709965,\n",
      "       0.57520929, 0.57574939, 0.58520119, 0.58196057, 0.58223062,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "       0.62327842, 0.62219822, 0.62624899, 0.56008642, 0.55873616,\n",
      "       0.56116662, 0.57115852, 0.56872806, 0.56845801, 0.5779098 ,\n",
      "       0.57709965, 0.57844991, 0.5868215 , 0.58925196, 0.58925196,\n",
      "       0.62111801, 0.62516878, 0.62624899, 0.56656765, 0.56413719,\n",
      "       0.56305698, 0.56845801, 0.56926816, 0.57331893, 0.57493924,\n",
      "       0.57520929, 0.5768296 , 0.59222252, 0.5873616 , 0.5879017 ,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan]), 'split1_test_score': array([0.60977586, 0.61463678, 0.61490683, 0.56494734, 0.56305698,\n",
      "       0.56386713, 0.56521739, 0.5673778 , 0.56710775, 0.57520929,\n",
      "       0.57520929, 0.5773697 , 0.59573319, 0.59033216, 0.59195247,\n",
      "       0.60923575, 0.61031596, 0.61193627, 0.56116662, 0.56386713,\n",
      "       0.56386713, 0.56089657, 0.56575749, 0.56521739, 0.57601944,\n",
      "       0.57169862, 0.57385903, 0.59168242, 0.59384283, 0.59087227,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "       0.5773697 , 0.58196057, 0.57953011, 0.5662976 , 0.56278693,\n",
      "       0.56386713, 0.56575749, 0.56872806, 0.5668377 , 0.57304888,\n",
      "       0.57277883, 0.57493924, 0.58142047, 0.58223062, 0.57763975,\n",
      "       0.58142047, 0.57980016, 0.57763975, 0.56467729, 0.56035647,\n",
      "       0.55981637, 0.56602755, 0.56656765, 0.56467729, 0.57331893,\n",
      "       0.57115852, 0.57277883, 0.57709965, 0.5779098 , 0.57926006,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "       0.6073454 , 0.6084256 , 0.61355658, 0.56440724, 0.56305698,\n",
      "       0.56170672, 0.56170672, 0.5679179 , 0.56764785, 0.57601944,\n",
      "       0.5762895 , 0.57817985, 0.59033216, 0.59033216, 0.59519309,\n",
      "       0.60977586, 0.61301647, 0.61166622, 0.56116662, 0.5668377 ,\n",
      "       0.56035647, 0.56602755, 0.5668377 , 0.56764785, 0.57655955,\n",
      "       0.57547934, 0.57385903, 0.59195247, 0.58952201, 0.59114232,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "       0.6078855 , 0.61274642, 0.61058601, 0.55792601, 0.56170672,\n",
      "       0.56413719, 0.56548744, 0.5673778 , 0.5673778 , 0.57466919,\n",
      "       0.57655955, 0.57493924, 0.59276262, 0.59195247, 0.59060221,\n",
      "       0.60518499, 0.61112611, 0.61355658, 0.56170672, 0.56602755,\n",
      "       0.56494734, 0.56953821, 0.56953821, 0.57223873, 0.57196867,\n",
      "       0.57358898, 0.57547934, 0.59303268, 0.58385093, 0.59006211,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "       0.57844991, 0.57980016, 0.58007021, 0.55657575, 0.56197678,\n",
      "       0.56305698, 0.57007831, 0.5679179 , 0.56764785, 0.57520929,\n",
      "       0.57223873, 0.57412908, 0.57817985, 0.58088037, 0.57871996,\n",
      "       0.58115042, 0.58304078, 0.58034026, 0.56413719, 0.56305698,\n",
      "       0.56656765, 0.56953821, 0.56548744, 0.5679179 , 0.57304888,\n",
      "       0.57250878, 0.57520929, 0.58250068, 0.57763975, 0.57844991,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "       0.6089657 , 0.61247637, 0.61166622, 0.56278693, 0.56035647,\n",
      "       0.56305698, 0.56278693, 0.56494734, 0.56764785, 0.5779098 ,\n",
      "       0.57655955, 0.57763975, 0.59087227, 0.59357278, 0.58817175,\n",
      "       0.61382663, 0.61274642, 0.61328652, 0.55954631, 0.5679179 ,\n",
      "       0.56197678, 0.56656765, 0.56548744, 0.57007831, 0.57223873,\n",
      "       0.57655955, 0.57331893, 0.59249257, 0.59519309, 0.58763165,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "       0.61328652, 0.61355658, 0.61382663, 0.56359708, 0.56494734,\n",
      "       0.56575749, 0.5668377 , 0.57061842, 0.5679179 , 0.56926816,\n",
      "       0.5779098 , 0.5773697 , 0.58979206, 0.58979206, 0.59114232,\n",
      "       0.60761545, 0.61328652, 0.61193627, 0.56548744, 0.56602755,\n",
      "       0.5668377 , 0.56818796, 0.56332703, 0.5668377 , 0.57574939,\n",
      "       0.57926006, 0.57439914, 0.59411288, 0.59357278, 0.59357278,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "       0.57574939, 0.58061032, 0.57980016, 0.56116662, 0.55927626,\n",
      "       0.56548744, 0.56710775, 0.56872806, 0.56980826, 0.5762895 ,\n",
      "       0.57899001, 0.57493924, 0.57520929, 0.58115042, 0.58223062,\n",
      "       0.58007021, 0.58088037, 0.58412098, 0.56386713, 0.56440724,\n",
      "       0.56656765, 0.56251688, 0.56521739, 0.56899811, 0.57196867,\n",
      "       0.57547934, 0.57709965, 0.57601944, 0.58007021, 0.58007021,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "       0.61463678, 0.61409668, 0.61490683, 0.56305698, 0.56251688,\n",
      "       0.56467729, 0.56386713, 0.5668377 , 0.56656765, 0.57601944,\n",
      "       0.57331893, 0.5779098 , 0.58898191, 0.59087227, 0.59276262,\n",
      "       0.61112611, 0.61058601, 0.61031596, 0.56116662, 0.56494734,\n",
      "       0.56197678, 0.56710775, 0.56575749, 0.56872806, 0.5768296 ,\n",
      "       0.57601944, 0.57763975, 0.58817175, 0.58898191, 0.59114232,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan]), 'split2_test_score': array([0.61723393, 0.61750405, 0.62236629, 0.56726094, 0.56320908,\n",
      "       0.56185845, 0.57104268, 0.56969206, 0.56834144, 0.57725554,\n",
      "       0.57914641, 0.5813074 , 0.59778498, 0.59562399, 0.59967585,\n",
      "       0.61102107, 0.61426256, 0.61777418, 0.56645057, 0.56672069,\n",
      "       0.56699082, 0.56537007, 0.56564019, 0.56861156, 0.57887628,\n",
      "       0.58076715, 0.5802269 , 0.59886548, 0.60075635, 0.60075635,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "       0.57995678, 0.57887628, 0.58265802, 0.56266883, 0.56131821,\n",
      "       0.55834684, 0.56861156, 0.57050243, 0.56888169, 0.57752566,\n",
      "       0.57779579, 0.57725554, 0.57563479, 0.58157753, 0.58184765,\n",
      "       0.58157753, 0.5802269 , 0.58454889, 0.56888169, 0.56509995,\n",
      "       0.56591032, 0.56861156, 0.57104268, 0.56888169, 0.57671529,\n",
      "       0.5813074 , 0.58292815, 0.58427877, 0.58562939, 0.5813074 ,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "       0.61156132, 0.61345219, 0.6169638 , 0.56509995, 0.55726634,\n",
      "       0.56726094, 0.56564019, 0.56996218, 0.56996218, 0.57779579,\n",
      "       0.57860616, 0.58265802, 0.59589411, 0.59562399, 0.5991356 ,\n",
      "       0.61723393, 0.61615343, 0.61831442, 0.56564019, 0.56374932,\n",
      "       0.56509995, 0.56888169, 0.5713128 , 0.56996218, 0.57887628,\n",
      "       0.57725554, 0.58049703, 0.59670448, 0.5991356 , 0.60183684,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "       0.61372231, 0.61615343, 0.6180443 , 0.57293355, 0.56104808,\n",
      "       0.56645057, 0.57077256, 0.56942193, 0.56915181, 0.58049703,\n",
      "       0.58427877, 0.5802269 , 0.59373312, 0.59535386, 0.59697461,\n",
      "       0.6091302 , 0.61507293, 0.61858455, 0.56672069, 0.5713128 ,\n",
      "       0.57077256, 0.57725554, 0.57779579, 0.57671529, 0.58265802,\n",
      "       0.58562939, 0.58562939, 0.59778498, 0.59319287, 0.59319287,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "       0.58076715, 0.58076715, 0.58292815, 0.55645597, 0.56645057,\n",
      "       0.56509995, 0.57212318, 0.56699082, 0.57293355, 0.57698541,\n",
      "       0.57995678, 0.58076715, 0.57833603, 0.58427877, 0.57833603,\n",
      "       0.589141  , 0.58887088, 0.58806051, 0.5713128 , 0.57077256,\n",
      "       0.56942193, 0.57266343, 0.57374392, 0.57671529, 0.5823879 ,\n",
      "       0.58698001, 0.58319827, 0.58887088, 0.58887088, 0.58508914,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "       0.61723393, 0.61993517, 0.61966505, 0.55942734, 0.56293895,\n",
      "       0.56699082, 0.57401405, 0.56969206, 0.56942193, 0.5802269 ,\n",
      "       0.58535927, 0.58508914, 0.59373312, 0.59481361, 0.59562399,\n",
      "       0.61507293, 0.62344679, 0.62128579, 0.57023231, 0.56618044,\n",
      "       0.5713128 , 0.57374392, 0.57779579, 0.57455429, 0.58184765,\n",
      "       0.58670989, 0.58562939, 0.59481361, 0.59454349, 0.59562399,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "       0.61858455, 0.61453269, 0.61399244, 0.55834684, 0.56591032,\n",
      "       0.56077796, 0.56861156, 0.56969206, 0.56834144, 0.58103728,\n",
      "       0.58211777, 0.5834684 , 0.5923825 , 0.59697461, 0.59265262,\n",
      "       0.61291194, 0.61966505, 0.61561318, 0.57077256, 0.57482442,\n",
      "       0.5713128 , 0.57428417, 0.57590492, 0.57590492, 0.58265802,\n",
      "       0.58400864, 0.58698001, 0.59886548, 0.59265262, 0.6002161 ,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "       0.57779579, 0.58292815, 0.5834684 , 0.56320908, 0.56374932,\n",
      "       0.5623987 , 0.56726094, 0.56834144, 0.56564019, 0.58076715,\n",
      "       0.57671529, 0.57968666, 0.58373852, 0.57779579, 0.57968666,\n",
      "       0.58725014, 0.59427337, 0.59049163, 0.57482442, 0.56969206,\n",
      "       0.5713128 , 0.57482442, 0.57671529, 0.57590492, 0.58211777,\n",
      "       0.58049703, 0.58616964, 0.58616964, 0.58698001, 0.59103187,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "       0.62047542, 0.61831442, 0.62317666, 0.56212858, 0.56942193,\n",
      "       0.5645597 , 0.56753106, 0.56726094, 0.57104268, 0.57833603,\n",
      "       0.58157753, 0.57995678, 0.59427337, 0.59643436, 0.59751486,\n",
      "       0.61480281, 0.61966505, 0.62182604, 0.56969206, 0.57104268,\n",
      "       0.57293355, 0.57050243, 0.57482442, 0.57590492, 0.5802269 ,\n",
      "       0.58643976, 0.58616964, 0.59697461, 0.59427337, 0.59778498,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan]), 'mean_test_score': array([0.61361214, 0.6154125 , 0.61829349, 0.56400822, 0.56211749,\n",
      "       0.56265747, 0.56796931, 0.56706902, 0.56778903, 0.57580136,\n",
      "       0.57796194, 0.57859225, 0.59488683, 0.59182605, 0.59551712,\n",
      "       0.60947079, 0.61190154, 0.61559256, 0.56346804, 0.56283794,\n",
      "       0.56346809, 0.56256777, 0.56490824, 0.56607873, 0.57544143,\n",
      "       0.57616174, 0.57634173, 0.5937167 , 0.59497711, 0.59380689,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "       0.57886218, 0.57976225, 0.58012266, 0.56238749, 0.56184727,\n",
      "       0.56103685, 0.56706892, 0.56814929, 0.56823916, 0.57427109,\n",
      "       0.57562137, 0.57517124, 0.57832169, 0.5808427 , 0.5796725 ,\n",
      "       0.5803026 , 0.57922227, 0.58111302, 0.56445845, 0.56193763,\n",
      "       0.56184768, 0.56436841, 0.56769926, 0.56535862, 0.57544124,\n",
      "       0.57562169, 0.576522  , 0.57931265, 0.58057302, 0.57922237,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "       0.61226137, 0.61397187, 0.61676271, 0.56355794, 0.56166687,\n",
      "       0.56472835, 0.56481822, 0.56769916, 0.56742911, 0.57679159,\n",
      "       0.57733177, 0.57976259, 0.5924562 , 0.5920961 , 0.59488695,\n",
      "       0.61514243, 0.61712271, 0.617663  , 0.56085747, 0.56373785,\n",
      "       0.56202764, 0.56562867, 0.56715918, 0.56742911, 0.57688171,\n",
      "       0.5755313 , 0.57688185, 0.59479671, 0.59317663, 0.59497721,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "       0.61361182, 0.61631255, 0.61577262, 0.56400873, 0.56238735,\n",
      "       0.56409816, 0.5675192 , 0.56904937, 0.56877929, 0.57589166,\n",
      "       0.57832247, 0.57679181, 0.59182588, 0.59182603, 0.59182617,\n",
      "       0.60974067, 0.61433208, 0.61739298, 0.56427822, 0.56643904,\n",
      "       0.56661903, 0.57202064, 0.57256079, 0.57238066, 0.57598188,\n",
      "       0.57949281, 0.57877267, 0.59407668, 0.58813513, 0.59002549,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "       0.57940236, 0.58102266, 0.58102286, 0.55887627, 0.56355806,\n",
      "       0.56463814, 0.56976975, 0.56715879, 0.56832955, 0.57571131,\n",
      "       0.57661175, 0.57715193, 0.57895205, 0.58237324, 0.57949215,\n",
      "       0.58399398, 0.58426401, 0.58246359, 0.5657189 , 0.56553882,\n",
      "       0.56562872, 0.5697698 , 0.56796955, 0.57202059, 0.57706206,\n",
      "       0.57805266, 0.5779623 , 0.58363389, 0.58183355, 0.58111307,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "       0.61496239, 0.6184733 , 0.61721304, 0.56211715, 0.56130731,\n",
      "       0.56499838, 0.56895976, 0.5675191 , 0.56895935, 0.57670179,\n",
      "       0.58012291, 0.5796728 , 0.59056564, 0.59200601, 0.5904758 ,\n",
      "       0.61541228, 0.62027396, 0.62009373, 0.56346838, 0.56670863,\n",
      "       0.5657189 , 0.57040001, 0.57076045, 0.57130026, 0.57697199,\n",
      "       0.58030306, 0.57814255, 0.59092581, 0.59218602, 0.59029576,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "       0.61748299, 0.61514218, 0.61505212, 0.56121688, 0.56472823,\n",
      "       0.56319748, 0.56760902, 0.56949948, 0.56850917, 0.57544163,\n",
      "       0.57904241, 0.5794026 , 0.59056552, 0.59101602, 0.59065556,\n",
      "       0.61028112, 0.61586278, 0.61496225, 0.56769923, 0.56814968,\n",
      "       0.5677893 , 0.56958991, 0.56995012, 0.57076028, 0.57742215,\n",
      "       0.57931263, 0.57868278, 0.59416679, 0.5913757 , 0.59299669,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "       0.57778178, 0.58192303, 0.58210311, 0.56283763, 0.56049723,\n",
      "       0.56373773, 0.56652869, 0.56886924, 0.56796882, 0.57796208,\n",
      "       0.57634141, 0.57688178, 0.57985271, 0.57904202, 0.58201275,\n",
      "       0.58309364, 0.5858848 , 0.58498429, 0.56913987, 0.566979  ,\n",
      "       0.56769928, 0.56832972, 0.56986018, 0.57121036, 0.57706203,\n",
      "       0.57706189, 0.57967289, 0.58246342, 0.5830036 , 0.58444424,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "       0.61946354, 0.61820311, 0.62144416, 0.56175733, 0.56355832,\n",
      "       0.56346787, 0.56751891, 0.5676089 , 0.56868944, 0.57742176,\n",
      "       0.57733204, 0.57877216, 0.59002559, 0.59218619, 0.59317648,\n",
      "       0.61568231, 0.61847328, 0.61946366, 0.56580878, 0.56670907,\n",
      "       0.5659891 , 0.5686894 , 0.56995002, 0.57265064, 0.57733191,\n",
      "       0.57922283, 0.580213  , 0.59245629, 0.59020562, 0.59227633,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan]), 'std_test_score': array([0.00304852, 0.00149529, 0.00308372, 0.00311094, 0.00143753,\n",
      "       0.00086993, 0.002389  , 0.00227825, 0.00051183, 0.00103416,\n",
      "       0.00195271, 0.00192306, 0.00277711, 0.00270583, 0.00318092,\n",
      "       0.00118159, 0.00170194, 0.00260145, 0.00221027, 0.00366343,\n",
      "       0.00305228, 0.00199375, 0.00111911, 0.00182127, 0.00306786,\n",
      "       0.00370358, 0.00278241, 0.00366734, 0.00433058, 0.00493376,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "       0.001093  , 0.00156416, 0.0018756 , 0.00331342, 0.0006662 ,\n",
      "       0.00225583, 0.00117658, 0.00219608, 0.00099213, 0.00232498,\n",
      "       0.00210195, 0.00161546, 0.00237988, 0.0015245 , 0.00172082,\n",
      "       0.00169318, 0.00113249, 0.00282077, 0.00370414, 0.0022361 ,\n",
      "       0.00287272, 0.00430481, 0.00240493, 0.00264271, 0.00151073,\n",
      "       0.00423249, 0.00455124, 0.00351849, 0.0035771 , 0.00171801,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "       0.00432807, 0.00475488, 0.00253969, 0.0017144 , 0.00318118,\n",
      "       0.00229372, 0.00228028, 0.00194322, 0.00216309, 0.00074352,\n",
      "       0.00095991, 0.00205034, 0.00245337, 0.00249459, 0.0036005 ,\n",
      "       0.00382538, 0.00381058, 0.00465327, 0.00403721, 0.00253572,\n",
      "       0.00217524, 0.00283299, 0.00326809, 0.00216309, 0.00151428,\n",
      "       0.0013871 , 0.00274213, 0.0020499 , 0.00424954, 0.00486176,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "       0.00463107, 0.00297882, 0.00367688, 0.00644798, 0.00145337,\n",
      "       0.00193686, 0.00232413, 0.00124101, 0.0010266 , 0.00337381,\n",
      "       0.00432704, 0.00243148, 0.00204965, 0.00293345, 0.00380377,\n",
      "       0.00399233, 0.00237374, 0.00277687, 0.00204898, 0.00382251,\n",
      "       0.00295555, 0.00370328, 0.00371645, 0.00348272, 0.00475282,\n",
      "       0.00491823, 0.00484968, 0.00270432, 0.00385285, 0.00260123,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "       0.00098993, 0.00111719, 0.00134724, 0.00333848, 0.00204828,\n",
      "       0.00114982, 0.00205913, 0.00056389, 0.00351407, 0.00090764,\n",
      "       0.00323364, 0.00274216, 0.00098368, 0.00141774, 0.0013725 ,\n",
      "       0.00364617, 0.00337484, 0.00399582, 0.00407804, 0.00370245,\n",
      "       0.00354322, 0.002274  , 0.00409647, 0.00361583, 0.003924  ,\n",
      "       0.00637391, 0.00370403, 0.00389663, 0.00500659, 0.00286504,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "       0.00428163, 0.00442218, 0.00393097, 0.00198026, 0.001159  ,\n",
      "       0.00160639, 0.00465119, 0.00195744, 0.00094068, 0.00347794,\n",
      "       0.00378224, 0.0038694 , 0.00272006, 0.00313435, 0.00364699,\n",
      "       0.00145317, 0.0053446 , 0.00514098, 0.00480311, 0.00085736,\n",
      "       0.00403007, 0.00294999, 0.00517708, 0.0023246 , 0.00392412,\n",
      "       0.00455172, 0.00536696, 0.00397165, 0.00380255, 0.00376762,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "       0.00307692, 0.00160251, 0.00161728, 0.00217121, 0.00106594,\n",
      "       0.00203531, 0.0007424 , 0.00100153, 0.00056385, 0.00482206,\n",
      "       0.00219962, 0.00287495, 0.00128952, 0.00445045, 0.0018614 ,\n",
      "       0.00216242, 0.00274453, 0.00225249, 0.00224199, 0.00482292,\n",
      "       0.00257781, 0.00340788, 0.0051566 , 0.0038012 , 0.0037819 ,\n",
      "       0.003813  , 0.00586807, 0.00381465, 0.00248505, 0.00614333,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "       0.00165375, 0.00097093, 0.00163776, 0.00124084, 0.00232324,\n",
      "       0.00129398, 0.00092934, 0.00049868, 0.00173641, 0.0019957 ,\n",
      "       0.00233025, 0.00203178, 0.00352301, 0.00149909, 0.00181685,\n",
      "       0.00303872, 0.00596838, 0.00418899, 0.00448276, 0.00215983,\n",
      "       0.00261393, 0.00504761, 0.00494781, 0.00332138, 0.00414344,\n",
      "       0.00243151, 0.00462685, 0.0045737 , 0.00291573, 0.00474093,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "       0.00359976, 0.00330838, 0.00478973, 0.00124082, 0.00442417,\n",
      "       0.00162794, 0.00297671, 0.00081001, 0.00183424, 0.00100674,\n",
      "       0.00337556, 0.00086619, 0.00313045, 0.00307588, 0.00338598,\n",
      "       0.00412631, 0.00601274, 0.0067157 , 0.00352162, 0.00308212,\n",
      "       0.00493023, 0.0013955 , 0.00373283, 0.0029678 , 0.00218771,\n",
      "       0.00511385, 0.00422495, 0.00359755, 0.00295141, 0.00411374,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan]), 'rank_test_score': array([ 30,  21,   8, 240, 257, 253, 195, 212, 198, 153, 128, 121,  41,\n",
      "        58,  37,  36,  33,  20, 248, 251, 247, 254, 230, 220, 159, 150,\n",
      "       148,  46,  39,  45, 316, 315, 314, 311, 310, 290, 291, 292, 318,\n",
      "       293, 309, 312, 313, 277, 279, 117, 101,  98, 255, 262, 267, 213,\n",
      "       193, 191, 162, 156, 161, 123,  92, 104,  95, 113,  89, 235, 260,\n",
      "       261, 236, 200, 228, 160, 155, 147, 109,  93, 112, 282, 281, 280,\n",
      "       288, 274, 383, 320, 381, 319, 365, 364, 372, 384, 394, 385,  32,\n",
      "        29,  15, 245, 264, 232, 231, 202, 208, 144, 134, 100,  51,  55,\n",
      "        40,  23,  14,  10, 268, 241, 259, 226, 210, 208, 142, 157, 140,\n",
      "        42,  47,  38, 403, 402, 401, 400, 399, 361, 350, 337, 336, 335,\n",
      "       334, 333, 332, 331, 330,  31,  16,  18, 239, 256, 238, 205, 181,\n",
      "       185, 152, 122, 143,  60,  59,  57,  35,  28,  12, 237, 219, 217,\n",
      "       166, 164, 165, 151, 105, 118,  44,  72,  71, 329, 328, 327, 326,\n",
      "       325, 324, 360, 351, 349, 348, 347, 346, 345, 343, 342, 108,  91,\n",
      "        90, 270, 244, 234, 177, 211, 190, 154, 146, 135, 116,  83, 106,\n",
      "        77,  76,  81, 223, 227, 225, 176, 194, 167, 136, 125, 126,  78,\n",
      "        87,  88, 352, 353, 354, 355, 356, 357, 358, 359, 341, 340, 339,\n",
      "       338, 321, 322, 323,  26,   6,  13, 258, 265, 229, 182, 206, 183,\n",
      "       145,  97, 103,  65,  56,  67,  22,   2,   3, 246, 216, 223, 172,\n",
      "       170, 168, 139,  94, 124,  63,  54,  68, 362, 363, 386, 387, 388,\n",
      "       389, 390, 391, 392, 393, 395, 404, 396, 397, 398,  11,  24,  25,\n",
      "       266, 233, 250, 203, 179, 188, 158, 114, 107,  66,  62,  64,  34,\n",
      "        17,  27, 201, 192, 197, 178, 173, 171, 130, 110, 120,  43,  61,\n",
      "        49, 366, 367, 368, 369, 370, 371, 373, 382, 374, 375, 376, 377,\n",
      "       378, 379, 380, 129,  86,  84, 252, 269, 242, 218, 184, 196, 127,\n",
      "       149, 141,  99, 115,  85,  79,  73,  74, 180, 214, 199, 189, 175,\n",
      "       169, 137, 138, 102,  82,  80,  75, 308, 307, 306, 305, 304, 303,\n",
      "       302, 301, 300, 299, 298, 297, 296, 295, 294,   5,   9,   1, 263,\n",
      "       243, 249, 207, 204, 186, 131, 132, 119,  70,  53,  48,  19,   7,\n",
      "         4, 222, 215, 221, 187, 174, 163, 133, 111,  96,  50,  69,  52,\n",
      "       317, 272, 276, 284, 285, 289, 287, 271, 278, 286, 283, 275, 273,\n",
      "       344, 405])}\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "params = {\n",
    "    'RF__n_estimators': [50, 100, 150],\n",
    "    'RF__criterion': [\"gini\", \"log_loss\", \"entropy\"],\n",
    "    'RF__max_depth': [None, 5, 10],\n",
    "    'RF__max_features': [\"sqrt\", \"log2\", \"None\"],\n",
    "    'RF__max_leaf_nodes': [None, 8, 10, 16, 32],\n",
    "}\n",
    "\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy=\"mean\")\n",
    "pipe = Pipeline(steps=[(\"imputer\", imp), (\"RF\", rf)])\n",
    "\n",
    "grid = GridSearchCV(pipe, params, scoring=\"accuracy\", cv=StratifiedKFold(3))\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(grid.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "bf6ece41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.618421052631579"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9a17eb84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 7 ... 0 1 7]\n",
      "[0 0 0 ... 0 3 7]\n"
     ]
    }
   ],
   "source": [
    "print(grid.predict(X_test))\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769b6f81",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59b0849b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34539576598185423"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm = SVC()\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "svm.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7f6c675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_examples, train_score, valid_score = learning_curve(svm, X_train, y_train, cv=StratifiedKFold(3))\n",
    "plt.plot(training_examples, np.mean(train_score, axis=1), label=\"train_score\")\n",
    "plt.plot(training_examples, np.mean(valid_score, axis=1), label=\"valid_score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab5ee89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = SimpleImputer(missing_values=np.nan, strategy=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d05423c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean_fit_time': array([13.19330668, 17.91954517, 17.77461116, 11.37800129, 10.17008464,\n",
      "       21.50481876, 31.86023172, 21.16573771]), 'std_fit_time': array([0.13133679, 0.10597168, 0.06772175, 0.95809099, 0.13336032,\n",
      "       1.74820163, 3.75132773, 0.04419573]), 'mean_score_time': array([ 2.31475504,  7.93599423,  7.90270933,  2.41259742,  3.43588726,\n",
      "       10.9155941 , 11.24243339,  3.65832551]), 'std_score_time': array([0.00772791, 0.0266227 , 0.00689637, 0.03642538, 0.3196999 ,\n",
      "       1.56441187, 1.53414897, 0.00835811]), 'param_SVM__kernel': masked_array(data=['sigmoid', 'rbf', 'rbf', 'sigmoid', 'sigmoid', 'rbf',\n",
      "                   'rbf', 'sigmoid'],\n",
      "             mask=[False, False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_SVM__gamma': masked_array(data=['auto', 'scale', 'scale', 'auto', 'scale', 'auto',\n",
      "                   'auto', 'scale'],\n",
      "             mask=[False, False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_SVM__class_weight': masked_array(data=['balanced', 'balanced', 'balanced', 'balanced',\n",
      "                   'balanced', 'balanced', 'balanced', 'balanced'],\n",
      "             mask=[False, False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_SVM__C': masked_array(data=[0.01, 2, 1, 5, 1, 5, 0.01, 0.01],\n",
      "             mask=[False, False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'SVM__kernel': 'sigmoid', 'SVM__gamma': 'auto', 'SVM__class_weight': 'balanced', 'SVM__C': 0.01}, {'SVM__kernel': 'rbf', 'SVM__gamma': 'scale', 'SVM__class_weight': 'balanced', 'SVM__C': 2}, {'SVM__kernel': 'rbf', 'SVM__gamma': 'scale', 'SVM__class_weight': 'balanced', 'SVM__C': 1}, {'SVM__kernel': 'sigmoid', 'SVM__gamma': 'auto', 'SVM__class_weight': 'balanced', 'SVM__C': 5}, {'SVM__kernel': 'sigmoid', 'SVM__gamma': 'scale', 'SVM__class_weight': 'balanced', 'SVM__C': 1}, {'SVM__kernel': 'rbf', 'SVM__gamma': 'auto', 'SVM__class_weight': 'balanced', 'SVM__C': 5}, {'SVM__kernel': 'rbf', 'SVM__gamma': 'auto', 'SVM__class_weight': 'balanced', 'SVM__C': 0.01}, {'SVM__kernel': 'sigmoid', 'SVM__gamma': 'scale', 'SVM__class_weight': 'balanced', 'SVM__C': 0.01}], 'split0_test_score': array([0.03621513, 0.11342272, 0.11265218, 0.01849283, 0.10987826,\n",
      "       0.24934505, 0.00924642, 0.00924642]), 'split1_test_score': array([0.09832023, 0.1530282 , 0.15241177, 0.02866389, 0.26799199,\n",
      "       0.2417938 , 0.345816  , 0.00924642]), 'split2_test_score': array([0.01803329, 0.1877312 , 0.19004316, 0.03483354, 0.17786683,\n",
      "       0.23936498, 0.3474106 , 0.32352035]), 'mean_test_score': array([0.05085622, 0.15139404, 0.15170237, 0.02733009, 0.18524569,\n",
      "       0.24350128, 0.23415767, 0.11400439]), 'std_test_score': array([0.03437314, 0.03035831, 0.03159872, 0.0067374 , 0.06476019,\n",
      "       0.00424947, 0.15903761, 0.14815015]), 'rank_test_score': array([7, 5, 4, 8, 3, 1, 2, 6])}\n"
     ]
    }
   ],
   "source": [
    "# Non-linear\n",
    "svm = SVC()\n",
    "\n",
    "params = {\n",
    "    'SVM__C': [0.01, 0.1, 1, 2, 5],\n",
    "    'SVM__kernel': [\"rbf\", \"sigmoid\"],\n",
    "    'SVM__gamma': [\"scale\", \"auto\"],\n",
    "    'SVM__class_weight': [\"balanced\"],\n",
    "}\n",
    "\n",
    "# imp = SimpleImputer(missing_values=np.nan, strategy=\"mean\")\n",
    "pipe = Pipeline(steps=[(\"imputer\", imp), (\"SVM\", svm)])\n",
    "\n",
    "rscv = RandomizedSearchCV(pipe, params, scoring=\"accuracy\", cv=StratifiedKFold(3), n_iter=8)\n",
    "rscv.fit(X_train, y_train)\n",
    "\n",
    "print(rscv.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02852ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('imputer', SimpleImputer()),\n",
      "                ('SVM', SVC(C=5, class_weight='balanced', gamma='auto'))])\n",
      "0.24538533736573157\n"
     ]
    }
   ],
   "source": [
    "print(rscv.best_estimator_)\n",
    "print(rscv.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fbaf411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_examples, train_score, valid_score = learning_curve(rscv.best_estimator_[1], X_train, y_train)\n",
    "plt.plot(training_examples, np.mean(train_score, axis=1), label=\"train_score\")\n",
    "plt.plot(training_examples, np.mean(valid_score, axis=1), label=\"valid_score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4da4fc4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wilso\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\wilso\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\wilso\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\wilso\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\wilso\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\wilso\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\wilso\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\wilso\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\wilso\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\wilso\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\wilso\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\wilso\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\wilso\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\wilso\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\wilso\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\wilso\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\wilso\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\wilso\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\wilso\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\wilso\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\wilso\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\wilso\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\wilso\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\wilso\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\wilso\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\wilso\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\wilso\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\wilso\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\wilso\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\wilso\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean_fit_time': array([ 40.58743636, 415.12259356, 208.00126537, 567.4912502 ,\n",
      "       434.60804152, 476.03067493, 468.46382332, 477.04781508,\n",
      "       486.28958686, 474.72887969]), 'std_fit_time': array([ 5.16882652, 13.90603618,  9.04242661, 58.86941407,  2.74913873,\n",
      "        2.70186177,  2.87085429,  0.81638131,  2.91632008,  2.49610405]), 'mean_score_time': array([0.00230575, 0.00267212, 0.0030334 , 0.00232712, 0.00198285,\n",
      "       0.00219552, 0.00210436, 0.00235105, 0.00232665, 0.00262459]), 'std_score_time': array([4.65518162e-04, 4.80025454e-04, 3.72792945e-05, 4.69572250e-04,\n",
      "       1.63014314e-05, 6.05227972e-04, 1.75409798e-04, 5.31401194e-04,\n",
      "       4.69066336e-04, 4.93213318e-04]), 'param_SVM__C': masked_array(data=[0.01, 0.01, 0.1, 0.1, 1, 1, 2, 2, 5, 5],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_SVM__class_weight': masked_array(data=['balanced', 'balanced', 'balanced', 'balanced',\n",
      "                   'balanced', 'balanced', 'balanced', 'balanced',\n",
      "                   'balanced', 'balanced'],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_SVM__loss': masked_array(data=['hinge', 'squared_hinge', 'hinge', 'squared_hinge',\n",
      "                   'hinge', 'squared_hinge', 'hinge', 'squared_hinge',\n",
      "                   'hinge', 'squared_hinge'],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_SVM__max_iter': masked_array(data=[100000, 100000, 100000, 100000, 100000, 100000, 100000,\n",
      "                   100000, 100000, 100000],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_SVM__penalty': masked_array(data=['l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
      "                   'l2'],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_SVM__tol': masked_array(data=[0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
      "                   0.0001, 0.0001, 0.0001],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'SVM__C': 0.01, 'SVM__class_weight': 'balanced', 'SVM__loss': 'hinge', 'SVM__max_iter': 100000, 'SVM__penalty': 'l2', 'SVM__tol': 0.0001}, {'SVM__C': 0.01, 'SVM__class_weight': 'balanced', 'SVM__loss': 'squared_hinge', 'SVM__max_iter': 100000, 'SVM__penalty': 'l2', 'SVM__tol': 0.0001}, {'SVM__C': 0.1, 'SVM__class_weight': 'balanced', 'SVM__loss': 'hinge', 'SVM__max_iter': 100000, 'SVM__penalty': 'l2', 'SVM__tol': 0.0001}, {'SVM__C': 0.1, 'SVM__class_weight': 'balanced', 'SVM__loss': 'squared_hinge', 'SVM__max_iter': 100000, 'SVM__penalty': 'l2', 'SVM__tol': 0.0001}, {'SVM__C': 1, 'SVM__class_weight': 'balanced', 'SVM__loss': 'hinge', 'SVM__max_iter': 100000, 'SVM__penalty': 'l2', 'SVM__tol': 0.0001}, {'SVM__C': 1, 'SVM__class_weight': 'balanced', 'SVM__loss': 'squared_hinge', 'SVM__max_iter': 100000, 'SVM__penalty': 'l2', 'SVM__tol': 0.0001}, {'SVM__C': 2, 'SVM__class_weight': 'balanced', 'SVM__loss': 'hinge', 'SVM__max_iter': 100000, 'SVM__penalty': 'l2', 'SVM__tol': 0.0001}, {'SVM__C': 2, 'SVM__class_weight': 'balanced', 'SVM__loss': 'squared_hinge', 'SVM__max_iter': 100000, 'SVM__penalty': 'l2', 'SVM__tol': 0.0001}, {'SVM__C': 5, 'SVM__class_weight': 'balanced', 'SVM__loss': 'hinge', 'SVM__max_iter': 100000, 'SVM__penalty': 'l2', 'SVM__tol': 0.0001}, {'SVM__C': 5, 'SVM__class_weight': 'balanced', 'SVM__loss': 'squared_hinge', 'SVM__max_iter': 100000, 'SVM__penalty': 'l2', 'SVM__tol': 0.0001}], 'split0_test_score': array([0.1639698 , 0.18261674, 0.02958853, 0.06040992, 0.03575281,\n",
      "       0.25643397, 0.04145477, 0.0776699 , 0.370319  , 0.1995685 ]), 'split1_test_score': array([0.31823085, 0.1818462 , 0.04068423, 0.05963939, 0.02558175,\n",
      "       0.35167206, 0.0708892 , 0.34442903, 0.06595777, 0.27677608]), 'split2_test_score': array([0.19713317, 0.16445746, 0.04593095, 0.04963009, 0.02666461,\n",
      "       0.0527127 , 0.01864982, 0.08215166, 0.18387793, 0.36313194]), 'mean_test_score': array([0.22644461, 0.1763068 , 0.03873457, 0.0565598 , 0.02933306,\n",
      "       0.22027291, 0.04366459, 0.16808353, 0.20671823, 0.27982551]), 'std_test_score': array([0.06629977, 0.00838465, 0.00681271, 0.00491013, 0.00456093,\n",
      "       0.12469935, 0.02138381, 0.12470852, 0.12530017, 0.0668093 ]), 'rank_test_score': array([ 2,  5,  9,  7, 10,  3,  8,  6,  4,  1])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wilso\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Linear\n",
    "svm_l2 = LinearSVC()\n",
    "\n",
    "params = {\n",
    "    'SVM__C': [0.01, 0.1, 1, 2, 5],\n",
    "    'SVM__penalty': [\"l2\"],\n",
    "    'SVM__loss': [\"hinge\", \"squared_hinge\"],\n",
    "    'SVM__class_weight': [\"balanced\"],\n",
    "    'SVM__max_iter': [100000],\n",
    "    'SVM__tol': [1e-4],\n",
    "}\n",
    "\n",
    "# imp = SimpleImputer(missing_values=np.nan, strategy=\"mean\")\n",
    "pipe_l2 = Pipeline(steps=[(\"imputer\", imp), (\"SVM\", svm_l2)])\n",
    "\n",
    "grid_l2 = GridSearchCV(pipe_l2, params, scoring=\"accuracy\", cv=StratifiedKFold(3), error_score=\"raise\")\n",
    "grid_l2.fit(X_train, y_train)\n",
    "\n",
    "print(grid_l2.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92b1ab41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('imputer', SimpleImputer()),\n",
      "                ('SVM',\n",
      "                 LinearSVC(C=5, class_weight='balanced', max_iter=100000))])\n",
      "0.3596829700698717\n"
     ]
    }
   ],
   "source": [
    "print(grid_l2.best_estimator_)\n",
    "print(grid_l2.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ee45116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean_fit_time': array([ 1.14047273,  5.53292743, 26.96769849, 23.05977241, 11.7703774 ]), 'std_fit_time': array([ 0.16874372,  0.30930244,  3.61307085, 16.58166695,  6.64639098]), 'mean_score_time': array([0.00253542, 0.00178464, 0.00231481, 0.00298047, 0.00199278]), 'std_score_time': array([1.55313909e-03, 8.57318917e-04, 4.78342699e-04, 1.52003141e-05,\n",
      "       8.30534678e-04]), 'param_SVM__C': masked_array(data=[0.01, 0.1, 1, 2, 5],\n",
      "             mask=[False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_SVM__class_weight': masked_array(data=['balanced', 'balanced', 'balanced', 'balanced',\n",
      "                   'balanced'],\n",
      "             mask=[False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_SVM__dual': masked_array(data=[False, False, False, False, False],\n",
      "             mask=[False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_SVM__loss': masked_array(data=['squared_hinge', 'squared_hinge', 'squared_hinge',\n",
      "                   'squared_hinge', 'squared_hinge'],\n",
      "             mask=[False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_SVM__max_iter': masked_array(data=[100000, 100000, 100000, 100000, 100000],\n",
      "             mask=[False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_SVM__penalty': masked_array(data=['l1', 'l1', 'l1', 'l1', 'l1'],\n",
      "             mask=[False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_SVM__tol': masked_array(data=[0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
      "             mask=[False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'SVM__C': 0.01, 'SVM__class_weight': 'balanced', 'SVM__dual': False, 'SVM__loss': 'squared_hinge', 'SVM__max_iter': 100000, 'SVM__penalty': 'l1', 'SVM__tol': 0.0001}, {'SVM__C': 0.1, 'SVM__class_weight': 'balanced', 'SVM__dual': False, 'SVM__loss': 'squared_hinge', 'SVM__max_iter': 100000, 'SVM__penalty': 'l1', 'SVM__tol': 0.0001}, {'SVM__C': 1, 'SVM__class_weight': 'balanced', 'SVM__dual': False, 'SVM__loss': 'squared_hinge', 'SVM__max_iter': 100000, 'SVM__penalty': 'l1', 'SVM__tol': 0.0001}, {'SVM__C': 2, 'SVM__class_weight': 'balanced', 'SVM__dual': False, 'SVM__loss': 'squared_hinge', 'SVM__max_iter': 100000, 'SVM__penalty': 'l1', 'SVM__tol': 0.0001}, {'SVM__C': 5, 'SVM__class_weight': 'balanced', 'SVM__dual': False, 'SVM__loss': 'squared_hinge', 'SVM__max_iter': 100000, 'SVM__penalty': 'l1', 'SVM__tol': 0.0001}], 'split0_test_score': array([0.41624287, 0.43643088, 0.43766374, 0.43643088, 0.43535213]), 'split1_test_score': array([0.40699646, 0.42857143, 0.43196178, 0.43196178, 0.43165357]), 'split2_test_score': array([0.42123921, 0.44605425, 0.44590012, 0.4472873 , 0.44620838]), 'mean_test_score': array([0.41482618, 0.43701886, 0.43850855, 0.43855999, 0.43773803]), 'std_test_score': array([0.00590024, 0.00714943, 0.00572157, 0.0064352 , 0.00617684]), 'rank_test_score': array([5, 4, 2, 1, 3])}\n"
     ]
    }
   ],
   "source": [
    "# Linear\n",
    "svm_l1 = LinearSVC()\n",
    "\n",
    "params = {\n",
    "    'SVM__C': [0.01, 0.1, 1, 2, 5],\n",
    "    'SVM__penalty': [\"l1\"],\n",
    "    'SVM__dual': [False],\n",
    "    'SVM__loss': [\"squared_hinge\"],\n",
    "    'SVM__class_weight': [\"balanced\"],\n",
    "    'SVM__max_iter': [100000],\n",
    "    'SVM__tol': [1e-4],\n",
    "}\n",
    "\n",
    "# imp = SimpleImputer(missing_values=np.nan, strategy=\"mean\")\n",
    "pipe_l1 = Pipeline(steps=[(\"imputer\", imp), (\"SVM\", svm_l1)])\n",
    "\n",
    "grid_l1 = GridSearchCV(pipe_l1, params, scoring=\"accuracy\", cv=StratifiedKFold(3), error_score=\"raise\")\n",
    "grid_l1.fit(X_train, y_train)\n",
    "\n",
    "print(grid_l1.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cb2e279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('imputer', SimpleImputer()),\n",
      "                ('SVM',\n",
      "                 LinearSVC(C=2, class_weight='balanced', dual=False,\n",
      "                           max_iter=100000, penalty='l1'))])\n",
      "0.43320471373448743\n"
     ]
    }
   ],
   "source": [
    "print(grid_l1.best_estimator_)\n",
    "print(grid_l1.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d70c4eda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_examples, train_score, valid_score = learning_curve(LinearSVC(C=2, class_weight='balanced', dual=False,max_iter=100000, penalty='l1'), \n",
    "                                                             X_train, y_train, cv=StratifiedKFold(3))\n",
    "plt.plot(training_examples, np.mean(train_score, axis=1), label=\"train_score\")\n",
    "plt.plot(training_examples, np.mean(valid_score, axis=1), label=\"valid_score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98da1596",
   "metadata": {},
   "source": [
    "# Neural Nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8063069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.Input(shape=(8, )))\n",
    "#     for i in range(hp.Int(\"n_layers\", 1, 3)):\n",
    "    model.add(Dense(hp.Int(\"n_neurons\", 64, 208, 16), activation=\"softmax\"))\n",
    "    model.add(Dense(5, activation=\"softmax\"))\n",
    "    \n",
    "    hp_learning_rate = hp.Choice(\"learning_rate\", values=[1e-3, 1e-4, 1e-5])\n",
    "    hp_beta_1 = hp.Choice(\"beta_1\", values=[0.9, 0.85, 0.95])\n",
    "    hp_beta_2 = hp.Choice(\"beta_2\", values=[0.999, 0.99, 0.9999])\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate, beta_1=hp_beta_1, beta_2=hp_beta_2), loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "27666ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_a(act=\"softmax\", opt=tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Dense(208, activation=act, input_shape=(8, )))\n",
    "    model.add(Dense(8, activation=act))\n",
    "    model.compile(optimizer=opt, loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "81a7ced8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_b(act=\"softmax\", opt=tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Dense(96, activation=act, input_shape=(8, )))\n",
    "    model.add(Dense(8, activation=act))\n",
    "    model.compile(optimizer=opt, loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ddf4038",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_c(act=\"softmax\", opt=tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.95, beta_2=0.99)):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Dense(196, activation=act, input_shape=(8, )))\n",
    "    model.add(Dense(8, activation=act))\n",
    "    model.compile(optimizer=opt, loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "853f672b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 101 Complete [00h 01m 24s]\n",
      "val_accuracy: 0.41184258460998535\n",
      "\n",
      "Best val_accuracy So Far: 0.5853893160820007\n",
      "Total elapsed time: 02h 56m 46s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=10)\n",
    "tuner = RandomSearch(build_model, objective=\"val_accuracy\", max_trials=100, project_name=\"no_back\", overwrite=False)\n",
    "tuner.search(x=X_train, y=y_train, epochs=120, batch_size=10, validation_data=(X_test, y_test), callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e43bf94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in .\\no_back\n",
      "Showing 10 best trials\n",
      "Objective(name='val_accuracy', direction='max')\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "n_neurons: 192\n",
      "learning_rate: 0.001\n",
      "beta_1: 0.95\n",
      "beta_2: 0.999\n",
      "Score: 0.5853893160820007\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "n_neurons: 208\n",
      "learning_rate: 0.001\n",
      "beta_1: 0.9\n",
      "beta_2: 0.999\n",
      "Score: 0.5838247537612915\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "n_neurons: 96\n",
      "learning_rate: 0.001\n",
      "beta_1: 0.9\n",
      "beta_2: 0.999\n",
      "Score: 0.5831026434898376\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "n_neurons: 128\n",
      "learning_rate: 0.001\n",
      "beta_1: 0.95\n",
      "beta_2: 0.9999\n",
      "Score: 0.5826212763786316\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "n_neurons: 112\n",
      "learning_rate: 0.001\n",
      "beta_1: 0.9\n",
      "beta_2: 0.9999\n",
      "Score: 0.5797328352928162\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "n_neurons: 96\n",
      "learning_rate: 0.001\n",
      "beta_1: 0.9\n",
      "beta_2: 0.99\n",
      "Score: 0.5793717503547668\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "n_neurons: 80\n",
      "learning_rate: 0.001\n",
      "beta_1: 0.95\n",
      "beta_2: 0.999\n",
      "Score: 0.578649640083313\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "n_neurons: 160\n",
      "learning_rate: 0.001\n",
      "beta_1: 0.9\n",
      "beta_2: 0.99\n",
      "Score: 0.5781682729721069\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "n_neurons: 64\n",
      "learning_rate: 0.001\n",
      "beta_1: 0.95\n",
      "beta_2: 0.999\n",
      "Score: 0.5768443942070007\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "n_neurons: 192\n",
      "learning_rate: 0.001\n",
      "beta_1: 0.85\n",
      "beta_2: 0.99\n",
      "Score: 0.5760019421577454\n"
     ]
    }
   ],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "140491e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps = tuner.get_best_hyperparameters()[0]\n",
    "model = tuner.hypermodel.build(best_hps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "59653f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.4723 - accuracy: 0.4008 - val_loss: 1.4240 - val_accuracy: 0.4132\n",
      "Epoch 2/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.4116 - accuracy: 0.4149 - val_loss: 1.4121 - val_accuracy: 0.4132\n",
      "Epoch 3/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.4051 - accuracy: 0.4149 - val_loss: 1.4113 - val_accuracy: 0.4132\n",
      "Epoch 4/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.3962 - accuracy: 0.4150 - val_loss: 1.3879 - val_accuracy: 0.4135\n",
      "Epoch 5/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.3735 - accuracy: 0.4191 - val_loss: 1.3587 - val_accuracy: 0.4253\n",
      "Epoch 6/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.3458 - accuracy: 0.4352 - val_loss: 1.3298 - val_accuracy: 0.4455\n",
      "Epoch 7/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.3184 - accuracy: 0.4523 - val_loss: 1.3134 - val_accuracy: 0.4748\n",
      "Epoch 8/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.2946 - accuracy: 0.4592 - val_loss: 1.2828 - val_accuracy: 0.4710\n",
      "Epoch 9/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.2761 - accuracy: 0.4641 - val_loss: 1.2955 - val_accuracy: 0.4707\n",
      "Epoch 10/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.2599 - accuracy: 0.4675 - val_loss: 1.2514 - val_accuracy: 0.4757\n",
      "Epoch 11/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.2477 - accuracy: 0.4726 - val_loss: 1.2403 - val_accuracy: 0.4834\n",
      "Epoch 12/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.2366 - accuracy: 0.4840 - val_loss: 1.2468 - val_accuracy: 0.4932\n",
      "Epoch 13/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.2291 - accuracy: 0.4921 - val_loss: 1.2241 - val_accuracy: 0.4994\n",
      "Epoch 14/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.2208 - accuracy: 0.4964 - val_loss: 1.2218 - val_accuracy: 0.5036\n",
      "Epoch 15/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.2167 - accuracy: 0.4986 - val_loss: 1.2163 - val_accuracy: 0.5095\n",
      "Epoch 16/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.2153 - accuracy: 0.4980 - val_loss: 1.2077 - val_accuracy: 0.5190\n",
      "Epoch 17/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.2095 - accuracy: 0.5083 - val_loss: 1.2027 - val_accuracy: 0.5163\n",
      "Epoch 18/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.2006 - accuracy: 0.5099 - val_loss: 1.2047 - val_accuracy: 0.5207\n",
      "Epoch 19/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.1933 - accuracy: 0.5127 - val_loss: 1.1869 - val_accuracy: 0.5216\n",
      "Epoch 20/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.1885 - accuracy: 0.5162 - val_loss: 1.1840 - val_accuracy: 0.5124\n",
      "Epoch 21/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.1846 - accuracy: 0.5157 - val_loss: 1.1742 - val_accuracy: 0.5350\n",
      "Epoch 22/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.1791 - accuracy: 0.5208 - val_loss: 1.1762 - val_accuracy: 0.5178\n",
      "Epoch 23/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.1733 - accuracy: 0.5220 - val_loss: 1.1790 - val_accuracy: 0.5320\n",
      "Epoch 24/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.1700 - accuracy: 0.5229 - val_loss: 1.1628 - val_accuracy: 0.5418\n",
      "Epoch 25/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1689 - accuracy: 0.5246 - val_loss: 1.1663 - val_accuracy: 0.5270\n",
      "Epoch 26/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.1660 - accuracy: 0.5278 - val_loss: 1.1641 - val_accuracy: 0.5264\n",
      "Epoch 27/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.1658 - accuracy: 0.5245 - val_loss: 1.1670 - val_accuracy: 0.5353\n",
      "Epoch 28/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1581 - accuracy: 0.5305 - val_loss: 1.1604 - val_accuracy: 0.5388\n",
      "Epoch 29/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.1586 - accuracy: 0.5273 - val_loss: 1.1507 - val_accuracy: 0.5400\n",
      "Epoch 30/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.1554 - accuracy: 0.5325 - val_loss: 1.1525 - val_accuracy: 0.5365\n",
      "Epoch 31/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.1519 - accuracy: 0.5315 - val_loss: 1.1571 - val_accuracy: 0.5539\n",
      "Epoch 32/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.1555 - accuracy: 0.5278 - val_loss: 1.1599 - val_accuracy: 0.5362\n",
      "Epoch 33/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1533 - accuracy: 0.5286 - val_loss: 1.1479 - val_accuracy: 0.5495\n",
      "Epoch 34/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.1477 - accuracy: 0.5308 - val_loss: 1.1440 - val_accuracy: 0.5477\n",
      "Epoch 35/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1442 - accuracy: 0.5332 - val_loss: 1.1567 - val_accuracy: 0.5406\n",
      "Epoch 36/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.1465 - accuracy: 0.5336 - val_loss: 1.1379 - val_accuracy: 0.5427\n",
      "Epoch 37/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.1389 - accuracy: 0.5383 - val_loss: 1.1377 - val_accuracy: 0.5486\n",
      "Epoch 38/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.1388 - accuracy: 0.5348 - val_loss: 1.1391 - val_accuracy: 0.5519\n",
      "Epoch 39/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.1373 - accuracy: 0.5348 - val_loss: 1.1385 - val_accuracy: 0.5522\n",
      "Epoch 40/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.1357 - accuracy: 0.5382 - val_loss: 1.1346 - val_accuracy: 0.5563\n",
      "Epoch 41/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.1352 - accuracy: 0.5362 - val_loss: 1.1444 - val_accuracy: 0.5486\n",
      "Epoch 42/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1340 - accuracy: 0.5390 - val_loss: 1.1809 - val_accuracy: 0.5175\n",
      "Epoch 43/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.1359 - accuracy: 0.5376 - val_loss: 1.1335 - val_accuracy: 0.5525\n",
      "Epoch 44/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.1288 - accuracy: 0.5452 - val_loss: 1.1289 - val_accuracy: 0.5554\n",
      "Epoch 45/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1269 - accuracy: 0.5453 - val_loss: 1.1256 - val_accuracy: 0.5566\n",
      "Epoch 46/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.1255 - accuracy: 0.5439 - val_loss: 1.1573 - val_accuracy: 0.5329\n",
      "Epoch 47/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1245 - accuracy: 0.5451 - val_loss: 1.1220 - val_accuracy: 0.5560\n",
      "Epoch 48/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1248 - accuracy: 0.5428 - val_loss: 1.1464 - val_accuracy: 0.5433\n",
      "Epoch 49/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.1240 - accuracy: 0.5439 - val_loss: 1.1378 - val_accuracy: 0.5525\n",
      "Epoch 50/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1202 - accuracy: 0.5483 - val_loss: 1.1235 - val_accuracy: 0.5619\n",
      "Epoch 51/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.1194 - accuracy: 0.5499 - val_loss: 1.1539 - val_accuracy: 0.5465\n",
      "Epoch 52/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1278 - accuracy: 0.5387 - val_loss: 1.1459 - val_accuracy: 0.5400\n",
      "Epoch 53/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1221 - accuracy: 0.5448 - val_loss: 1.1224 - val_accuracy: 0.5640\n",
      "Epoch 54/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.1160 - accuracy: 0.5487 - val_loss: 1.1267 - val_accuracy: 0.5572\n",
      "Epoch 55/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1170 - accuracy: 0.5504 - val_loss: 1.1476 - val_accuracy: 0.5439\n",
      "Epoch 56/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1142 - accuracy: 0.5501 - val_loss: 1.1179 - val_accuracy: 0.5697\n",
      "Epoch 57/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.1088 - accuracy: 0.5557 - val_loss: 1.1187 - val_accuracy: 0.5619\n",
      "Epoch 58/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.1112 - accuracy: 0.5535 - val_loss: 1.1915 - val_accuracy: 0.5083\n",
      "Epoch 59/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.1091 - accuracy: 0.5538 - val_loss: 1.1179 - val_accuracy: 0.5714\n",
      "Epoch 60/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.1076 - accuracy: 0.5527 - val_loss: 1.1498 - val_accuracy: 0.5412\n",
      "Epoch 61/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1146 - accuracy: 0.5525 - val_loss: 1.1211 - val_accuracy: 0.5575\n",
      "Epoch 62/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.1104 - accuracy: 0.5505 - val_loss: 1.1091 - val_accuracy: 0.5714\n",
      "Epoch 63/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.1062 - accuracy: 0.5548 - val_loss: 1.1076 - val_accuracy: 0.5735\n",
      "Epoch 64/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1042 - accuracy: 0.5554 - val_loss: 1.1080 - val_accuracy: 0.5732\n",
      "Epoch 65/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1075 - accuracy: 0.5544 - val_loss: 1.1223 - val_accuracy: 0.5608\n",
      "Epoch 66/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.1074 - accuracy: 0.5537 - val_loss: 1.1073 - val_accuracy: 0.5705\n",
      "Epoch 67/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.1111 - accuracy: 0.5539 - val_loss: 1.1230 - val_accuracy: 0.5578\n",
      "Epoch 68/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1041 - accuracy: 0.5561 - val_loss: 1.1240 - val_accuracy: 0.5711\n",
      "Epoch 69/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.1033 - accuracy: 0.5590 - val_loss: 1.1088 - val_accuracy: 0.5723\n",
      "Epoch 70/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.1081 - accuracy: 0.5562 - val_loss: 1.1286 - val_accuracy: 0.5611\n",
      "Epoch 71/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.1042 - accuracy: 0.5588 - val_loss: 1.1067 - val_accuracy: 0.5682\n",
      "Epoch 72/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1025 - accuracy: 0.5576 - val_loss: 1.1211 - val_accuracy: 0.5625\n",
      "Epoch 73/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.1002 - accuracy: 0.5598 - val_loss: 1.1145 - val_accuracy: 0.5667\n",
      "Epoch 74/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0989 - accuracy: 0.5611 - val_loss: 1.1012 - val_accuracy: 0.5744\n",
      "Epoch 75/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0981 - accuracy: 0.5616 - val_loss: 1.1187 - val_accuracy: 0.5697\n",
      "Epoch 76/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.1039 - accuracy: 0.5584 - val_loss: 1.1195 - val_accuracy: 0.5619\n",
      "Epoch 77/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0985 - accuracy: 0.5624 - val_loss: 1.1230 - val_accuracy: 0.5590\n",
      "Epoch 78/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0942 - accuracy: 0.5622 - val_loss: 1.1022 - val_accuracy: 0.5797\n",
      "Epoch 79/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0922 - accuracy: 0.5643 - val_loss: 1.1004 - val_accuracy: 0.5774\n",
      "Epoch 80/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0954 - accuracy: 0.5645 - val_loss: 1.1008 - val_accuracy: 0.5788\n",
      "Epoch 81/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.1016 - accuracy: 0.5576 - val_loss: 1.1055 - val_accuracy: 0.5732\n",
      "Epoch 82/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0962 - accuracy: 0.5630 - val_loss: 1.0972 - val_accuracy: 0.5753\n",
      "Epoch 83/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0996 - accuracy: 0.5584 - val_loss: 1.0991 - val_accuracy: 0.5741\n",
      "Epoch 84/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0977 - accuracy: 0.5584 - val_loss: 1.0989 - val_accuracy: 0.5711\n",
      "Epoch 85/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0972 - accuracy: 0.5618 - val_loss: 1.1222 - val_accuracy: 0.5631\n",
      "Epoch 86/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0929 - accuracy: 0.5625 - val_loss: 1.1277 - val_accuracy: 0.5584\n",
      "Epoch 87/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0921 - accuracy: 0.5587 - val_loss: 1.1214 - val_accuracy: 0.5750\n",
      "Epoch 88/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0917 - accuracy: 0.5614 - val_loss: 1.0920 - val_accuracy: 0.5723\n",
      "Epoch 89/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0885 - accuracy: 0.5629 - val_loss: 1.0979 - val_accuracy: 0.5827\n",
      "Epoch 90/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0900 - accuracy: 0.5634 - val_loss: 1.1018 - val_accuracy: 0.5779\n",
      "Epoch 91/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0904 - accuracy: 0.5649 - val_loss: 1.0959 - val_accuracy: 0.5785\n",
      "Epoch 92/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0878 - accuracy: 0.5652 - val_loss: 1.0945 - val_accuracy: 0.5782\n",
      "Epoch 93/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0879 - accuracy: 0.5637 - val_loss: 1.0938 - val_accuracy: 0.5676\n",
      "Epoch 94/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0848 - accuracy: 0.5637 - val_loss: 1.0865 - val_accuracy: 0.5750\n",
      "Epoch 95/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0902 - accuracy: 0.5614 - val_loss: 1.0971 - val_accuracy: 0.5723\n",
      "Epoch 96/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0824 - accuracy: 0.5680 - val_loss: 1.1174 - val_accuracy: 0.5634\n",
      "Epoch 97/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0853 - accuracy: 0.5661 - val_loss: 1.1092 - val_accuracy: 0.5765\n",
      "Epoch 98/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0820 - accuracy: 0.5665 - val_loss: 1.0941 - val_accuracy: 0.5797\n",
      "Epoch 99/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0805 - accuracy: 0.5658 - val_loss: 1.0930 - val_accuracy: 0.5815\n",
      "Epoch 100/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0784 - accuracy: 0.5654 - val_loss: 1.0829 - val_accuracy: 0.5774\n",
      "Epoch 101/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0774 - accuracy: 0.5656 - val_loss: 1.0873 - val_accuracy: 0.5848\n",
      "Epoch 102/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0799 - accuracy: 0.5643 - val_loss: 1.0875 - val_accuracy: 0.5845\n",
      "Epoch 103/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0835 - accuracy: 0.5655 - val_loss: 1.0874 - val_accuracy: 0.5697\n",
      "Epoch 104/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0816 - accuracy: 0.5668 - val_loss: 1.0780 - val_accuracy: 0.5836\n",
      "Epoch 105/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0804 - accuracy: 0.5666 - val_loss: 1.0885 - val_accuracy: 0.5738\n",
      "Epoch 106/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0834 - accuracy: 0.5651 - val_loss: 1.0833 - val_accuracy: 0.5756\n",
      "Epoch 107/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0768 - accuracy: 0.5700 - val_loss: 1.1173 - val_accuracy: 0.5628\n",
      "Epoch 108/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0835 - accuracy: 0.5643 - val_loss: 1.1044 - val_accuracy: 0.5563\n",
      "Epoch 109/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0744 - accuracy: 0.5688 - val_loss: 1.0876 - val_accuracy: 0.5711\n",
      "Epoch 110/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0775 - accuracy: 0.5680 - val_loss: 1.1124 - val_accuracy: 0.5697\n",
      "Epoch 111/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0738 - accuracy: 0.5688 - val_loss: 1.0879 - val_accuracy: 0.5679\n",
      "Epoch 112/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0751 - accuracy: 0.5684 - val_loss: 1.1099 - val_accuracy: 0.5702\n",
      "Epoch 113/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0726 - accuracy: 0.5657 - val_loss: 1.0885 - val_accuracy: 0.5842\n",
      "Epoch 114/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0791 - accuracy: 0.5645 - val_loss: 1.0810 - val_accuracy: 0.5756\n",
      "Epoch 115/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0777 - accuracy: 0.5688 - val_loss: 1.0778 - val_accuracy: 0.5791\n",
      "Epoch 116/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0733 - accuracy: 0.5697 - val_loss: 1.1065 - val_accuracy: 0.5705\n",
      "Epoch 117/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0742 - accuracy: 0.5682 - val_loss: 1.0759 - val_accuracy: 0.5791\n",
      "Epoch 118/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0752 - accuracy: 0.5665 - val_loss: 1.0841 - val_accuracy: 0.5771\n",
      "Epoch 119/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0683 - accuracy: 0.5706 - val_loss: 1.0733 - val_accuracy: 0.5777\n",
      "Epoch 120/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0703 - accuracy: 0.5685 - val_loss: 1.0772 - val_accuracy: 0.5741\n",
      "Epoch 121/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0692 - accuracy: 0.5702 - val_loss: 1.0751 - val_accuracy: 0.5797\n",
      "Epoch 122/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0759 - accuracy: 0.5665 - val_loss: 1.0925 - val_accuracy: 0.5705\n",
      "Epoch 123/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0712 - accuracy: 0.5691 - val_loss: 1.0847 - val_accuracy: 0.5756\n",
      "Epoch 124/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0719 - accuracy: 0.5684 - val_loss: 1.0778 - val_accuracy: 0.5791\n",
      "Epoch 125/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0703 - accuracy: 0.5709 - val_loss: 1.0734 - val_accuracy: 0.5768\n",
      "Epoch 126/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0730 - accuracy: 0.5677 - val_loss: 1.0709 - val_accuracy: 0.5883\n",
      "Epoch 127/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0688 - accuracy: 0.5694 - val_loss: 1.0746 - val_accuracy: 0.5762\n",
      "Epoch 128/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0713 - accuracy: 0.5698 - val_loss: 1.0869 - val_accuracy: 0.5661\n",
      "Epoch 129/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0723 - accuracy: 0.5689 - val_loss: 1.0774 - val_accuracy: 0.5797\n",
      "Epoch 130/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0696 - accuracy: 0.5688 - val_loss: 1.0692 - val_accuracy: 0.5771\n",
      "Epoch 131/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0689 - accuracy: 0.5696 - val_loss: 1.0708 - val_accuracy: 0.5788\n",
      "Epoch 132/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0700 - accuracy: 0.5720 - val_loss: 1.0759 - val_accuracy: 0.5791\n",
      "Epoch 133/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0702 - accuracy: 0.5674 - val_loss: 1.0708 - val_accuracy: 0.5800\n",
      "Epoch 134/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0691 - accuracy: 0.5720 - val_loss: 1.0768 - val_accuracy: 0.5738\n",
      "Epoch 135/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0673 - accuracy: 0.5705 - val_loss: 1.0793 - val_accuracy: 0.5726\n",
      "Epoch 136/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0655 - accuracy: 0.5698 - val_loss: 1.0670 - val_accuracy: 0.5797\n",
      "Epoch 137/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0673 - accuracy: 0.5691 - val_loss: 1.0712 - val_accuracy: 0.5809\n",
      "Epoch 138/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0661 - accuracy: 0.5719 - val_loss: 1.0817 - val_accuracy: 0.5735\n",
      "Epoch 139/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0743 - accuracy: 0.5664 - val_loss: 1.0960 - val_accuracy: 0.5738\n",
      "Epoch 140/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0670 - accuracy: 0.5714 - val_loss: 1.0855 - val_accuracy: 0.5699\n",
      "Epoch 141/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0671 - accuracy: 0.5726 - val_loss: 1.0756 - val_accuracy: 0.5753\n",
      "Epoch 142/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0636 - accuracy: 0.5734 - val_loss: 1.0709 - val_accuracy: 0.5777\n",
      "Epoch 143/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0688 - accuracy: 0.5694 - val_loss: 1.0729 - val_accuracy: 0.5827\n",
      "Epoch 144/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0647 - accuracy: 0.5728 - val_loss: 1.0679 - val_accuracy: 0.5821\n",
      "Epoch 145/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0669 - accuracy: 0.5708 - val_loss: 1.0666 - val_accuracy: 0.5818\n",
      "Epoch 146/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0639 - accuracy: 0.5725 - val_loss: 1.0654 - val_accuracy: 0.5803\n",
      "Epoch 147/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0617 - accuracy: 0.5742 - val_loss: 1.0765 - val_accuracy: 0.5747\n",
      "Epoch 148/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0689 - accuracy: 0.5686 - val_loss: 1.0688 - val_accuracy: 0.5860\n",
      "Epoch 149/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0615 - accuracy: 0.5745 - val_loss: 1.0738 - val_accuracy: 0.5854\n",
      "Epoch 150/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0608 - accuracy: 0.5751 - val_loss: 1.0674 - val_accuracy: 0.5880\n",
      "Epoch 151/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0611 - accuracy: 0.5737 - val_loss: 1.0706 - val_accuracy: 0.5851\n",
      "Epoch 152/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0588 - accuracy: 0.5730 - val_loss: 1.0648 - val_accuracy: 0.5830\n",
      "Epoch 153/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0633 - accuracy: 0.5726 - val_loss: 1.0700 - val_accuracy: 0.5771\n",
      "Epoch 154/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0597 - accuracy: 0.5757 - val_loss: 1.0592 - val_accuracy: 0.5821\n",
      "Epoch 155/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0584 - accuracy: 0.5768 - val_loss: 1.0647 - val_accuracy: 0.5765\n",
      "Epoch 156/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0632 - accuracy: 0.5737 - val_loss: 1.0745 - val_accuracy: 0.5836\n",
      "Epoch 157/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0602 - accuracy: 0.5766 - val_loss: 1.0825 - val_accuracy: 0.5827\n",
      "Epoch 158/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0653 - accuracy: 0.5708 - val_loss: 1.0826 - val_accuracy: 0.5741\n",
      "Epoch 159/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0581 - accuracy: 0.5766 - val_loss: 1.0625 - val_accuracy: 0.5854\n",
      "Epoch 160/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0597 - accuracy: 0.5755 - val_loss: 1.0666 - val_accuracy: 0.5744\n",
      "Epoch 161/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0576 - accuracy: 0.5778 - val_loss: 1.0573 - val_accuracy: 0.5854\n",
      "Epoch 162/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0570 - accuracy: 0.5768 - val_loss: 1.0694 - val_accuracy: 0.5815\n",
      "Epoch 163/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0569 - accuracy: 0.5764 - val_loss: 1.0920 - val_accuracy: 0.5765\n",
      "Epoch 164/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0561 - accuracy: 0.5751 - val_loss: 1.0571 - val_accuracy: 0.5860\n",
      "Epoch 165/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0633 - accuracy: 0.5732 - val_loss: 1.0688 - val_accuracy: 0.5836\n",
      "Epoch 166/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0582 - accuracy: 0.5754 - val_loss: 1.0568 - val_accuracy: 0.5865\n",
      "Epoch 167/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0529 - accuracy: 0.5785 - val_loss: 1.0661 - val_accuracy: 0.5851\n",
      "Epoch 168/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0543 - accuracy: 0.5797 - val_loss: 1.0700 - val_accuracy: 0.5969\n",
      "Epoch 169/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0554 - accuracy: 0.5748 - val_loss: 1.0561 - val_accuracy: 0.5913\n",
      "Epoch 170/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0538 - accuracy: 0.5741 - val_loss: 1.0634 - val_accuracy: 0.5818\n",
      "Epoch 171/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0555 - accuracy: 0.5788 - val_loss: 1.0716 - val_accuracy: 0.5851\n",
      "Epoch 172/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0520 - accuracy: 0.5805 - val_loss: 1.0765 - val_accuracy: 0.5943\n",
      "Epoch 173/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0543 - accuracy: 0.5768 - val_loss: 1.0825 - val_accuracy: 0.5791\n",
      "Epoch 174/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0503 - accuracy: 0.5816 - val_loss: 1.0667 - val_accuracy: 0.5975\n",
      "Epoch 175/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0550 - accuracy: 0.5796 - val_loss: 1.0595 - val_accuracy: 0.5851\n",
      "Epoch 176/400\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0527 - accuracy: 0.5761 - val_loss: 1.0579 - val_accuracy: 0.5857\n",
      "Epoch 177/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0483 - accuracy: 0.5788 - val_loss: 1.1016 - val_accuracy: 0.5622\n",
      "Epoch 178/400\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0510 - accuracy: 0.5771 - val_loss: 1.0670 - val_accuracy: 0.5762\n",
      "Epoch 179/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0488 - accuracy: 0.5784 - val_loss: 1.0646 - val_accuracy: 0.5851\n",
      "Epoch 180/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0578 - accuracy: 0.5737 - val_loss: 1.0664 - val_accuracy: 0.5880\n",
      "Epoch 181/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0518 - accuracy: 0.5781 - val_loss: 1.0574 - val_accuracy: 0.5818\n",
      "Epoch 182/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0478 - accuracy: 0.5786 - val_loss: 1.0637 - val_accuracy: 0.5771\n",
      "Epoch 183/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0557 - accuracy: 0.5776 - val_loss: 1.0644 - val_accuracy: 0.5857\n",
      "Epoch 184/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0482 - accuracy: 0.5800 - val_loss: 1.0603 - val_accuracy: 0.5871\n",
      "Epoch 185/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0535 - accuracy: 0.5788 - val_loss: 1.0647 - val_accuracy: 0.5862\n",
      "Epoch 186/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0480 - accuracy: 0.5799 - val_loss: 1.0540 - val_accuracy: 0.5874\n",
      "Epoch 187/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0496 - accuracy: 0.5812 - val_loss: 1.0626 - val_accuracy: 0.5809\n",
      "Epoch 188/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0476 - accuracy: 0.5828 - val_loss: 1.0530 - val_accuracy: 0.5880\n",
      "Epoch 189/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0475 - accuracy: 0.5820 - val_loss: 1.0567 - val_accuracy: 0.5842\n",
      "Epoch 190/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0486 - accuracy: 0.5786 - val_loss: 1.0662 - val_accuracy: 0.5860\n",
      "Epoch 191/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0462 - accuracy: 0.5827 - val_loss: 1.0776 - val_accuracy: 0.5756\n",
      "Epoch 192/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0507 - accuracy: 0.5774 - val_loss: 1.0744 - val_accuracy: 0.5862\n",
      "Epoch 193/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0476 - accuracy: 0.5829 - val_loss: 1.0886 - val_accuracy: 0.5726\n",
      "Epoch 194/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0494 - accuracy: 0.5811 - val_loss: 1.0717 - val_accuracy: 0.5771\n",
      "Epoch 195/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0447 - accuracy: 0.5797 - val_loss: 1.0485 - val_accuracy: 0.5860\n",
      "Epoch 196/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0419 - accuracy: 0.5815 - val_loss: 1.0846 - val_accuracy: 0.5806\n",
      "Epoch 197/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0436 - accuracy: 0.5825 - val_loss: 1.0733 - val_accuracy: 0.5862\n",
      "Epoch 198/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0507 - accuracy: 0.5777 - val_loss: 1.0667 - val_accuracy: 0.5762\n",
      "Epoch 199/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0460 - accuracy: 0.5807 - val_loss: 1.0555 - val_accuracy: 0.5874\n",
      "Epoch 200/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0435 - accuracy: 0.5840 - val_loss: 1.0649 - val_accuracy: 0.5785\n",
      "Epoch 201/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0455 - accuracy: 0.5777 - val_loss: 1.0652 - val_accuracy: 0.5919\n",
      "Epoch 202/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0478 - accuracy: 0.5806 - val_loss: 1.0559 - val_accuracy: 0.5791\n",
      "Epoch 203/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0388 - accuracy: 0.5832 - val_loss: 1.0614 - val_accuracy: 0.5862\n",
      "Epoch 204/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0441 - accuracy: 0.5809 - val_loss: 1.0633 - val_accuracy: 0.5886\n",
      "Epoch 205/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0442 - accuracy: 0.5823 - val_loss: 1.0510 - val_accuracy: 0.5889\n",
      "Epoch 206/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0430 - accuracy: 0.5794 - val_loss: 1.0627 - val_accuracy: 0.5848\n",
      "Epoch 207/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0470 - accuracy: 0.5789 - val_loss: 1.0561 - val_accuracy: 0.5948\n",
      "Epoch 208/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0456 - accuracy: 0.5820 - val_loss: 1.0461 - val_accuracy: 0.5860\n",
      "Epoch 209/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0473 - accuracy: 0.5758 - val_loss: 1.0806 - val_accuracy: 0.5821\n",
      "Epoch 210/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0443 - accuracy: 0.5785 - val_loss: 1.0640 - val_accuracy: 0.5747\n",
      "Epoch 211/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0419 - accuracy: 0.5844 - val_loss: 1.0491 - val_accuracy: 0.5937\n",
      "Epoch 212/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0440 - accuracy: 0.5803 - val_loss: 1.0506 - val_accuracy: 0.5833\n",
      "Epoch 213/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0461 - accuracy: 0.5806 - val_loss: 1.0888 - val_accuracy: 0.5774\n",
      "Epoch 214/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0416 - accuracy: 0.5789 - val_loss: 1.0637 - val_accuracy: 0.5836\n",
      "Epoch 215/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0455 - accuracy: 0.5789 - val_loss: 1.0725 - val_accuracy: 0.5717\n",
      "Epoch 216/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0424 - accuracy: 0.5799 - val_loss: 1.0548 - val_accuracy: 0.5904\n",
      "Epoch 217/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0411 - accuracy: 0.5809 - val_loss: 1.0549 - val_accuracy: 0.5898\n",
      "Epoch 218/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0448 - accuracy: 0.5819 - val_loss: 1.0518 - val_accuracy: 0.5931\n",
      "Epoch 219/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0435 - accuracy: 0.5788 - val_loss: 1.0513 - val_accuracy: 0.5910\n",
      "Epoch 220/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0392 - accuracy: 0.5821 - val_loss: 1.0438 - val_accuracy: 0.5874\n",
      "Epoch 221/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0416 - accuracy: 0.5816 - val_loss: 1.0510 - val_accuracy: 0.5827\n",
      "Epoch 222/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0394 - accuracy: 0.5822 - val_loss: 1.0536 - val_accuracy: 0.5809\n",
      "Epoch 223/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0375 - accuracy: 0.5817 - val_loss: 1.0512 - val_accuracy: 0.5815\n",
      "Epoch 224/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0356 - accuracy: 0.5834 - val_loss: 1.0508 - val_accuracy: 0.5865\n",
      "Epoch 225/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0387 - accuracy: 0.5825 - val_loss: 1.0770 - val_accuracy: 0.5874\n",
      "Epoch 226/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0416 - accuracy: 0.5808 - val_loss: 1.0790 - val_accuracy: 0.5732\n",
      "Epoch 227/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0417 - accuracy: 0.5843 - val_loss: 1.0770 - val_accuracy: 0.5871\n",
      "Epoch 228/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0441 - accuracy: 0.5803 - val_loss: 1.0528 - val_accuracy: 0.5788\n",
      "Epoch 229/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0385 - accuracy: 0.5825 - val_loss: 1.0495 - val_accuracy: 0.5934\n",
      "Epoch 230/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0354 - accuracy: 0.5827 - val_loss: 1.0457 - val_accuracy: 0.5874\n",
      "Epoch 231/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0361 - accuracy: 0.5829 - val_loss: 1.0794 - val_accuracy: 0.5682\n",
      "Epoch 232/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0409 - accuracy: 0.5838 - val_loss: 1.0540 - val_accuracy: 0.5937\n",
      "Epoch 233/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0362 - accuracy: 0.5848 - val_loss: 1.0687 - val_accuracy: 0.5765\n",
      "Epoch 234/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0454 - accuracy: 0.5808 - val_loss: 1.0735 - val_accuracy: 0.5824\n",
      "Epoch 235/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0361 - accuracy: 0.5822 - val_loss: 1.0470 - val_accuracy: 0.5913\n",
      "Epoch 236/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0357 - accuracy: 0.5849 - val_loss: 1.0783 - val_accuracy: 0.5830\n",
      "Epoch 237/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0372 - accuracy: 0.5800 - val_loss: 1.0605 - val_accuracy: 0.5857\n",
      "Epoch 238/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0380 - accuracy: 0.5815 - val_loss: 1.0579 - val_accuracy: 0.5845\n",
      "Epoch 239/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0349 - accuracy: 0.5837 - val_loss: 1.0442 - val_accuracy: 0.5928\n",
      "Epoch 240/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0346 - accuracy: 0.5868 - val_loss: 1.0439 - val_accuracy: 0.5854\n",
      "Epoch 241/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0362 - accuracy: 0.5861 - val_loss: 1.0932 - val_accuracy: 0.5676\n",
      "Epoch 242/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0395 - accuracy: 0.5814 - val_loss: 1.0430 - val_accuracy: 0.5916\n",
      "Epoch 243/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0443 - accuracy: 0.5811 - val_loss: 1.0540 - val_accuracy: 0.5830\n",
      "Epoch 244/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0364 - accuracy: 0.5839 - val_loss: 1.0648 - val_accuracy: 0.5845\n",
      "Epoch 245/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0395 - accuracy: 0.5819 - val_loss: 1.0635 - val_accuracy: 0.5857\n",
      "Epoch 246/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0356 - accuracy: 0.5864 - val_loss: 1.0492 - val_accuracy: 0.5948\n",
      "Epoch 247/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0399 - accuracy: 0.5829 - val_loss: 1.0633 - val_accuracy: 0.5774\n",
      "Epoch 248/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0347 - accuracy: 0.5830 - val_loss: 1.0455 - val_accuracy: 0.5919\n",
      "Epoch 249/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0380 - accuracy: 0.5854 - val_loss: 1.0532 - val_accuracy: 0.5812\n",
      "Epoch 250/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0324 - accuracy: 0.5843 - val_loss: 1.0725 - val_accuracy: 0.5824\n",
      "Epoch 251/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0354 - accuracy: 0.5814 - val_loss: 1.0471 - val_accuracy: 0.5874\n",
      "Epoch 252/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0343 - accuracy: 0.5851 - val_loss: 1.0555 - val_accuracy: 0.5904\n",
      "Epoch 253/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0297 - accuracy: 0.5843 - val_loss: 1.0782 - val_accuracy: 0.5762\n",
      "Epoch 254/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0355 - accuracy: 0.5835 - val_loss: 1.0586 - val_accuracy: 0.5922\n",
      "Epoch 255/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0395 - accuracy: 0.5828 - val_loss: 1.0599 - val_accuracy: 0.5925\n",
      "Epoch 256/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0335 - accuracy: 0.5856 - val_loss: 1.0411 - val_accuracy: 0.5862\n",
      "Epoch 257/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0314 - accuracy: 0.5860 - val_loss: 1.0480 - val_accuracy: 0.5842\n",
      "Epoch 258/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0362 - accuracy: 0.5850 - val_loss: 1.0412 - val_accuracy: 0.5916\n",
      "Epoch 259/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0322 - accuracy: 0.5849 - val_loss: 1.0621 - val_accuracy: 0.5738\n",
      "Epoch 260/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0318 - accuracy: 0.5851 - val_loss: 1.0561 - val_accuracy: 0.5874\n",
      "Epoch 261/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0343 - accuracy: 0.5826 - val_loss: 1.0441 - val_accuracy: 0.5845\n",
      "Epoch 262/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0315 - accuracy: 0.5826 - val_loss: 1.0462 - val_accuracy: 0.5848\n",
      "Epoch 263/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0342 - accuracy: 0.5843 - val_loss: 1.0635 - val_accuracy: 0.5741\n",
      "Epoch 264/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0375 - accuracy: 0.5846 - val_loss: 1.0530 - val_accuracy: 0.5809\n",
      "Epoch 265/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0270 - accuracy: 0.5849 - val_loss: 1.0805 - val_accuracy: 0.5735\n",
      "Epoch 266/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0318 - accuracy: 0.5833 - val_loss: 1.0427 - val_accuracy: 0.5892\n",
      "Epoch 267/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0342 - accuracy: 0.5848 - val_loss: 1.0586 - val_accuracy: 0.5868\n",
      "Epoch 268/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0337 - accuracy: 0.5819 - val_loss: 1.0716 - val_accuracy: 0.5889\n",
      "Epoch 269/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0363 - accuracy: 0.5831 - val_loss: 1.0504 - val_accuracy: 0.5839\n",
      "Epoch 270/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0330 - accuracy: 0.5842 - val_loss: 1.0464 - val_accuracy: 0.5836\n",
      "Epoch 271/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0347 - accuracy: 0.5829 - val_loss: 1.0766 - val_accuracy: 0.5779\n",
      "Epoch 272/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0318 - accuracy: 0.5857 - val_loss: 1.0494 - val_accuracy: 0.5907\n",
      "Epoch 273/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0363 - accuracy: 0.5851 - val_loss: 1.0418 - val_accuracy: 0.5839\n",
      "Epoch 274/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0360 - accuracy: 0.5824 - val_loss: 1.0511 - val_accuracy: 0.5954\n",
      "Epoch 275/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0337 - accuracy: 0.5868 - val_loss: 1.0662 - val_accuracy: 0.5702\n",
      "Epoch 276/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0310 - accuracy: 0.5857 - val_loss: 1.0644 - val_accuracy: 0.5812\n",
      "Epoch 277/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0401 - accuracy: 0.5821 - val_loss: 1.0424 - val_accuracy: 0.5860\n",
      "Epoch 278/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0273 - accuracy: 0.5850 - val_loss: 1.0457 - val_accuracy: 0.5812\n",
      "Epoch 279/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0350 - accuracy: 0.5822 - val_loss: 1.0538 - val_accuracy: 0.5782\n",
      "Epoch 280/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0294 - accuracy: 0.5837 - val_loss: 1.0438 - val_accuracy: 0.5842\n",
      "Epoch 281/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0299 - accuracy: 0.5836 - val_loss: 1.0554 - val_accuracy: 0.5839\n",
      "Epoch 282/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0323 - accuracy: 0.5848 - val_loss: 1.0509 - val_accuracy: 0.5865\n",
      "Epoch 283/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0321 - accuracy: 0.5807 - val_loss: 1.0372 - val_accuracy: 0.5913\n",
      "Epoch 284/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0292 - accuracy: 0.5857 - val_loss: 1.0471 - val_accuracy: 0.5877\n",
      "Epoch 285/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0277 - accuracy: 0.5865 - val_loss: 1.0649 - val_accuracy: 0.5794\n",
      "Epoch 286/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0287 - accuracy: 0.5871 - val_loss: 1.0621 - val_accuracy: 0.5851\n",
      "Epoch 287/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0231 - accuracy: 0.5890 - val_loss: 1.0440 - val_accuracy: 0.5913\n",
      "Epoch 288/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0277 - accuracy: 0.5866 - val_loss: 1.0471 - val_accuracy: 0.5854\n",
      "Epoch 289/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0277 - accuracy: 0.5874 - val_loss: 1.0411 - val_accuracy: 0.5898\n",
      "Epoch 290/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0300 - accuracy: 0.5843 - val_loss: 1.0693 - val_accuracy: 0.5800\n",
      "Epoch 291/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0292 - accuracy: 0.5858 - val_loss: 1.0464 - val_accuracy: 0.5931\n",
      "Epoch 292/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0287 - accuracy: 0.5858 - val_loss: 1.0416 - val_accuracy: 0.5860\n",
      "Epoch 293/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0275 - accuracy: 0.5871 - val_loss: 1.0562 - val_accuracy: 0.5868\n",
      "Epoch 294/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0284 - accuracy: 0.5839 - val_loss: 1.0372 - val_accuracy: 0.5860\n",
      "Epoch 295/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0280 - accuracy: 0.5863 - val_loss: 1.0657 - val_accuracy: 0.5868\n",
      "Epoch 296/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0285 - accuracy: 0.5845 - val_loss: 1.0542 - val_accuracy: 0.5907\n",
      "Epoch 297/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0268 - accuracy: 0.5846 - val_loss: 1.0445 - val_accuracy: 0.5827\n",
      "Epoch 298/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0254 - accuracy: 0.5860 - val_loss: 1.0417 - val_accuracy: 0.5877\n",
      "Epoch 299/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0318 - accuracy: 0.5844 - val_loss: 1.0879 - val_accuracy: 0.5803\n",
      "Epoch 300/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0322 - accuracy: 0.5823 - val_loss: 1.0484 - val_accuracy: 0.5904\n",
      "Epoch 301/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0287 - accuracy: 0.5848 - val_loss: 1.0392 - val_accuracy: 0.5874\n",
      "Epoch 302/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0255 - accuracy: 0.5858 - val_loss: 1.0554 - val_accuracy: 0.5928\n",
      "Epoch 303/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0283 - accuracy: 0.5862 - val_loss: 1.0390 - val_accuracy: 0.5919\n",
      "Epoch 304/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0306 - accuracy: 0.5828 - val_loss: 1.0377 - val_accuracy: 0.5910\n",
      "Epoch 305/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0304 - accuracy: 0.5871 - val_loss: 1.0424 - val_accuracy: 0.5886\n",
      "Epoch 306/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0308 - accuracy: 0.5847 - val_loss: 1.0437 - val_accuracy: 0.5857\n",
      "Epoch 307/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0218 - accuracy: 0.5886 - val_loss: 1.0353 - val_accuracy: 0.5857\n",
      "Epoch 308/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0253 - accuracy: 0.5868 - val_loss: 1.0434 - val_accuracy: 0.5910\n",
      "Epoch 309/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0258 - accuracy: 0.5833 - val_loss: 1.0463 - val_accuracy: 0.5874\n",
      "Epoch 310/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0254 - accuracy: 0.5873 - val_loss: 1.0409 - val_accuracy: 0.5901\n",
      "Epoch 311/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0242 - accuracy: 0.5874 - val_loss: 1.0558 - val_accuracy: 0.5777\n",
      "Epoch 312/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0274 - accuracy: 0.5850 - val_loss: 1.0551 - val_accuracy: 0.5871\n",
      "Epoch 313/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0292 - accuracy: 0.5829 - val_loss: 1.0381 - val_accuracy: 0.5904\n",
      "Epoch 314/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0244 - accuracy: 0.5872 - val_loss: 1.0616 - val_accuracy: 0.5862\n",
      "Epoch 315/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0284 - accuracy: 0.5871 - val_loss: 1.0576 - val_accuracy: 0.5768\n",
      "Epoch 316/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0280 - accuracy: 0.5862 - val_loss: 1.0902 - val_accuracy: 0.5756\n",
      "Epoch 317/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0335 - accuracy: 0.5860 - val_loss: 1.0502 - val_accuracy: 0.5865\n",
      "Epoch 318/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0237 - accuracy: 0.5883 - val_loss: 1.0586 - val_accuracy: 0.5839\n",
      "Epoch 319/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0253 - accuracy: 0.5868 - val_loss: 1.0644 - val_accuracy: 0.5910\n",
      "Epoch 320/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0285 - accuracy: 0.5881 - val_loss: 1.0478 - val_accuracy: 0.5791\n",
      "Epoch 321/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0293 - accuracy: 0.5824 - val_loss: 1.0694 - val_accuracy: 0.5768\n",
      "Epoch 322/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0238 - accuracy: 0.5838 - val_loss: 1.0476 - val_accuracy: 0.5871\n",
      "Epoch 323/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0264 - accuracy: 0.5868 - val_loss: 1.0450 - val_accuracy: 0.5880\n",
      "Epoch 324/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0297 - accuracy: 0.5834 - val_loss: 1.0497 - val_accuracy: 0.5925\n",
      "Epoch 325/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0258 - accuracy: 0.5855 - val_loss: 1.0524 - val_accuracy: 0.5922\n",
      "Epoch 326/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0257 - accuracy: 0.5880 - val_loss: 1.0361 - val_accuracy: 0.5862\n",
      "Epoch 327/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0211 - accuracy: 0.5886 - val_loss: 1.0373 - val_accuracy: 0.5842\n",
      "Epoch 328/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0284 - accuracy: 0.5861 - val_loss: 1.0575 - val_accuracy: 0.5945\n",
      "Epoch 329/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0234 - accuracy: 0.5880 - val_loss: 1.0496 - val_accuracy: 0.5845\n",
      "Epoch 330/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0194 - accuracy: 0.5878 - val_loss: 1.0382 - val_accuracy: 0.5842\n",
      "Epoch 331/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0279 - accuracy: 0.5877 - val_loss: 1.0459 - val_accuracy: 0.5892\n",
      "Epoch 332/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0282 - accuracy: 0.5869 - val_loss: 1.0420 - val_accuracy: 0.5839\n",
      "Epoch 333/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0234 - accuracy: 0.5899 - val_loss: 1.0447 - val_accuracy: 0.5812\n",
      "Epoch 334/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0220 - accuracy: 0.5881 - val_loss: 1.0502 - val_accuracy: 0.5815\n",
      "Epoch 335/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0220 - accuracy: 0.5891 - val_loss: 1.0428 - val_accuracy: 0.5821\n",
      "Epoch 336/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0236 - accuracy: 0.5866 - val_loss: 1.0702 - val_accuracy: 0.5830\n",
      "Epoch 337/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0251 - accuracy: 0.5847 - val_loss: 1.0444 - val_accuracy: 0.5874\n",
      "Epoch 338/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0269 - accuracy: 0.5851 - val_loss: 1.0578 - val_accuracy: 0.5806\n",
      "Epoch 339/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0247 - accuracy: 0.5894 - val_loss: 1.0514 - val_accuracy: 0.5800\n",
      "Epoch 340/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0244 - accuracy: 0.5872 - val_loss: 1.0357 - val_accuracy: 0.5860\n",
      "Epoch 341/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0221 - accuracy: 0.5888 - val_loss: 1.0483 - val_accuracy: 0.5883\n",
      "Epoch 342/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0213 - accuracy: 0.5867 - val_loss: 1.0384 - val_accuracy: 0.5892\n",
      "Epoch 343/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0238 - accuracy: 0.5881 - val_loss: 1.0459 - val_accuracy: 0.5833\n",
      "Epoch 344/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0233 - accuracy: 0.5864 - val_loss: 1.0655 - val_accuracy: 0.5865\n",
      "Epoch 345/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0222 - accuracy: 0.5902 - val_loss: 1.0556 - val_accuracy: 0.5886\n",
      "Epoch 346/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0236 - accuracy: 0.5861 - val_loss: 1.0642 - val_accuracy: 0.5753\n",
      "Epoch 347/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0212 - accuracy: 0.5906 - val_loss: 1.0402 - val_accuracy: 0.5910\n",
      "Epoch 348/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0235 - accuracy: 0.5868 - val_loss: 1.0425 - val_accuracy: 0.5898\n",
      "Epoch 349/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0239 - accuracy: 0.5869 - val_loss: 1.0491 - val_accuracy: 0.5857\n",
      "Epoch 350/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0231 - accuracy: 0.5860 - val_loss: 1.0493 - val_accuracy: 0.5913\n",
      "Epoch 351/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0231 - accuracy: 0.5861 - val_loss: 1.0449 - val_accuracy: 0.5910\n",
      "Epoch 352/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0249 - accuracy: 0.5874 - val_loss: 1.0663 - val_accuracy: 0.5815\n",
      "Epoch 353/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0286 - accuracy: 0.5851 - val_loss: 1.0515 - val_accuracy: 0.5845\n",
      "Epoch 354/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0239 - accuracy: 0.5866 - val_loss: 1.0755 - val_accuracy: 0.5794\n",
      "Epoch 355/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0292 - accuracy: 0.5844 - val_loss: 1.0447 - val_accuracy: 0.5857\n",
      "Epoch 356/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0228 - accuracy: 0.5876 - val_loss: 1.0428 - val_accuracy: 0.5877\n",
      "Epoch 357/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0207 - accuracy: 0.5892 - val_loss: 1.0499 - val_accuracy: 0.5848\n",
      "Epoch 358/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0238 - accuracy: 0.5863 - val_loss: 1.0521 - val_accuracy: 0.5777\n",
      "Epoch 359/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0215 - accuracy: 0.5895 - val_loss: 1.0461 - val_accuracy: 0.5827\n",
      "Epoch 360/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0198 - accuracy: 0.5878 - val_loss: 1.0420 - val_accuracy: 0.5910\n",
      "Epoch 361/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0252 - accuracy: 0.5866 - val_loss: 1.0446 - val_accuracy: 0.5913\n",
      "Epoch 362/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0187 - accuracy: 0.5889 - val_loss: 1.0418 - val_accuracy: 0.5871\n",
      "Epoch 363/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0206 - accuracy: 0.5873 - val_loss: 1.0381 - val_accuracy: 0.5913\n",
      "Epoch 364/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0203 - accuracy: 0.5897 - val_loss: 1.0463 - val_accuracy: 0.5910\n",
      "Epoch 365/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0259 - accuracy: 0.5877 - val_loss: 1.0460 - val_accuracy: 0.5824\n",
      "Epoch 366/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0266 - accuracy: 0.5860 - val_loss: 1.0456 - val_accuracy: 0.5821\n",
      "Epoch 367/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0226 - accuracy: 0.5888 - val_loss: 1.0696 - val_accuracy: 0.5782\n",
      "Epoch 368/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0223 - accuracy: 0.5860 - val_loss: 1.0861 - val_accuracy: 0.5771\n",
      "Epoch 369/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0224 - accuracy: 0.5891 - val_loss: 1.0620 - val_accuracy: 0.5913\n",
      "Epoch 370/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0234 - accuracy: 0.5866 - val_loss: 1.0495 - val_accuracy: 0.5934\n",
      "Epoch 371/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0235 - accuracy: 0.5888 - val_loss: 1.0447 - val_accuracy: 0.5886\n",
      "Epoch 372/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0224 - accuracy: 0.5878 - val_loss: 1.0536 - val_accuracy: 0.5868\n",
      "Epoch 373/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0223 - accuracy: 0.5879 - val_loss: 1.0559 - val_accuracy: 0.5800\n",
      "Epoch 374/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0226 - accuracy: 0.5900 - val_loss: 1.0448 - val_accuracy: 0.5880\n",
      "Epoch 375/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0238 - accuracy: 0.5903 - val_loss: 1.0463 - val_accuracy: 0.5851\n",
      "Epoch 376/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0227 - accuracy: 0.5872 - val_loss: 1.0403 - val_accuracy: 0.5901\n",
      "Epoch 377/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0162 - accuracy: 0.5914 - val_loss: 1.0387 - val_accuracy: 0.5957\n",
      "Epoch 378/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0242 - accuracy: 0.5889 - val_loss: 1.0389 - val_accuracy: 0.5833\n",
      "Epoch 379/400\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0198 - accuracy: 0.5879 - val_loss: 1.0394 - val_accuracy: 0.5922\n",
      "Epoch 380/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0218 - accuracy: 0.5855 - val_loss: 1.0413 - val_accuracy: 0.5892\n",
      "Epoch 381/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0229 - accuracy: 0.5899 - val_loss: 1.0646 - val_accuracy: 0.5809\n",
      "Epoch 382/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0240 - accuracy: 0.5877 - val_loss: 1.0445 - val_accuracy: 0.5818\n",
      "Epoch 383/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0197 - accuracy: 0.5871 - val_loss: 1.0525 - val_accuracy: 0.5806\n",
      "Epoch 384/400\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0196 - accuracy: 0.5910 - val_loss: 1.0435 - val_accuracy: 0.5913\n",
      "Epoch 385/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0201 - accuracy: 0.5886 - val_loss: 1.0464 - val_accuracy: 0.5951\n",
      "Epoch 386/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0198 - accuracy: 0.5904 - val_loss: 1.0535 - val_accuracy: 0.5945\n",
      "Epoch 387/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0223 - accuracy: 0.5874 - val_loss: 1.0440 - val_accuracy: 0.5889\n",
      "Epoch 388/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0251 - accuracy: 0.5891 - val_loss: 1.0414 - val_accuracy: 0.5895\n",
      "Epoch 389/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0224 - accuracy: 0.5885 - val_loss: 1.0442 - val_accuracy: 0.5907\n",
      "Epoch 390/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0160 - accuracy: 0.5894 - val_loss: 1.0459 - val_accuracy: 0.5910\n",
      "Epoch 391/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0204 - accuracy: 0.5903 - val_loss: 1.0429 - val_accuracy: 0.5868\n",
      "Epoch 392/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0201 - accuracy: 0.5893 - val_loss: 1.0601 - val_accuracy: 0.5916\n",
      "Epoch 393/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0250 - accuracy: 0.5869 - val_loss: 1.0543 - val_accuracy: 0.5830\n",
      "Epoch 394/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0313 - accuracy: 0.5856 - val_loss: 1.0337 - val_accuracy: 0.5904\n",
      "Epoch 395/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0164 - accuracy: 0.5897 - val_loss: 1.0390 - val_accuracy: 0.5943\n",
      "Epoch 396/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0232 - accuracy: 0.5857 - val_loss: 1.0462 - val_accuracy: 0.5937\n",
      "Epoch 397/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0185 - accuracy: 0.5883 - val_loss: 1.0411 - val_accuracy: 0.5928\n",
      "Epoch 398/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0197 - accuracy: 0.5897 - val_loss: 1.0382 - val_accuracy: 0.5895\n",
      "Epoch 399/400\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0226 - accuracy: 0.5881 - val_loss: 1.0436 - val_accuracy: 0.5871\n",
      "Epoch 400/400\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0210 - accuracy: 0.5861 - val_loss: 1.0521 - val_accuracy: 0.5842\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=400, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a53aa430",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"accuracy\"], label=\"accuracy\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"val_accuracy\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.legend([\"train\", \"test\"], loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "val_acc_epoch = history.history[\"val_accuracy\"]\n",
    "best_epoch = val_acc_epoch.index(max(val_acc_epoch)) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "98024ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.4657 - accuracy: 0.3993 - val_loss: 1.4206 - val_accuracy: 0.4132\n",
      "Epoch 2/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.4106 - accuracy: 0.4149 - val_loss: 1.4122 - val_accuracy: 0.4132\n",
      "Epoch 3/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.4058 - accuracy: 0.4149 - val_loss: 1.4140 - val_accuracy: 0.4132\n",
      "Epoch 4/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.4022 - accuracy: 0.4149 - val_loss: 1.4100 - val_accuracy: 0.4132\n",
      "Epoch 5/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.3991 - accuracy: 0.4149 - val_loss: 1.4013 - val_accuracy: 0.4132\n",
      "Epoch 6/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.3871 - accuracy: 0.4151 - val_loss: 1.3862 - val_accuracy: 0.4111\n",
      "Epoch 7/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.3576 - accuracy: 0.4162 - val_loss: 1.3393 - val_accuracy: 0.4123\n",
      "Epoch 8/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.3199 - accuracy: 0.4250 - val_loss: 1.3004 - val_accuracy: 0.4271\n",
      "Epoch 9/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.2890 - accuracy: 0.4393 - val_loss: 1.2914 - val_accuracy: 0.4644\n",
      "Epoch 10/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.2689 - accuracy: 0.4532 - val_loss: 1.2627 - val_accuracy: 0.4641\n",
      "Epoch 11/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.2522 - accuracy: 0.4738 - val_loss: 1.2417 - val_accuracy: 0.4852\n",
      "Epoch 12/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.2369 - accuracy: 0.4879 - val_loss: 1.2314 - val_accuracy: 0.4982\n",
      "Epoch 13/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.2343 - accuracy: 0.4924 - val_loss: 1.2256 - val_accuracy: 0.5133\n",
      "Epoch 14/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.2197 - accuracy: 0.5064 - val_loss: 1.2143 - val_accuracy: 0.5154\n",
      "Epoch 15/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.2142 - accuracy: 0.5104 - val_loss: 1.2166 - val_accuracy: 0.4944\n",
      "Epoch 16/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.2074 - accuracy: 0.5162 - val_loss: 1.2143 - val_accuracy: 0.5101\n",
      "Epoch 17/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1986 - accuracy: 0.5177 - val_loss: 1.2046 - val_accuracy: 0.5252\n",
      "Epoch 18/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1874 - accuracy: 0.5248 - val_loss: 1.1770 - val_accuracy: 0.5495\n",
      "Epoch 19/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1740 - accuracy: 0.5313 - val_loss: 1.1704 - val_accuracy: 0.5382\n",
      "Epoch 20/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1643 - accuracy: 0.5407 - val_loss: 1.2073 - val_accuracy: 0.5175\n",
      "Epoch 21/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1585 - accuracy: 0.5377 - val_loss: 1.1593 - val_accuracy: 0.5456\n",
      "Epoch 22/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1488 - accuracy: 0.5426 - val_loss: 1.1724 - val_accuracy: 0.5305\n",
      "Epoch 23/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1459 - accuracy: 0.5425 - val_loss: 1.1456 - val_accuracy: 0.5539\n",
      "Epoch 24/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1453 - accuracy: 0.5372 - val_loss: 1.1403 - val_accuracy: 0.5605\n",
      "Epoch 25/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1397 - accuracy: 0.5462 - val_loss: 1.1311 - val_accuracy: 0.5631\n",
      "Epoch 26/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1352 - accuracy: 0.5474 - val_loss: 1.1276 - val_accuracy: 0.5637\n",
      "Epoch 27/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1335 - accuracy: 0.5482 - val_loss: 1.1339 - val_accuracy: 0.5584\n",
      "Epoch 28/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1250 - accuracy: 0.5504 - val_loss: 1.1288 - val_accuracy: 0.5673\n",
      "Epoch 29/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1234 - accuracy: 0.5494 - val_loss: 1.1246 - val_accuracy: 0.5599\n",
      "Epoch 30/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1220 - accuracy: 0.5478 - val_loss: 1.1229 - val_accuracy: 0.5614\n",
      "Epoch 31/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1224 - accuracy: 0.5477 - val_loss: 1.1163 - val_accuracy: 0.5673\n",
      "Epoch 32/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1161 - accuracy: 0.5534 - val_loss: 1.1182 - val_accuracy: 0.5702\n",
      "Epoch 33/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1135 - accuracy: 0.5534 - val_loss: 1.1141 - val_accuracy: 0.5655\n",
      "Epoch 34/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1092 - accuracy: 0.5565 - val_loss: 1.1199 - val_accuracy: 0.5628\n",
      "Epoch 35/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1065 - accuracy: 0.5565 - val_loss: 1.1115 - val_accuracy: 0.5735\n",
      "Epoch 36/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1104 - accuracy: 0.5536 - val_loss: 1.1224 - val_accuracy: 0.5676\n",
      "Epoch 37/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1027 - accuracy: 0.5584 - val_loss: 1.1050 - val_accuracy: 0.5699\n",
      "Epoch 38/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1030 - accuracy: 0.5590 - val_loss: 1.1061 - val_accuracy: 0.5593\n",
      "Epoch 39/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1027 - accuracy: 0.5568 - val_loss: 1.0961 - val_accuracy: 0.5762\n",
      "Epoch 40/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0978 - accuracy: 0.5590 - val_loss: 1.1041 - val_accuracy: 0.5702\n",
      "Epoch 41/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0973 - accuracy: 0.5600 - val_loss: 1.0989 - val_accuracy: 0.5699\n",
      "Epoch 42/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0938 - accuracy: 0.5625 - val_loss: 1.1059 - val_accuracy: 0.5625\n",
      "Epoch 43/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0951 - accuracy: 0.5612 - val_loss: 1.0940 - val_accuracy: 0.5753\n",
      "Epoch 44/174\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0936 - accuracy: 0.5565 - val_loss: 1.0903 - val_accuracy: 0.5803\n",
      "Epoch 45/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0904 - accuracy: 0.5626 - val_loss: 1.0973 - val_accuracy: 0.5581\n",
      "Epoch 46/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0856 - accuracy: 0.5657 - val_loss: 1.0965 - val_accuracy: 0.5625\n",
      "Epoch 47/174\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0844 - accuracy: 0.5632 - val_loss: 1.0917 - val_accuracy: 0.5788\n",
      "Epoch 48/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0869 - accuracy: 0.5624 - val_loss: 1.1118 - val_accuracy: 0.5522\n",
      "Epoch 49/174\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0810 - accuracy: 0.5679 - val_loss: 1.0873 - val_accuracy: 0.5685\n",
      "Epoch 50/174\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0861 - accuracy: 0.5624 - val_loss: 1.0982 - val_accuracy: 0.5569\n",
      "Epoch 51/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0823 - accuracy: 0.5633 - val_loss: 1.1170 - val_accuracy: 0.5667\n",
      "Epoch 52/174\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0773 - accuracy: 0.5674 - val_loss: 1.1097 - val_accuracy: 0.5599\n",
      "Epoch 53/174\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0793 - accuracy: 0.5626 - val_loss: 1.0797 - val_accuracy: 0.5753\n",
      "Epoch 54/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0795 - accuracy: 0.5663 - val_loss: 1.0830 - val_accuracy: 0.5676\n",
      "Epoch 55/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0776 - accuracy: 0.5638 - val_loss: 1.0820 - val_accuracy: 0.5714\n",
      "Epoch 56/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0785 - accuracy: 0.5657 - val_loss: 1.1218 - val_accuracy: 0.5560\n",
      "Epoch 57/174\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0786 - accuracy: 0.5657 - val_loss: 1.0809 - val_accuracy: 0.5637\n",
      "Epoch 58/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0738 - accuracy: 0.5702 - val_loss: 1.0869 - val_accuracy: 0.5673\n",
      "Epoch 59/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0733 - accuracy: 0.5688 - val_loss: 1.0783 - val_accuracy: 0.5785\n",
      "Epoch 60/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0761 - accuracy: 0.5688 - val_loss: 1.0932 - val_accuracy: 0.5667\n",
      "Epoch 61/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0752 - accuracy: 0.5663 - val_loss: 1.0768 - val_accuracy: 0.5815\n",
      "Epoch 62/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0716 - accuracy: 0.5666 - val_loss: 1.0705 - val_accuracy: 0.5794\n",
      "Epoch 63/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0712 - accuracy: 0.5656 - val_loss: 1.0887 - val_accuracy: 0.5779\n",
      "Epoch 64/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0763 - accuracy: 0.5645 - val_loss: 1.1072 - val_accuracy: 0.5655\n",
      "Epoch 65/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0739 - accuracy: 0.5676 - val_loss: 1.0711 - val_accuracy: 0.5815\n",
      "Epoch 66/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0719 - accuracy: 0.5675 - val_loss: 1.0782 - val_accuracy: 0.5750\n",
      "Epoch 67/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0678 - accuracy: 0.5698 - val_loss: 1.0749 - val_accuracy: 0.5809\n",
      "Epoch 68/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0675 - accuracy: 0.5700 - val_loss: 1.0712 - val_accuracy: 0.5842\n",
      "Epoch 69/174\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0680 - accuracy: 0.5701 - val_loss: 1.0974 - val_accuracy: 0.5741\n",
      "Epoch 70/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0710 - accuracy: 0.5656 - val_loss: 1.0737 - val_accuracy: 0.5774\n",
      "Epoch 71/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0666 - accuracy: 0.5728 - val_loss: 1.0658 - val_accuracy: 0.5824\n",
      "Epoch 72/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0653 - accuracy: 0.5665 - val_loss: 1.0833 - val_accuracy: 0.5726\n",
      "Epoch 73/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0709 - accuracy: 0.5697 - val_loss: 1.0827 - val_accuracy: 0.5726\n",
      "Epoch 74/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0680 - accuracy: 0.5688 - val_loss: 1.0778 - val_accuracy: 0.5812\n",
      "Epoch 75/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0641 - accuracy: 0.5694 - val_loss: 1.1081 - val_accuracy: 0.5611\n",
      "Epoch 76/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0644 - accuracy: 0.5723 - val_loss: 1.0782 - val_accuracy: 0.5774\n",
      "Epoch 77/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0676 - accuracy: 0.5734 - val_loss: 1.0787 - val_accuracy: 0.5809\n",
      "Epoch 78/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0607 - accuracy: 0.5709 - val_loss: 1.0872 - val_accuracy: 0.5679\n",
      "Epoch 79/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0675 - accuracy: 0.5697 - val_loss: 1.0860 - val_accuracy: 0.5619\n",
      "Epoch 80/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0623 - accuracy: 0.5764 - val_loss: 1.0863 - val_accuracy: 0.5652\n",
      "Epoch 81/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0634 - accuracy: 0.5731 - val_loss: 1.0646 - val_accuracy: 0.5756\n",
      "Epoch 82/174\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0639 - accuracy: 0.5697 - val_loss: 1.0728 - val_accuracy: 0.5824\n",
      "Epoch 83/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0632 - accuracy: 0.5735 - val_loss: 1.0918 - val_accuracy: 0.5699\n",
      "Epoch 84/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0577 - accuracy: 0.5743 - val_loss: 1.0641 - val_accuracy: 0.5877\n",
      "Epoch 85/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0638 - accuracy: 0.5703 - val_loss: 1.0723 - val_accuracy: 0.5732\n",
      "Epoch 86/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0593 - accuracy: 0.5746 - val_loss: 1.0627 - val_accuracy: 0.5827\n",
      "Epoch 87/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0628 - accuracy: 0.5736 - val_loss: 1.0659 - val_accuracy: 0.5830\n",
      "Epoch 88/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0622 - accuracy: 0.5715 - val_loss: 1.0627 - val_accuracy: 0.5836\n",
      "Epoch 89/174\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0586 - accuracy: 0.5721 - val_loss: 1.0635 - val_accuracy: 0.5768\n",
      "Epoch 90/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0599 - accuracy: 0.5740 - val_loss: 1.1091 - val_accuracy: 0.5513\n",
      "Epoch 91/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0680 - accuracy: 0.5694 - val_loss: 1.0935 - val_accuracy: 0.5560\n",
      "Epoch 92/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0573 - accuracy: 0.5758 - val_loss: 1.0764 - val_accuracy: 0.5744\n",
      "Epoch 93/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0569 - accuracy: 0.5720 - val_loss: 1.0607 - val_accuracy: 0.5797\n",
      "Epoch 94/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0574 - accuracy: 0.5727 - val_loss: 1.0790 - val_accuracy: 0.5714\n",
      "Epoch 95/174\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0552 - accuracy: 0.5743 - val_loss: 1.0769 - val_accuracy: 0.5777\n",
      "Epoch 96/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0524 - accuracy: 0.5789 - val_loss: 1.0641 - val_accuracy: 0.5782\n",
      "Epoch 97/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0525 - accuracy: 0.5740 - val_loss: 1.0673 - val_accuracy: 0.5785\n",
      "Epoch 98/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0540 - accuracy: 0.5731 - val_loss: 1.0769 - val_accuracy: 0.5705\n",
      "Epoch 99/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0618 - accuracy: 0.5713 - val_loss: 1.0713 - val_accuracy: 0.5765\n",
      "Epoch 100/174\n",
      "422/422 [==============================] - 2s 6ms/step - loss: 1.0617 - accuracy: 0.5737 - val_loss: 1.0716 - val_accuracy: 0.5865\n",
      "Epoch 101/174\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0530 - accuracy: 0.5740 - val_loss: 1.0586 - val_accuracy: 0.5836\n",
      "Epoch 102/174\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0559 - accuracy: 0.5758 - val_loss: 1.0701 - val_accuracy: 0.5729\n",
      "Epoch 103/174\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0540 - accuracy: 0.5720 - val_loss: 1.0543 - val_accuracy: 0.5865\n",
      "Epoch 104/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0534 - accuracy: 0.5740 - val_loss: 1.0602 - val_accuracy: 0.5809\n",
      "Epoch 105/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0522 - accuracy: 0.5733 - val_loss: 1.0746 - val_accuracy: 0.5717\n",
      "Epoch 106/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0539 - accuracy: 0.5745 - val_loss: 1.0609 - val_accuracy: 0.5768\n",
      "Epoch 107/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0535 - accuracy: 0.5743 - val_loss: 1.0722 - val_accuracy: 0.5782\n",
      "Epoch 108/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0500 - accuracy: 0.5752 - val_loss: 1.0724 - val_accuracy: 0.5862\n",
      "Epoch 109/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0541 - accuracy: 0.5737 - val_loss: 1.0646 - val_accuracy: 0.5886\n",
      "Epoch 110/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0484 - accuracy: 0.5768 - val_loss: 1.0576 - val_accuracy: 0.5916\n",
      "Epoch 111/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0550 - accuracy: 0.5751 - val_loss: 1.0583 - val_accuracy: 0.5827\n",
      "Epoch 112/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0485 - accuracy: 0.5771 - val_loss: 1.0537 - val_accuracy: 0.5862\n",
      "Epoch 113/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0476 - accuracy: 0.5792 - val_loss: 1.0765 - val_accuracy: 0.5682\n",
      "Epoch 114/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0512 - accuracy: 0.5758 - val_loss: 1.0644 - val_accuracy: 0.5777\n",
      "Epoch 115/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0495 - accuracy: 0.5787 - val_loss: 1.0693 - val_accuracy: 0.5702\n",
      "Epoch 116/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0511 - accuracy: 0.5741 - val_loss: 1.0831 - val_accuracy: 0.5809\n",
      "Epoch 117/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0523 - accuracy: 0.5737 - val_loss: 1.0656 - val_accuracy: 0.5830\n",
      "Epoch 118/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0533 - accuracy: 0.5738 - val_loss: 1.0668 - val_accuracy: 0.5779\n",
      "Epoch 119/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0520 - accuracy: 0.5766 - val_loss: 1.0560 - val_accuracy: 0.5925\n",
      "Epoch 120/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0475 - accuracy: 0.5777 - val_loss: 1.0604 - val_accuracy: 0.5848\n",
      "Epoch 121/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0482 - accuracy: 0.5776 - val_loss: 1.0507 - val_accuracy: 0.5827\n",
      "Epoch 122/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0466 - accuracy: 0.5746 - val_loss: 1.0589 - val_accuracy: 0.5928\n",
      "Epoch 123/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0484 - accuracy: 0.5765 - val_loss: 1.0554 - val_accuracy: 0.5922\n",
      "Epoch 124/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0491 - accuracy: 0.5771 - val_loss: 1.0695 - val_accuracy: 0.5779\n",
      "Epoch 125/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0473 - accuracy: 0.5794 - val_loss: 1.0924 - val_accuracy: 0.5750\n",
      "Epoch 126/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0496 - accuracy: 0.5757 - val_loss: 1.0518 - val_accuracy: 0.5818\n",
      "Epoch 127/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0482 - accuracy: 0.5775 - val_loss: 1.0742 - val_accuracy: 0.5797\n",
      "Epoch 128/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0505 - accuracy: 0.5772 - val_loss: 1.0470 - val_accuracy: 0.5874\n",
      "Epoch 129/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0478 - accuracy: 0.5777 - val_loss: 1.0618 - val_accuracy: 0.5868\n",
      "Epoch 130/174\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0496 - accuracy: 0.5787 - val_loss: 1.0477 - val_accuracy: 0.5836\n",
      "Epoch 131/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0444 - accuracy: 0.5791 - val_loss: 1.0625 - val_accuracy: 0.5889\n",
      "Epoch 132/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0486 - accuracy: 0.5757 - val_loss: 1.0663 - val_accuracy: 0.5815\n",
      "Epoch 133/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0400 - accuracy: 0.5809 - val_loss: 1.0487 - val_accuracy: 0.5871\n",
      "Epoch 134/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0455 - accuracy: 0.5779 - val_loss: 1.0621 - val_accuracy: 0.5931\n",
      "Epoch 135/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0440 - accuracy: 0.5803 - val_loss: 1.0543 - val_accuracy: 0.5845\n",
      "Epoch 136/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0499 - accuracy: 0.5728 - val_loss: 1.0527 - val_accuracy: 0.5860\n",
      "Epoch 137/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0510 - accuracy: 0.5756 - val_loss: 1.0670 - val_accuracy: 0.5771\n",
      "Epoch 138/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0437 - accuracy: 0.5807 - val_loss: 1.1141 - val_accuracy: 0.5578\n",
      "Epoch 139/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0436 - accuracy: 0.5764 - val_loss: 1.0522 - val_accuracy: 0.5886\n",
      "Epoch 140/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0420 - accuracy: 0.5788 - val_loss: 1.0467 - val_accuracy: 0.5883\n",
      "Epoch 141/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0429 - accuracy: 0.5794 - val_loss: 1.0612 - val_accuracy: 0.5889\n",
      "Epoch 142/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0453 - accuracy: 0.5793 - val_loss: 1.0446 - val_accuracy: 0.5865\n",
      "Epoch 143/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0428 - accuracy: 0.5811 - val_loss: 1.0528 - val_accuracy: 0.5821\n",
      "Epoch 144/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0482 - accuracy: 0.5801 - val_loss: 1.0433 - val_accuracy: 0.5868\n",
      "Epoch 145/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0450 - accuracy: 0.5773 - val_loss: 1.0525 - val_accuracy: 0.5901\n",
      "Epoch 146/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0391 - accuracy: 0.5814 - val_loss: 1.0512 - val_accuracy: 0.5824\n",
      "Epoch 147/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0414 - accuracy: 0.5791 - val_loss: 1.0952 - val_accuracy: 0.5649\n",
      "Epoch 148/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0464 - accuracy: 0.5782 - val_loss: 1.0646 - val_accuracy: 0.5765\n",
      "Epoch 149/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0407 - accuracy: 0.5809 - val_loss: 1.0439 - val_accuracy: 0.5857\n",
      "Epoch 150/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0393 - accuracy: 0.5826 - val_loss: 1.0680 - val_accuracy: 0.5782\n",
      "Epoch 151/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0393 - accuracy: 0.5824 - val_loss: 1.1019 - val_accuracy: 0.5655\n",
      "Epoch 152/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0415 - accuracy: 0.5802 - val_loss: 1.0456 - val_accuracy: 0.5931\n",
      "Epoch 153/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0430 - accuracy: 0.5803 - val_loss: 1.0424 - val_accuracy: 0.5862\n",
      "Epoch 154/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0363 - accuracy: 0.5833 - val_loss: 1.0464 - val_accuracy: 0.5895\n",
      "Epoch 155/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0368 - accuracy: 0.5809 - val_loss: 1.0785 - val_accuracy: 0.5774\n",
      "Epoch 156/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0370 - accuracy: 0.5810 - val_loss: 1.0474 - val_accuracy: 0.5928\n",
      "Epoch 157/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0398 - accuracy: 0.5805 - val_loss: 1.0477 - val_accuracy: 0.5824\n",
      "Epoch 158/174\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0392 - accuracy: 0.5791 - val_loss: 1.0469 - val_accuracy: 0.5836\n",
      "Epoch 159/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0373 - accuracy: 0.5801 - val_loss: 1.0529 - val_accuracy: 0.5898\n",
      "Epoch 160/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0382 - accuracy: 0.5822 - val_loss: 1.0483 - val_accuracy: 0.5815\n",
      "Epoch 161/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0380 - accuracy: 0.5829 - val_loss: 1.0427 - val_accuracy: 0.5857\n",
      "Epoch 162/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0475 - accuracy: 0.5786 - val_loss: 1.0508 - val_accuracy: 0.5910\n",
      "Epoch 163/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0398 - accuracy: 0.5831 - val_loss: 1.0464 - val_accuracy: 0.5830\n",
      "Epoch 164/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0371 - accuracy: 0.5839 - val_loss: 1.0691 - val_accuracy: 0.5765\n",
      "Epoch 165/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0431 - accuracy: 0.5809 - val_loss: 1.0458 - val_accuracy: 0.5922\n",
      "Epoch 166/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0407 - accuracy: 0.5814 - val_loss: 1.0458 - val_accuracy: 0.5824\n",
      "Epoch 167/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0351 - accuracy: 0.5849 - val_loss: 1.0498 - val_accuracy: 0.5851\n",
      "Epoch 168/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0416 - accuracy: 0.5828 - val_loss: 1.0414 - val_accuracy: 0.5851\n",
      "Epoch 169/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0374 - accuracy: 0.5787 - val_loss: 1.0412 - val_accuracy: 0.5860\n",
      "Epoch 170/174\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0362 - accuracy: 0.5829 - val_loss: 1.0506 - val_accuracy: 0.5845\n",
      "Epoch 171/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0361 - accuracy: 0.5822 - val_loss: 1.0444 - val_accuracy: 0.5889\n",
      "Epoch 172/174\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0348 - accuracy: 0.5823 - val_loss: 1.0520 - val_accuracy: 0.5889\n",
      "Epoch 173/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0366 - accuracy: 0.5808 - val_loss: 1.0396 - val_accuracy: 0.5901\n",
      "Epoch 174/174\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0329 - accuracy: 0.5852 - val_loss: 1.0546 - val_accuracy: 0.5851\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22a80bfa370>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tuner.hypermodel.build(best_hps)\n",
    "model.fit(X_train, y_train, epochs=best_epoch, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3f38298d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260/260 [==============================] - 0s 1ms/step\n",
      "(array([0, 1, 2, 3], dtype=int64), array([4337, 1962,    4, 2006], dtype=int64))\n",
      "(array([0, 1, 2, 3, 4], dtype=int64), array([3422, 1974,  601, 1734,  578], dtype=int64))\n",
      "(array([0, 1, 2, 3, 4], dtype=int64), array([6993, 3910, 1197, 3571, 1197], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(np.argmax(model.predict(X_test), axis=1), return_counts=True))\n",
    "print(np.unique(y_test, return_counts=True))\n",
    "print(np.unique(y_train, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f1ce9c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260/260 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5744373570826814"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, np.argmax(model.predict(X_test), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3cf54550",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"./model_d.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4501cc87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in .\\no_back\n",
      "Showing 10 best trials\n",
      "Objective(name='val_accuracy', direction='max')\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "n_neurons: 192\n",
      "learning_rate: 0.001\n",
      "beta_1: 0.95\n",
      "beta_2: 0.999\n",
      "Score: 0.5853893160820007\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "n_neurons: 208\n",
      "learning_rate: 0.001\n",
      "beta_1: 0.9\n",
      "beta_2: 0.999\n",
      "Score: 0.5838247537612915\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "n_neurons: 96\n",
      "learning_rate: 0.001\n",
      "beta_1: 0.9\n",
      "beta_2: 0.999\n",
      "Score: 0.5831026434898376\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "n_neurons: 128\n",
      "learning_rate: 0.001\n",
      "beta_1: 0.95\n",
      "beta_2: 0.9999\n",
      "Score: 0.5826212763786316\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "n_neurons: 112\n",
      "learning_rate: 0.001\n",
      "beta_1: 0.9\n",
      "beta_2: 0.9999\n",
      "Score: 0.5797328352928162\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "n_neurons: 96\n",
      "learning_rate: 0.001\n",
      "beta_1: 0.9\n",
      "beta_2: 0.99\n",
      "Score: 0.5793717503547668\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "n_neurons: 80\n",
      "learning_rate: 0.001\n",
      "beta_1: 0.95\n",
      "beta_2: 0.999\n",
      "Score: 0.578649640083313\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "n_neurons: 160\n",
      "learning_rate: 0.001\n",
      "beta_1: 0.9\n",
      "beta_2: 0.99\n",
      "Score: 0.5781682729721069\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "n_neurons: 64\n",
      "learning_rate: 0.001\n",
      "beta_1: 0.95\n",
      "beta_2: 0.999\n",
      "Score: 0.5768443942070007\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "n_neurons: 192\n",
      "learning_rate: 0.001\n",
      "beta_1: 0.85\n",
      "beta_2: 0.99\n",
      "Score: 0.5760019421577454\n"
     ]
    }
   ],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8b5fdf74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "422/422 [==============================] - 3s 5ms/step - loss: 1.4226 - accuracy: 0.4108 - val_loss: 1.4114 - val_accuracy: 0.4132\n",
      "Epoch 2/250\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.4066 - accuracy: 0.4149 - val_loss: 1.4110 - val_accuracy: 0.4132\n",
      "Epoch 3/250\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.4057 - accuracy: 0.4149 - val_loss: 1.4086 - val_accuracy: 0.4132\n",
      "Epoch 4/250\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.4001 - accuracy: 0.4149 - val_loss: 1.4019 - val_accuracy: 0.4132\n",
      "Epoch 5/250\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.3840 - accuracy: 0.4149 - val_loss: 1.3733 - val_accuracy: 0.4132\n",
      "Epoch 6/250\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.3560 - accuracy: 0.4180 - val_loss: 1.3400 - val_accuracy: 0.4188\n",
      "Epoch 7/250\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.3265 - accuracy: 0.4301 - val_loss: 1.3116 - val_accuracy: 0.4431\n",
      "Epoch 8/250\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.3029 - accuracy: 0.4467 - val_loss: 1.2893 - val_accuracy: 0.4603\n",
      "Epoch 9/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.2807 - accuracy: 0.4689 - val_loss: 1.2683 - val_accuracy: 0.4775\n",
      "Epoch 10/250\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.2651 - accuracy: 0.4782 - val_loss: 1.2686 - val_accuracy: 0.4884\n",
      "Epoch 11/250\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.2486 - accuracy: 0.4942 - val_loss: 1.2355 - val_accuracy: 0.5086\n",
      "Epoch 12/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.2337 - accuracy: 0.5019 - val_loss: 1.2329 - val_accuracy: 0.5003\n",
      "Epoch 13/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.2242 - accuracy: 0.5084 - val_loss: 1.2160 - val_accuracy: 0.5249\n",
      "Epoch 14/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.2188 - accuracy: 0.5104 - val_loss: 1.2137 - val_accuracy: 0.5130\n",
      "Epoch 15/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.2058 - accuracy: 0.5164 - val_loss: 1.2053 - val_accuracy: 0.5258\n",
      "Epoch 16/250\n",
      "422/422 [==============================] - 2s 6ms/step - loss: 1.1986 - accuracy: 0.5162 - val_loss: 1.2288 - val_accuracy: 0.5068\n",
      "Epoch 17/250\n",
      "422/422 [==============================] - 2s 6ms/step - loss: 1.1887 - accuracy: 0.5210 - val_loss: 1.1913 - val_accuracy: 0.5317\n",
      "Epoch 18/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.1846 - accuracy: 0.5207 - val_loss: 1.1789 - val_accuracy: 0.5480\n",
      "Epoch 19/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.1777 - accuracy: 0.5244 - val_loss: 1.1705 - val_accuracy: 0.5436\n",
      "Epoch 20/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1767 - accuracy: 0.5256 - val_loss: 1.1669 - val_accuracy: 0.5533\n",
      "Epoch 21/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1711 - accuracy: 0.5262 - val_loss: 1.1934 - val_accuracy: 0.5353\n",
      "Epoch 22/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1697 - accuracy: 0.5315 - val_loss: 1.1610 - val_accuracy: 0.5430\n",
      "Epoch 23/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1655 - accuracy: 0.5337 - val_loss: 1.1655 - val_accuracy: 0.5462\n",
      "Epoch 24/250\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.1636 - accuracy: 0.5302 - val_loss: 1.1669 - val_accuracy: 0.5317\n",
      "Epoch 25/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.1583 - accuracy: 0.5335 - val_loss: 1.2037 - val_accuracy: 0.5222\n",
      "Epoch 26/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1582 - accuracy: 0.5348 - val_loss: 1.1630 - val_accuracy: 0.5376\n",
      "Epoch 27/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1569 - accuracy: 0.5325 - val_loss: 1.1599 - val_accuracy: 0.5412\n",
      "Epoch 28/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1518 - accuracy: 0.5394 - val_loss: 1.1458 - val_accuracy: 0.5480\n",
      "Epoch 29/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1558 - accuracy: 0.5400 - val_loss: 1.1590 - val_accuracy: 0.5522\n",
      "Epoch 30/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1483 - accuracy: 0.5400 - val_loss: 1.1412 - val_accuracy: 0.5699\n",
      "Epoch 31/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1478 - accuracy: 0.5399 - val_loss: 1.1448 - val_accuracy: 0.5542\n",
      "Epoch 32/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1439 - accuracy: 0.5431 - val_loss: 1.1377 - val_accuracy: 0.5643\n",
      "Epoch 33/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1431 - accuracy: 0.5439 - val_loss: 1.1401 - val_accuracy: 0.5640\n",
      "Epoch 34/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1413 - accuracy: 0.5456 - val_loss: 1.1367 - val_accuracy: 0.5702\n",
      "Epoch 35/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1402 - accuracy: 0.5455 - val_loss: 1.1549 - val_accuracy: 0.5477\n",
      "Epoch 36/250\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.1388 - accuracy: 0.5469 - val_loss: 1.1342 - val_accuracy: 0.5575\n",
      "Epoch 37/250\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.1366 - accuracy: 0.5491 - val_loss: 1.1351 - val_accuracy: 0.5676\n",
      "Epoch 38/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1357 - accuracy: 0.5491 - val_loss: 1.1280 - val_accuracy: 0.5643\n",
      "Epoch 39/250\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.1319 - accuracy: 0.5556 - val_loss: 1.1288 - val_accuracy: 0.5685\n",
      "Epoch 40/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1362 - accuracy: 0.5514 - val_loss: 1.1419 - val_accuracy: 0.5599\n",
      "Epoch 41/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1332 - accuracy: 0.5477 - val_loss: 1.1251 - val_accuracy: 0.5711\n",
      "Epoch 42/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1322 - accuracy: 0.5502 - val_loss: 1.1252 - val_accuracy: 0.5661\n",
      "Epoch 43/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1281 - accuracy: 0.5516 - val_loss: 1.1366 - val_accuracy: 0.5531\n",
      "Epoch 44/250\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.1282 - accuracy: 0.5538 - val_loss: 1.1923 - val_accuracy: 0.5332\n",
      "Epoch 45/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1294 - accuracy: 0.5525 - val_loss: 1.1339 - val_accuracy: 0.5664\n",
      "Epoch 46/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1248 - accuracy: 0.5522 - val_loss: 1.1147 - val_accuracy: 0.5670\n",
      "Epoch 47/250\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.1256 - accuracy: 0.5537 - val_loss: 1.1210 - val_accuracy: 0.5750\n",
      "Epoch 48/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1182 - accuracy: 0.5565 - val_loss: 1.1368 - val_accuracy: 0.5501\n",
      "Epoch 49/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1215 - accuracy: 0.5511 - val_loss: 1.1121 - val_accuracy: 0.5711\n",
      "Epoch 50/250\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.1184 - accuracy: 0.5525 - val_loss: 1.1298 - val_accuracy: 0.5608\n",
      "Epoch 51/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1155 - accuracy: 0.5542 - val_loss: 1.1034 - val_accuracy: 0.5688\n",
      "Epoch 52/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1147 - accuracy: 0.5545 - val_loss: 1.1403 - val_accuracy: 0.5329\n",
      "Epoch 53/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1131 - accuracy: 0.5553 - val_loss: 1.1648 - val_accuracy: 0.5498\n",
      "Epoch 54/250\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.1135 - accuracy: 0.5554 - val_loss: 1.1052 - val_accuracy: 0.5762\n",
      "Epoch 55/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1070 - accuracy: 0.5593 - val_loss: 1.1021 - val_accuracy: 0.5732\n",
      "Epoch 56/250\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.1073 - accuracy: 0.5596 - val_loss: 1.1077 - val_accuracy: 0.5658\n",
      "Epoch 57/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1066 - accuracy: 0.5575 - val_loss: 1.0979 - val_accuracy: 0.5699\n",
      "Epoch 58/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1063 - accuracy: 0.5562 - val_loss: 1.0976 - val_accuracy: 0.5774\n",
      "Epoch 59/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1010 - accuracy: 0.5617 - val_loss: 1.1017 - val_accuracy: 0.5619\n",
      "Epoch 60/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1057 - accuracy: 0.5595 - val_loss: 1.0981 - val_accuracy: 0.5723\n",
      "Epoch 61/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1041 - accuracy: 0.5567 - val_loss: 1.1123 - val_accuracy: 0.5575\n",
      "Epoch 62/250\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.1033 - accuracy: 0.5589 - val_loss: 1.1045 - val_accuracy: 0.5673\n",
      "Epoch 63/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1007 - accuracy: 0.5580 - val_loss: 1.1024 - val_accuracy: 0.5625\n",
      "Epoch 64/250\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.1045 - accuracy: 0.5577 - val_loss: 1.1289 - val_accuracy: 0.5670\n",
      "Epoch 65/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0973 - accuracy: 0.5615 - val_loss: 1.0974 - val_accuracy: 0.5744\n",
      "Epoch 66/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0962 - accuracy: 0.5628 - val_loss: 1.0901 - val_accuracy: 0.5765\n",
      "Epoch 67/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0982 - accuracy: 0.5620 - val_loss: 1.1136 - val_accuracy: 0.5477\n",
      "Epoch 68/250\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0960 - accuracy: 0.5631 - val_loss: 1.1099 - val_accuracy: 0.5682\n",
      "Epoch 69/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0955 - accuracy: 0.5640 - val_loss: 1.0914 - val_accuracy: 0.5797\n",
      "Epoch 70/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0945 - accuracy: 0.5659 - val_loss: 1.0889 - val_accuracy: 0.5714\n",
      "Epoch 71/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0972 - accuracy: 0.5603 - val_loss: 1.0873 - val_accuracy: 0.5732\n",
      "Epoch 72/250\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0910 - accuracy: 0.5657 - val_loss: 1.0999 - val_accuracy: 0.5676\n",
      "Epoch 73/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0948 - accuracy: 0.5619 - val_loss: 1.1233 - val_accuracy: 0.5397\n",
      "Epoch 74/250\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0916 - accuracy: 0.5629 - val_loss: 1.0880 - val_accuracy: 0.5691\n",
      "Epoch 75/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0932 - accuracy: 0.5650 - val_loss: 1.0840 - val_accuracy: 0.5744\n",
      "Epoch 76/250\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0900 - accuracy: 0.5651 - val_loss: 1.0999 - val_accuracy: 0.5685\n",
      "Epoch 77/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0890 - accuracy: 0.5651 - val_loss: 1.0820 - val_accuracy: 0.5771\n",
      "Epoch 78/250\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0901 - accuracy: 0.5643 - val_loss: 1.0838 - val_accuracy: 0.5821\n",
      "Epoch 79/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0909 - accuracy: 0.5657 - val_loss: 1.0992 - val_accuracy: 0.5791\n",
      "Epoch 80/250\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0932 - accuracy: 0.5624 - val_loss: 1.0977 - val_accuracy: 0.5673\n",
      "Epoch 81/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0874 - accuracy: 0.5655 - val_loss: 1.1215 - val_accuracy: 0.5498\n",
      "Epoch 82/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0886 - accuracy: 0.5649 - val_loss: 1.0961 - val_accuracy: 0.5717\n",
      "Epoch 83/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0892 - accuracy: 0.5666 - val_loss: 1.0938 - val_accuracy: 0.5839\n",
      "Epoch 84/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0859 - accuracy: 0.5648 - val_loss: 1.0805 - val_accuracy: 0.5833\n",
      "Epoch 85/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0871 - accuracy: 0.5701 - val_loss: 1.0970 - val_accuracy: 0.5631\n",
      "Epoch 86/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0892 - accuracy: 0.5650 - val_loss: 1.0845 - val_accuracy: 0.5738\n",
      "Epoch 87/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0823 - accuracy: 0.5674 - val_loss: 1.0892 - val_accuracy: 0.5643\n",
      "Epoch 88/250\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0822 - accuracy: 0.5694 - val_loss: 1.0790 - val_accuracy: 0.5705\n",
      "Epoch 89/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0841 - accuracy: 0.5670 - val_loss: 1.0954 - val_accuracy: 0.5744\n",
      "Epoch 90/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0824 - accuracy: 0.5668 - val_loss: 1.0768 - val_accuracy: 0.5871\n",
      "Epoch 91/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0800 - accuracy: 0.5685 - val_loss: 1.0889 - val_accuracy: 0.5673\n",
      "Epoch 92/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0830 - accuracy: 0.5655 - val_loss: 1.0759 - val_accuracy: 0.5717\n",
      "Epoch 93/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0813 - accuracy: 0.5664 - val_loss: 1.0824 - val_accuracy: 0.5854\n",
      "Epoch 94/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0824 - accuracy: 0.5683 - val_loss: 1.0734 - val_accuracy: 0.5874\n",
      "Epoch 95/250\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0769 - accuracy: 0.5686 - val_loss: 1.0754 - val_accuracy: 0.5806\n",
      "Epoch 96/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0785 - accuracy: 0.5700 - val_loss: 1.0840 - val_accuracy: 0.5833\n",
      "Epoch 97/250\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0799 - accuracy: 0.5702 - val_loss: 1.0958 - val_accuracy: 0.5643\n",
      "Epoch 98/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0767 - accuracy: 0.5688 - val_loss: 1.0678 - val_accuracy: 0.5794\n",
      "Epoch 99/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0785 - accuracy: 0.5678 - val_loss: 1.0746 - val_accuracy: 0.5830\n",
      "Epoch 100/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0775 - accuracy: 0.5680 - val_loss: 1.0839 - val_accuracy: 0.5812\n",
      "Epoch 101/250\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0785 - accuracy: 0.5680 - val_loss: 1.0915 - val_accuracy: 0.5637\n",
      "Epoch 102/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0758 - accuracy: 0.5679 - val_loss: 1.0748 - val_accuracy: 0.5777\n",
      "Epoch 103/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0753 - accuracy: 0.5684 - val_loss: 1.0836 - val_accuracy: 0.5744\n",
      "Epoch 104/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0728 - accuracy: 0.5726 - val_loss: 1.0687 - val_accuracy: 0.5827\n",
      "Epoch 105/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0738 - accuracy: 0.5691 - val_loss: 1.0862 - val_accuracy: 0.5779\n",
      "Epoch 106/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0720 - accuracy: 0.5721 - val_loss: 1.0835 - val_accuracy: 0.5857\n",
      "Epoch 107/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0747 - accuracy: 0.5726 - val_loss: 1.0676 - val_accuracy: 0.5806\n",
      "Epoch 108/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0734 - accuracy: 0.5719 - val_loss: 1.0759 - val_accuracy: 0.5658\n",
      "Epoch 109/250\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0721 - accuracy: 0.5711 - val_loss: 1.0862 - val_accuracy: 0.5726\n",
      "Epoch 110/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0716 - accuracy: 0.5707 - val_loss: 1.0921 - val_accuracy: 0.5791\n",
      "Epoch 111/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0681 - accuracy: 0.5701 - val_loss: 1.0882 - val_accuracy: 0.5782\n",
      "Epoch 112/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0686 - accuracy: 0.5742 - val_loss: 1.0776 - val_accuracy: 0.5851\n",
      "Epoch 113/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0705 - accuracy: 0.5700 - val_loss: 1.0676 - val_accuracy: 0.5871\n",
      "Epoch 114/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0670 - accuracy: 0.5711 - val_loss: 1.0693 - val_accuracy: 0.5750\n",
      "Epoch 115/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0653 - accuracy: 0.5714 - val_loss: 1.0803 - val_accuracy: 0.5803\n",
      "Epoch 116/250\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0669 - accuracy: 0.5716 - val_loss: 1.0862 - val_accuracy: 0.5797\n",
      "Epoch 117/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0670 - accuracy: 0.5725 - val_loss: 1.0628 - val_accuracy: 0.5877\n",
      "Epoch 118/250\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0645 - accuracy: 0.5743 - val_loss: 1.0694 - val_accuracy: 0.5723\n",
      "Epoch 119/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0668 - accuracy: 0.5726 - val_loss: 1.0904 - val_accuracy: 0.5771\n",
      "Epoch 120/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0661 - accuracy: 0.5756 - val_loss: 1.0647 - val_accuracy: 0.5901\n",
      "Epoch 121/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0615 - accuracy: 0.5739 - val_loss: 1.0744 - val_accuracy: 0.5812\n",
      "Epoch 122/250\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0645 - accuracy: 0.5717 - val_loss: 1.0696 - val_accuracy: 0.5865\n",
      "Epoch 123/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0613 - accuracy: 0.5740 - val_loss: 1.0597 - val_accuracy: 0.5830\n",
      "Epoch 124/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0587 - accuracy: 0.5754 - val_loss: 1.0558 - val_accuracy: 0.5892\n",
      "Epoch 125/250\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0630 - accuracy: 0.5688 - val_loss: 1.0615 - val_accuracy: 0.5910\n",
      "Epoch 126/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0632 - accuracy: 0.5700 - val_loss: 1.0685 - val_accuracy: 0.5800\n",
      "Epoch 127/250\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0646 - accuracy: 0.5731 - val_loss: 1.0627 - val_accuracy: 0.5821\n",
      "Epoch 128/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0573 - accuracy: 0.5751 - val_loss: 1.0625 - val_accuracy: 0.5907\n",
      "Epoch 129/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0599 - accuracy: 0.5764 - val_loss: 1.0590 - val_accuracy: 0.5860\n",
      "Epoch 130/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0572 - accuracy: 0.5757 - val_loss: 1.0819 - val_accuracy: 0.5824\n",
      "Epoch 131/250\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0598 - accuracy: 0.5771 - val_loss: 1.0612 - val_accuracy: 0.5788\n",
      "Epoch 132/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0595 - accuracy: 0.5746 - val_loss: 1.1219 - val_accuracy: 0.5554\n",
      "Epoch 133/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0595 - accuracy: 0.5743 - val_loss: 1.0516 - val_accuracy: 0.5865\n",
      "Epoch 134/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0588 - accuracy: 0.5768 - val_loss: 1.0536 - val_accuracy: 0.5898\n",
      "Epoch 135/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0558 - accuracy: 0.5765 - val_loss: 1.0712 - val_accuracy: 0.5815\n",
      "Epoch 136/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0552 - accuracy: 0.5778 - val_loss: 1.0749 - val_accuracy: 0.5934\n",
      "Epoch 137/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0573 - accuracy: 0.5755 - val_loss: 1.0580 - val_accuracy: 0.5845\n",
      "Epoch 138/250\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0564 - accuracy: 0.5746 - val_loss: 1.0780 - val_accuracy: 0.5839\n",
      "Epoch 139/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0572 - accuracy: 0.5741 - val_loss: 1.0786 - val_accuracy: 0.5729\n",
      "Epoch 140/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0545 - accuracy: 0.5775 - val_loss: 1.0580 - val_accuracy: 0.5785\n",
      "Epoch 141/250\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0567 - accuracy: 0.5747 - val_loss: 1.0850 - val_accuracy: 0.5848\n",
      "Epoch 142/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0585 - accuracy: 0.5748 - val_loss: 1.0804 - val_accuracy: 0.5771\n",
      "Epoch 143/250\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0532 - accuracy: 0.5780 - val_loss: 1.0797 - val_accuracy: 0.5892\n",
      "Epoch 144/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0585 - accuracy: 0.5750 - val_loss: 1.0512 - val_accuracy: 0.5916\n",
      "Epoch 145/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0535 - accuracy: 0.5767 - val_loss: 1.0522 - val_accuracy: 0.5895\n",
      "Epoch 146/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0554 - accuracy: 0.5788 - val_loss: 1.0823 - val_accuracy: 0.5699\n",
      "Epoch 147/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0565 - accuracy: 0.5754 - val_loss: 1.0616 - val_accuracy: 0.5934\n",
      "Epoch 148/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0565 - accuracy: 0.5768 - val_loss: 1.0623 - val_accuracy: 0.5830\n",
      "Epoch 149/250\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0540 - accuracy: 0.5777 - val_loss: 1.0624 - val_accuracy: 0.5925\n",
      "Epoch 150/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0533 - accuracy: 0.5757 - val_loss: 1.0690 - val_accuracy: 0.5809\n",
      "Epoch 151/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0544 - accuracy: 0.5769 - val_loss: 1.0546 - val_accuracy: 0.5806\n",
      "Epoch 152/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0497 - accuracy: 0.5787 - val_loss: 1.0676 - val_accuracy: 0.5791\n",
      "Epoch 153/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0541 - accuracy: 0.5759 - val_loss: 1.0474 - val_accuracy: 0.5916\n",
      "Epoch 154/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0525 - accuracy: 0.5768 - val_loss: 1.0662 - val_accuracy: 0.5922\n",
      "Epoch 155/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0534 - accuracy: 0.5770 - val_loss: 1.0656 - val_accuracy: 0.5815\n",
      "Epoch 156/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0488 - accuracy: 0.5788 - val_loss: 1.0499 - val_accuracy: 0.5889\n",
      "Epoch 157/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0518 - accuracy: 0.5800 - val_loss: 1.0535 - val_accuracy: 0.5833\n",
      "Epoch 158/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0463 - accuracy: 0.5794 - val_loss: 1.0456 - val_accuracy: 0.5922\n",
      "Epoch 159/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0489 - accuracy: 0.5784 - val_loss: 1.0552 - val_accuracy: 0.5960\n",
      "Epoch 160/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0491 - accuracy: 0.5786 - val_loss: 1.0501 - val_accuracy: 0.5892\n",
      "Epoch 161/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0454 - accuracy: 0.5796 - val_loss: 1.0471 - val_accuracy: 0.5913\n",
      "Epoch 162/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0487 - accuracy: 0.5786 - val_loss: 1.0555 - val_accuracy: 0.5886\n",
      "Epoch 163/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0494 - accuracy: 0.5788 - val_loss: 1.0537 - val_accuracy: 0.5904\n",
      "Epoch 164/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0450 - accuracy: 0.5820 - val_loss: 1.0443 - val_accuracy: 0.5889\n",
      "Epoch 165/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0452 - accuracy: 0.5829 - val_loss: 1.0565 - val_accuracy: 0.5830\n",
      "Epoch 166/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0434 - accuracy: 0.5817 - val_loss: 1.0598 - val_accuracy: 0.5836\n",
      "Epoch 167/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0449 - accuracy: 0.5803 - val_loss: 1.0543 - val_accuracy: 0.5860\n",
      "Epoch 168/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0452 - accuracy: 0.5809 - val_loss: 1.0440 - val_accuracy: 0.5951\n",
      "Epoch 169/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0447 - accuracy: 0.5822 - val_loss: 1.0502 - val_accuracy: 0.5972\n",
      "Epoch 170/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0452 - accuracy: 0.5813 - val_loss: 1.0481 - val_accuracy: 0.5865\n",
      "Epoch 171/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0448 - accuracy: 0.5817 - val_loss: 1.0482 - val_accuracy: 0.5848\n",
      "Epoch 172/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0449 - accuracy: 0.5806 - val_loss: 1.0559 - val_accuracy: 0.5854\n",
      "Epoch 173/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0455 - accuracy: 0.5815 - val_loss: 1.0558 - val_accuracy: 0.5940\n",
      "Epoch 174/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0440 - accuracy: 0.5798 - val_loss: 1.0518 - val_accuracy: 0.5937\n",
      "Epoch 175/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0426 - accuracy: 0.5831 - val_loss: 1.0661 - val_accuracy: 0.5830\n",
      "Epoch 176/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0411 - accuracy: 0.5829 - val_loss: 1.0513 - val_accuracy: 0.5824\n",
      "Epoch 177/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0432 - accuracy: 0.5820 - val_loss: 1.0506 - val_accuracy: 0.5945\n",
      "Epoch 178/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0434 - accuracy: 0.5799 - val_loss: 1.0391 - val_accuracy: 0.5934\n",
      "Epoch 179/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0419 - accuracy: 0.5829 - val_loss: 1.0525 - val_accuracy: 0.5937\n",
      "Epoch 180/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0391 - accuracy: 0.5842 - val_loss: 1.0452 - val_accuracy: 0.5871\n",
      "Epoch 181/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0439 - accuracy: 0.5794 - val_loss: 1.0608 - val_accuracy: 0.5943\n",
      "Epoch 182/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0403 - accuracy: 0.5828 - val_loss: 1.0454 - val_accuracy: 0.5883\n",
      "Epoch 183/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0399 - accuracy: 0.5832 - val_loss: 1.0412 - val_accuracy: 0.5969\n",
      "Epoch 184/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0388 - accuracy: 0.5826 - val_loss: 1.0527 - val_accuracy: 0.5788\n",
      "Epoch 185/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0413 - accuracy: 0.5852 - val_loss: 1.0636 - val_accuracy: 0.5782\n",
      "Epoch 186/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0384 - accuracy: 0.5844 - val_loss: 1.0506 - val_accuracy: 0.5842\n",
      "Epoch 187/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0397 - accuracy: 0.5824 - val_loss: 1.0471 - val_accuracy: 0.5954\n",
      "Epoch 188/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0418 - accuracy: 0.5829 - val_loss: 1.0418 - val_accuracy: 0.5922\n",
      "Epoch 189/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0457 - accuracy: 0.5786 - val_loss: 1.0470 - val_accuracy: 0.5845\n",
      "Epoch 190/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0390 - accuracy: 0.5874 - val_loss: 1.0457 - val_accuracy: 0.5975\n",
      "Epoch 191/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0393 - accuracy: 0.5837 - val_loss: 1.0598 - val_accuracy: 0.5806\n",
      "Epoch 192/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0402 - accuracy: 0.5871 - val_loss: 1.0368 - val_accuracy: 0.5919\n",
      "Epoch 193/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0383 - accuracy: 0.5823 - val_loss: 1.0582 - val_accuracy: 0.5862\n",
      "Epoch 194/250\n",
      "422/422 [==============================] - 3s 6ms/step - loss: 1.0384 - accuracy: 0.5840 - val_loss: 1.0575 - val_accuracy: 0.5845\n",
      "Epoch 195/250\n",
      "422/422 [==============================] - 2s 6ms/step - loss: 1.0392 - accuracy: 0.5823 - val_loss: 1.0494 - val_accuracy: 0.5848\n",
      "Epoch 196/250\n",
      "422/422 [==============================] - 2s 6ms/step - loss: 1.0424 - accuracy: 0.5829 - val_loss: 1.0558 - val_accuracy: 0.5886\n",
      "Epoch 197/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0379 - accuracy: 0.5848 - val_loss: 1.0450 - val_accuracy: 0.5886\n",
      "Epoch 198/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0408 - accuracy: 0.5810 - val_loss: 1.0406 - val_accuracy: 0.5883\n",
      "Epoch 199/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0382 - accuracy: 0.5822 - val_loss: 1.0515 - val_accuracy: 0.5842\n",
      "Epoch 200/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0364 - accuracy: 0.5841 - val_loss: 1.0369 - val_accuracy: 0.5948\n",
      "Epoch 201/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0348 - accuracy: 0.5859 - val_loss: 1.0433 - val_accuracy: 0.5916\n",
      "Epoch 202/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0378 - accuracy: 0.5849 - val_loss: 1.0546 - val_accuracy: 0.5960\n",
      "Epoch 203/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0353 - accuracy: 0.5843 - val_loss: 1.0521 - val_accuracy: 0.5925\n",
      "Epoch 204/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0347 - accuracy: 0.5843 - val_loss: 1.0486 - val_accuracy: 0.5880\n",
      "Epoch 205/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0339 - accuracy: 0.5832 - val_loss: 1.0491 - val_accuracy: 0.5913\n",
      "Epoch 206/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0359 - accuracy: 0.5830 - val_loss: 1.0659 - val_accuracy: 0.5904\n",
      "Epoch 207/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0397 - accuracy: 0.5785 - val_loss: 1.0387 - val_accuracy: 0.5981\n",
      "Epoch 208/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0371 - accuracy: 0.5823 - val_loss: 1.0328 - val_accuracy: 0.5919\n",
      "Epoch 209/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0372 - accuracy: 0.5834 - val_loss: 1.0444 - val_accuracy: 0.5981\n",
      "Epoch 210/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0355 - accuracy: 0.5829 - val_loss: 1.0351 - val_accuracy: 0.5945\n",
      "Epoch 211/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0341 - accuracy: 0.5843 - val_loss: 1.0479 - val_accuracy: 0.5948\n",
      "Epoch 212/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0319 - accuracy: 0.5836 - val_loss: 1.0456 - val_accuracy: 0.5877\n",
      "Epoch 213/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0355 - accuracy: 0.5846 - val_loss: 1.0365 - val_accuracy: 0.5954\n",
      "Epoch 214/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0339 - accuracy: 0.5849 - val_loss: 1.0343 - val_accuracy: 0.5945\n",
      "Epoch 215/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0362 - accuracy: 0.5849 - val_loss: 1.0451 - val_accuracy: 0.5937\n",
      "Epoch 216/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0323 - accuracy: 0.5814 - val_loss: 1.0513 - val_accuracy: 0.5945\n",
      "Epoch 217/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0316 - accuracy: 0.5819 - val_loss: 1.0385 - val_accuracy: 0.5913\n",
      "Epoch 218/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0369 - accuracy: 0.5854 - val_loss: 1.0523 - val_accuracy: 0.5919\n",
      "Epoch 219/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0329 - accuracy: 0.5870 - val_loss: 1.0365 - val_accuracy: 0.5919\n",
      "Epoch 220/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0327 - accuracy: 0.5835 - val_loss: 1.0314 - val_accuracy: 0.5945\n",
      "Epoch 221/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0328 - accuracy: 0.5837 - val_loss: 1.0401 - val_accuracy: 0.5975\n",
      "Epoch 222/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0345 - accuracy: 0.5831 - val_loss: 1.0584 - val_accuracy: 0.5889\n",
      "Epoch 223/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0363 - accuracy: 0.5856 - val_loss: 1.0409 - val_accuracy: 0.5877\n",
      "Epoch 224/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0370 - accuracy: 0.5822 - val_loss: 1.1075 - val_accuracy: 0.5664\n",
      "Epoch 225/250\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0335 - accuracy: 0.5844 - val_loss: 1.0593 - val_accuracy: 0.5830\n",
      "Epoch 226/250\n",
      "422/422 [==============================] - 2s 6ms/step - loss: 1.0309 - accuracy: 0.5843 - val_loss: 1.0410 - val_accuracy: 0.5966\n",
      "Epoch 227/250\n",
      "422/422 [==============================] - 2s 6ms/step - loss: 1.0327 - accuracy: 0.5868 - val_loss: 1.1445 - val_accuracy: 0.5477\n",
      "Epoch 228/250\n",
      "422/422 [==============================] - 2s 6ms/step - loss: 1.0334 - accuracy: 0.5880 - val_loss: 1.0386 - val_accuracy: 0.5978\n",
      "Epoch 229/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0304 - accuracy: 0.5884 - val_loss: 1.0553 - val_accuracy: 0.5842\n",
      "Epoch 230/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0344 - accuracy: 0.5846 - val_loss: 1.0369 - val_accuracy: 0.5889\n",
      "Epoch 231/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0302 - accuracy: 0.5855 - val_loss: 1.0344 - val_accuracy: 0.5928\n",
      "Epoch 232/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0349 - accuracy: 0.5837 - val_loss: 1.0467 - val_accuracy: 0.5874\n",
      "Epoch 233/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0292 - accuracy: 0.5843 - val_loss: 1.0341 - val_accuracy: 0.6002\n",
      "Epoch 234/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0303 - accuracy: 0.5883 - val_loss: 1.0325 - val_accuracy: 0.5948\n",
      "Epoch 235/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0291 - accuracy: 0.5867 - val_loss: 1.0377 - val_accuracy: 0.5922\n",
      "Epoch 236/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0336 - accuracy: 0.5822 - val_loss: 1.0342 - val_accuracy: 0.5913\n",
      "Epoch 237/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0311 - accuracy: 0.5859 - val_loss: 1.0528 - val_accuracy: 0.5842\n",
      "Epoch 238/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0301 - accuracy: 0.5841 - val_loss: 1.0303 - val_accuracy: 0.5913\n",
      "Epoch 239/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0313 - accuracy: 0.5858 - val_loss: 1.0406 - val_accuracy: 0.5990\n",
      "Epoch 240/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0304 - accuracy: 0.5868 - val_loss: 1.0383 - val_accuracy: 0.5951\n",
      "Epoch 241/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0315 - accuracy: 0.5870 - val_loss: 1.0418 - val_accuracy: 0.5937\n",
      "Epoch 242/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0355 - accuracy: 0.5856 - val_loss: 1.0448 - val_accuracy: 0.5937\n",
      "Epoch 243/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0291 - accuracy: 0.5847 - val_loss: 1.0649 - val_accuracy: 0.5800\n",
      "Epoch 244/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0290 - accuracy: 0.5849 - val_loss: 1.0419 - val_accuracy: 0.5963\n",
      "Epoch 245/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0305 - accuracy: 0.5838 - val_loss: 1.0327 - val_accuracy: 0.5943\n",
      "Epoch 246/250\n",
      "422/422 [==============================] - 3s 6ms/step - loss: 1.0338 - accuracy: 0.5833 - val_loss: 1.0364 - val_accuracy: 0.5895\n",
      "Epoch 247/250\n",
      "422/422 [==============================] - 2s 6ms/step - loss: 1.0296 - accuracy: 0.5854 - val_loss: 1.0368 - val_accuracy: 0.5860\n",
      "Epoch 248/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0281 - accuracy: 0.5851 - val_loss: 1.0483 - val_accuracy: 0.5898\n",
      "Epoch 249/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0287 - accuracy: 0.5857 - val_loss: 1.0337 - val_accuracy: 0.5889\n",
      "Epoch 250/250\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0302 - accuracy: 0.5858 - val_loss: 1.0406 - val_accuracy: 0.5922\n"
     ]
    }
   ],
   "source": [
    "model_a = build_model_a()\n",
    "model_b = build_model_b()\n",
    "\n",
    "hist1 = model_a.fit(X_train, y_train, epochs=250, validation_split=0.2)\n",
    "hist2 = model_b.fit(X_train, y_train, epochs=250, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2899e224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist1.history[\"accuracy\"], label=\"accuracy\")\n",
    "plt.plot(hist1.history[\"val_accuracy\"], label=\"val_accuracy\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.legend([\"train\", \"test\"], loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8e0a187a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/215\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.4207 - accuracy: 0.4149 - val_loss: 1.4119 - val_accuracy: 0.4132\n",
      "Epoch 2/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.4064 - accuracy: 0.4149 - val_loss: 1.4111 - val_accuracy: 0.4132\n",
      "Epoch 3/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.4048 - accuracy: 0.4149 - val_loss: 1.4080 - val_accuracy: 0.4132\n",
      "Epoch 4/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.4016 - accuracy: 0.4149 - val_loss: 1.4096 - val_accuracy: 0.4132\n",
      "Epoch 5/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.3985 - accuracy: 0.4149 - val_loss: 1.4065 - val_accuracy: 0.4132\n",
      "Epoch 6/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.3967 - accuracy: 0.4149 - val_loss: 1.4017 - val_accuracy: 0.4132\n",
      "Epoch 7/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.3908 - accuracy: 0.4160 - val_loss: 1.3958 - val_accuracy: 0.4152\n",
      "Epoch 8/215\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.3795 - accuracy: 0.4207 - val_loss: 1.3807 - val_accuracy: 0.4247\n",
      "Epoch 9/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.3589 - accuracy: 0.4297 - val_loss: 1.3519 - val_accuracy: 0.4366\n",
      "Epoch 10/215\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.3363 - accuracy: 0.4461 - val_loss: 1.3215 - val_accuracy: 0.4585\n",
      "Epoch 11/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.3096 - accuracy: 0.4594 - val_loss: 1.3105 - val_accuracy: 0.4677\n",
      "Epoch 12/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.2837 - accuracy: 0.4656 - val_loss: 1.2743 - val_accuracy: 0.4887\n",
      "Epoch 13/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.2597 - accuracy: 0.4862 - val_loss: 1.2557 - val_accuracy: 0.4849\n",
      "Epoch 14/215\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.2454 - accuracy: 0.4984 - val_loss: 1.2538 - val_accuracy: 0.4947\n",
      "Epoch 15/215\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.2358 - accuracy: 0.5041 - val_loss: 1.2441 - val_accuracy: 0.4953\n",
      "Epoch 16/215\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.2270 - accuracy: 0.5062 - val_loss: 1.2156 - val_accuracy: 0.5074\n",
      "Epoch 17/215\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.2161 - accuracy: 0.5099 - val_loss: 1.2070 - val_accuracy: 0.5311\n",
      "Epoch 18/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.2050 - accuracy: 0.5202 - val_loss: 1.2016 - val_accuracy: 0.5130\n",
      "Epoch 19/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.1974 - accuracy: 0.5229 - val_loss: 1.1886 - val_accuracy: 0.5403\n",
      "Epoch 20/215\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.1886 - accuracy: 0.5277 - val_loss: 1.1812 - val_accuracy: 0.5400\n",
      "Epoch 21/215\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1863 - accuracy: 0.5277 - val_loss: 1.1854 - val_accuracy: 0.5439\n",
      "Epoch 22/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.1767 - accuracy: 0.5346 - val_loss: 1.2095 - val_accuracy: 0.5086\n",
      "Epoch 23/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.1739 - accuracy: 0.5325 - val_loss: 1.1698 - val_accuracy: 0.5397\n",
      "Epoch 24/215\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.1694 - accuracy: 0.5376 - val_loss: 1.1825 - val_accuracy: 0.5193\n",
      "Epoch 25/215\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.1641 - accuracy: 0.5408 - val_loss: 1.1545 - val_accuracy: 0.5539\n",
      "Epoch 26/215\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.1514 - accuracy: 0.5431 - val_loss: 1.1449 - val_accuracy: 0.5602\n",
      "Epoch 27/215\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.1480 - accuracy: 0.5400 - val_loss: 1.1427 - val_accuracy: 0.5510\n",
      "Epoch 28/215\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.1403 - accuracy: 0.5493 - val_loss: 1.1360 - val_accuracy: 0.5649\n",
      "Epoch 29/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.1407 - accuracy: 0.5452 - val_loss: 1.1420 - val_accuracy: 0.5628\n",
      "Epoch 30/215\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.1347 - accuracy: 0.5457 - val_loss: 1.1401 - val_accuracy: 0.5637\n",
      "Epoch 31/215\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.1312 - accuracy: 0.5478 - val_loss: 1.1280 - val_accuracy: 0.5697\n",
      "Epoch 32/215\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.1273 - accuracy: 0.5518 - val_loss: 1.1324 - val_accuracy: 0.5619\n",
      "Epoch 33/215\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.1264 - accuracy: 0.5504 - val_loss: 1.1173 - val_accuracy: 0.5720\n",
      "Epoch 34/215\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1223 - accuracy: 0.5528 - val_loss: 1.1204 - val_accuracy: 0.5634\n",
      "Epoch 35/215\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.1207 - accuracy: 0.5530 - val_loss: 1.1493 - val_accuracy: 0.5412\n",
      "Epoch 36/215\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.1201 - accuracy: 0.5537 - val_loss: 1.1186 - val_accuracy: 0.5708\n",
      "Epoch 37/215\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.1148 - accuracy: 0.5534 - val_loss: 1.1143 - val_accuracy: 0.5691\n",
      "Epoch 38/215\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.1157 - accuracy: 0.5541 - val_loss: 1.1072 - val_accuracy: 0.5705\n",
      "Epoch 39/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.1134 - accuracy: 0.5520 - val_loss: 1.1207 - val_accuracy: 0.5708\n",
      "Epoch 40/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.1112 - accuracy: 0.5571 - val_loss: 1.1101 - val_accuracy: 0.5694\n",
      "Epoch 41/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.1075 - accuracy: 0.5584 - val_loss: 1.1012 - val_accuracy: 0.5691\n",
      "Epoch 42/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.1065 - accuracy: 0.5543 - val_loss: 1.1011 - val_accuracy: 0.5726\n",
      "Epoch 43/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.1030 - accuracy: 0.5598 - val_loss: 1.1062 - val_accuracy: 0.5711\n",
      "Epoch 44/215\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.1036 - accuracy: 0.5574 - val_loss: 1.1040 - val_accuracy: 0.5676\n",
      "Epoch 45/215\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.1029 - accuracy: 0.5551 - val_loss: 1.1199 - val_accuracy: 0.5495\n",
      "Epoch 46/215\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.1014 - accuracy: 0.5582 - val_loss: 1.1478 - val_accuracy: 0.5370\n",
      "Epoch 47/215\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0973 - accuracy: 0.5622 - val_loss: 1.1053 - val_accuracy: 0.5699\n",
      "Epoch 48/215\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0995 - accuracy: 0.5591 - val_loss: 1.0936 - val_accuracy: 0.5791\n",
      "Epoch 49/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0984 - accuracy: 0.5599 - val_loss: 1.1029 - val_accuracy: 0.5655\n",
      "Epoch 50/215\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0951 - accuracy: 0.5587 - val_loss: 1.0954 - val_accuracy: 0.5726\n",
      "Epoch 51/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0923 - accuracy: 0.5621 - val_loss: 1.0896 - val_accuracy: 0.5723\n",
      "Epoch 52/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0926 - accuracy: 0.5611 - val_loss: 1.1058 - val_accuracy: 0.5708\n",
      "Epoch 53/215\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0907 - accuracy: 0.5629 - val_loss: 1.1163 - val_accuracy: 0.5599\n",
      "Epoch 54/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0899 - accuracy: 0.5637 - val_loss: 1.0807 - val_accuracy: 0.5774\n",
      "Epoch 55/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0850 - accuracy: 0.5620 - val_loss: 1.0937 - val_accuracy: 0.5762\n",
      "Epoch 56/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0845 - accuracy: 0.5657 - val_loss: 1.0876 - val_accuracy: 0.5670\n",
      "Epoch 57/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0840 - accuracy: 0.5628 - val_loss: 1.0976 - val_accuracy: 0.5563\n",
      "Epoch 58/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0851 - accuracy: 0.5604 - val_loss: 1.1040 - val_accuracy: 0.5673\n",
      "Epoch 59/215\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0845 - accuracy: 0.5653 - val_loss: 1.1081 - val_accuracy: 0.5533\n",
      "Epoch 60/215\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0827 - accuracy: 0.5664 - val_loss: 1.0851 - val_accuracy: 0.5771\n",
      "Epoch 61/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0785 - accuracy: 0.5673 - val_loss: 1.0853 - val_accuracy: 0.5779\n",
      "Epoch 62/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0786 - accuracy: 0.5663 - val_loss: 1.1002 - val_accuracy: 0.5587\n",
      "Epoch 63/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0753 - accuracy: 0.5700 - val_loss: 1.0750 - val_accuracy: 0.5809\n",
      "Epoch 64/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0808 - accuracy: 0.5640 - val_loss: 1.0881 - val_accuracy: 0.5779\n",
      "Epoch 65/215\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0753 - accuracy: 0.5662 - val_loss: 1.0810 - val_accuracy: 0.5744\n",
      "Epoch 66/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0762 - accuracy: 0.5666 - val_loss: 1.0719 - val_accuracy: 0.5750\n",
      "Epoch 67/215\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0762 - accuracy: 0.5670 - val_loss: 1.1084 - val_accuracy: 0.5625\n",
      "Epoch 68/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0732 - accuracy: 0.5665 - val_loss: 1.0705 - val_accuracy: 0.5699\n",
      "Epoch 69/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0763 - accuracy: 0.5681 - val_loss: 1.0698 - val_accuracy: 0.5821\n",
      "Epoch 70/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0744 - accuracy: 0.5677 - val_loss: 1.0747 - val_accuracy: 0.5877\n",
      "Epoch 71/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0686 - accuracy: 0.5695 - val_loss: 1.0967 - val_accuracy: 0.5732\n",
      "Epoch 72/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0727 - accuracy: 0.5691 - val_loss: 1.0757 - val_accuracy: 0.5833\n",
      "Epoch 73/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0704 - accuracy: 0.5692 - val_loss: 1.0763 - val_accuracy: 0.5702\n",
      "Epoch 74/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0708 - accuracy: 0.5714 - val_loss: 1.0744 - val_accuracy: 0.5842\n",
      "Epoch 75/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0725 - accuracy: 0.5699 - val_loss: 1.0646 - val_accuracy: 0.5815\n",
      "Epoch 76/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0717 - accuracy: 0.5700 - val_loss: 1.0851 - val_accuracy: 0.5788\n",
      "Epoch 77/215\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0659 - accuracy: 0.5689 - val_loss: 1.0867 - val_accuracy: 0.5800\n",
      "Epoch 78/215\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0689 - accuracy: 0.5701 - val_loss: 1.0787 - val_accuracy: 0.5685\n",
      "Epoch 79/215\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0658 - accuracy: 0.5697 - val_loss: 1.0717 - val_accuracy: 0.5717\n",
      "Epoch 80/215\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0696 - accuracy: 0.5698 - val_loss: 1.0772 - val_accuracy: 0.5753\n",
      "Epoch 81/215\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0666 - accuracy: 0.5689 - val_loss: 1.0719 - val_accuracy: 0.5821\n",
      "Epoch 82/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0644 - accuracy: 0.5714 - val_loss: 1.1084 - val_accuracy: 0.5714\n",
      "Epoch 83/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0648 - accuracy: 0.5720 - val_loss: 1.0696 - val_accuracy: 0.5720\n",
      "Epoch 84/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0646 - accuracy: 0.5694 - val_loss: 1.0616 - val_accuracy: 0.5818\n",
      "Epoch 85/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0623 - accuracy: 0.5716 - val_loss: 1.0587 - val_accuracy: 0.5827\n",
      "Epoch 86/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0632 - accuracy: 0.5710 - val_loss: 1.0806 - val_accuracy: 0.5800\n",
      "Epoch 87/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0629 - accuracy: 0.5732 - val_loss: 1.0683 - val_accuracy: 0.5777\n",
      "Epoch 88/215\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0637 - accuracy: 0.5714 - val_loss: 1.0747 - val_accuracy: 0.5694\n",
      "Epoch 89/215\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0627 - accuracy: 0.5699 - val_loss: 1.0683 - val_accuracy: 0.5827\n",
      "Epoch 90/215\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0653 - accuracy: 0.5711 - val_loss: 1.0599 - val_accuracy: 0.5836\n",
      "Epoch 91/215\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0620 - accuracy: 0.5726 - val_loss: 1.0759 - val_accuracy: 0.5883\n",
      "Epoch 92/215\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0625 - accuracy: 0.5704 - val_loss: 1.0671 - val_accuracy: 0.5774\n",
      "Epoch 93/215\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0616 - accuracy: 0.5723 - val_loss: 1.0591 - val_accuracy: 0.5874\n",
      "Epoch 94/215\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0592 - accuracy: 0.5742 - val_loss: 1.0682 - val_accuracy: 0.5851\n",
      "Epoch 95/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0585 - accuracy: 0.5754 - val_loss: 1.0723 - val_accuracy: 0.5821\n",
      "Epoch 96/215\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0547 - accuracy: 0.5726 - val_loss: 1.0643 - val_accuracy: 0.5747\n",
      "Epoch 97/215\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0579 - accuracy: 0.5740 - val_loss: 1.0739 - val_accuracy: 0.5824\n",
      "Epoch 98/215\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0570 - accuracy: 0.5757 - val_loss: 1.0959 - val_accuracy: 0.5608\n",
      "Epoch 99/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0547 - accuracy: 0.5768 - val_loss: 1.0641 - val_accuracy: 0.5865\n",
      "Epoch 100/215\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0586 - accuracy: 0.5723 - val_loss: 1.0910 - val_accuracy: 0.5702\n",
      "Epoch 101/215\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0589 - accuracy: 0.5745 - val_loss: 1.0619 - val_accuracy: 0.5771\n",
      "Epoch 102/215\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0584 - accuracy: 0.5734 - val_loss: 1.0810 - val_accuracy: 0.5797\n",
      "Epoch 103/215\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0564 - accuracy: 0.5741 - val_loss: 1.0714 - val_accuracy: 0.5836\n",
      "Epoch 104/215\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0548 - accuracy: 0.5782 - val_loss: 1.0616 - val_accuracy: 0.5848\n",
      "Epoch 105/215\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0552 - accuracy: 0.5746 - val_loss: 1.0796 - val_accuracy: 0.5679\n",
      "Epoch 106/215\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 1.0580 - accuracy: 0.5743 - val_loss: 1.1234 - val_accuracy: 0.5557\n",
      "Epoch 107/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0538 - accuracy: 0.5786 - val_loss: 1.0722 - val_accuracy: 0.5762\n",
      "Epoch 108/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0570 - accuracy: 0.5782 - val_loss: 1.0940 - val_accuracy: 0.5744\n",
      "Epoch 109/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0516 - accuracy: 0.5747 - val_loss: 1.0651 - val_accuracy: 0.5756\n",
      "Epoch 110/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0551 - accuracy: 0.5760 - val_loss: 1.0649 - val_accuracy: 0.5898\n",
      "Epoch 111/215\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0532 - accuracy: 0.5744 - val_loss: 1.0578 - val_accuracy: 0.5898\n",
      "Epoch 112/215\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0522 - accuracy: 0.5751 - val_loss: 1.0718 - val_accuracy: 0.5694\n",
      "Epoch 113/215\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0519 - accuracy: 0.5745 - val_loss: 1.0571 - val_accuracy: 0.5806\n",
      "Epoch 114/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0498 - accuracy: 0.5771 - val_loss: 1.0677 - val_accuracy: 0.5788\n",
      "Epoch 115/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0525 - accuracy: 0.5763 - val_loss: 1.0737 - val_accuracy: 0.5768\n",
      "Epoch 116/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0532 - accuracy: 0.5769 - val_loss: 1.0651 - val_accuracy: 0.5860\n",
      "Epoch 117/215\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0526 - accuracy: 0.5754 - val_loss: 1.0855 - val_accuracy: 0.5821\n",
      "Epoch 118/215\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0523 - accuracy: 0.5775 - val_loss: 1.0628 - val_accuracy: 0.5747\n",
      "Epoch 119/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0531 - accuracy: 0.5745 - val_loss: 1.1131 - val_accuracy: 0.5614\n",
      "Epoch 120/215\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0506 - accuracy: 0.5777 - val_loss: 1.0663 - val_accuracy: 0.5732\n",
      "Epoch 121/215\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0474 - accuracy: 0.5782 - val_loss: 1.0500 - val_accuracy: 0.5901\n",
      "Epoch 122/215\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0490 - accuracy: 0.5773 - val_loss: 1.0535 - val_accuracy: 0.5913\n",
      "Epoch 123/215\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0487 - accuracy: 0.5762 - val_loss: 1.0745 - val_accuracy: 0.5815\n",
      "Epoch 124/215\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0491 - accuracy: 0.5790 - val_loss: 1.0541 - val_accuracy: 0.5839\n",
      "Epoch 125/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0431 - accuracy: 0.5809 - val_loss: 1.0494 - val_accuracy: 0.5854\n",
      "Epoch 126/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0507 - accuracy: 0.5783 - val_loss: 1.0574 - val_accuracy: 0.5765\n",
      "Epoch 127/215\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0495 - accuracy: 0.5777 - val_loss: 1.0688 - val_accuracy: 0.5741\n",
      "Epoch 128/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0461 - accuracy: 0.5822 - val_loss: 1.0885 - val_accuracy: 0.5765\n",
      "Epoch 129/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0481 - accuracy: 0.5789 - val_loss: 1.0610 - val_accuracy: 0.5777\n",
      "Epoch 130/215\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0470 - accuracy: 0.5785 - val_loss: 1.0535 - val_accuracy: 0.5889\n",
      "Epoch 131/215\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0492 - accuracy: 0.5807 - val_loss: 1.0551 - val_accuracy: 0.5877\n",
      "Epoch 132/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0449 - accuracy: 0.5788 - val_loss: 1.0574 - val_accuracy: 0.5913\n",
      "Epoch 133/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0506 - accuracy: 0.5783 - val_loss: 1.0597 - val_accuracy: 0.5877\n",
      "Epoch 134/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0440 - accuracy: 0.5795 - val_loss: 1.1142 - val_accuracy: 0.5560\n",
      "Epoch 135/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0461 - accuracy: 0.5776 - val_loss: 1.0553 - val_accuracy: 0.5865\n",
      "Epoch 136/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0448 - accuracy: 0.5789 - val_loss: 1.0529 - val_accuracy: 0.5785\n",
      "Epoch 137/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0426 - accuracy: 0.5809 - val_loss: 1.0785 - val_accuracy: 0.5720\n",
      "Epoch 138/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0427 - accuracy: 0.5794 - val_loss: 1.0567 - val_accuracy: 0.5913\n",
      "Epoch 139/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0469 - accuracy: 0.5794 - val_loss: 1.0651 - val_accuracy: 0.5836\n",
      "Epoch 140/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0409 - accuracy: 0.5786 - val_loss: 1.0439 - val_accuracy: 0.5895\n",
      "Epoch 141/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0492 - accuracy: 0.5794 - val_loss: 1.0699 - val_accuracy: 0.5788\n",
      "Epoch 142/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0422 - accuracy: 0.5802 - val_loss: 1.0770 - val_accuracy: 0.5851\n",
      "Epoch 143/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0423 - accuracy: 0.5784 - val_loss: 1.0623 - val_accuracy: 0.5768\n",
      "Epoch 144/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0412 - accuracy: 0.5810 - val_loss: 1.1179 - val_accuracy: 0.5637\n",
      "Epoch 145/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0454 - accuracy: 0.5800 - val_loss: 1.0571 - val_accuracy: 0.5851\n",
      "Epoch 146/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0412 - accuracy: 0.5825 - val_loss: 1.0590 - val_accuracy: 0.5762\n",
      "Epoch 147/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0409 - accuracy: 0.5809 - val_loss: 1.0533 - val_accuracy: 0.5916\n",
      "Epoch 148/215\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0435 - accuracy: 0.5789 - val_loss: 1.0444 - val_accuracy: 0.5865\n",
      "Epoch 149/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0411 - accuracy: 0.5832 - val_loss: 1.0767 - val_accuracy: 0.5682\n",
      "Epoch 150/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0393 - accuracy: 0.5822 - val_loss: 1.0462 - val_accuracy: 0.5865\n",
      "Epoch 151/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0429 - accuracy: 0.5806 - val_loss: 1.0547 - val_accuracy: 0.5791\n",
      "Epoch 152/215\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0434 - accuracy: 0.5803 - val_loss: 1.0470 - val_accuracy: 0.5824\n",
      "Epoch 153/215\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.0378 - accuracy: 0.5826 - val_loss: 1.0645 - val_accuracy: 0.5762\n",
      "Epoch 154/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0408 - accuracy: 0.5804 - val_loss: 1.0636 - val_accuracy: 0.5845\n",
      "Epoch 155/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0409 - accuracy: 0.5827 - val_loss: 1.0621 - val_accuracy: 0.5744\n",
      "Epoch 156/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0384 - accuracy: 0.5825 - val_loss: 1.0857 - val_accuracy: 0.5643\n",
      "Epoch 157/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0385 - accuracy: 0.5819 - val_loss: 1.0520 - val_accuracy: 0.5833\n",
      "Epoch 158/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0400 - accuracy: 0.5803 - val_loss: 1.0462 - val_accuracy: 0.5892\n",
      "Epoch 159/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0430 - accuracy: 0.5778 - val_loss: 1.0554 - val_accuracy: 0.5788\n",
      "Epoch 160/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0378 - accuracy: 0.5820 - val_loss: 1.0433 - val_accuracy: 0.5877\n",
      "Epoch 161/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0387 - accuracy: 0.5807 - val_loss: 1.0428 - val_accuracy: 0.5874\n",
      "Epoch 162/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0364 - accuracy: 0.5843 - val_loss: 1.0507 - val_accuracy: 0.5910\n",
      "Epoch 163/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0355 - accuracy: 0.5814 - val_loss: 1.0630 - val_accuracy: 0.5741\n",
      "Epoch 164/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0371 - accuracy: 0.5831 - val_loss: 1.0714 - val_accuracy: 0.5809\n",
      "Epoch 165/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0362 - accuracy: 0.5820 - val_loss: 1.0447 - val_accuracy: 0.5910\n",
      "Epoch 166/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0356 - accuracy: 0.5829 - val_loss: 1.0650 - val_accuracy: 0.5785\n",
      "Epoch 167/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0399 - accuracy: 0.5786 - val_loss: 1.0624 - val_accuracy: 0.5726\n",
      "Epoch 168/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0355 - accuracy: 0.5840 - val_loss: 1.0852 - val_accuracy: 0.5771\n",
      "Epoch 169/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0360 - accuracy: 0.5808 - val_loss: 1.0456 - val_accuracy: 0.5913\n",
      "Epoch 170/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0458 - accuracy: 0.5789 - val_loss: 1.0402 - val_accuracy: 0.5848\n",
      "Epoch 171/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0352 - accuracy: 0.5800 - val_loss: 1.0470 - val_accuracy: 0.5898\n",
      "Epoch 172/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0395 - accuracy: 0.5834 - val_loss: 1.0456 - val_accuracy: 0.5960\n",
      "Epoch 173/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0372 - accuracy: 0.5835 - val_loss: 1.0598 - val_accuracy: 0.5779\n",
      "Epoch 174/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0352 - accuracy: 0.5816 - val_loss: 1.0366 - val_accuracy: 0.5895\n",
      "Epoch 175/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0381 - accuracy: 0.5802 - val_loss: 1.0623 - val_accuracy: 0.5782\n",
      "Epoch 176/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0365 - accuracy: 0.5828 - val_loss: 1.0436 - val_accuracy: 0.5919\n",
      "Epoch 177/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0358 - accuracy: 0.5792 - val_loss: 1.0417 - val_accuracy: 0.5862\n",
      "Epoch 178/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0343 - accuracy: 0.5831 - val_loss: 1.0627 - val_accuracy: 0.5845\n",
      "Epoch 179/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0343 - accuracy: 0.5818 - val_loss: 1.0553 - val_accuracy: 0.5771\n",
      "Epoch 180/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0389 - accuracy: 0.5810 - val_loss: 1.0469 - val_accuracy: 0.5803\n",
      "Epoch 181/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0346 - accuracy: 0.5872 - val_loss: 1.0408 - val_accuracy: 0.5857\n",
      "Epoch 182/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0334 - accuracy: 0.5824 - val_loss: 1.0452 - val_accuracy: 0.5862\n",
      "Epoch 183/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0354 - accuracy: 0.5822 - val_loss: 1.0404 - val_accuracy: 0.5868\n",
      "Epoch 184/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0327 - accuracy: 0.5814 - val_loss: 1.0643 - val_accuracy: 0.5848\n",
      "Epoch 185/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0291 - accuracy: 0.5826 - val_loss: 1.0632 - val_accuracy: 0.5697\n",
      "Epoch 186/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0365 - accuracy: 0.5841 - val_loss: 1.0429 - val_accuracy: 0.5901\n",
      "Epoch 187/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0346 - accuracy: 0.5818 - val_loss: 1.0703 - val_accuracy: 0.5744\n",
      "Epoch 188/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0308 - accuracy: 0.5825 - val_loss: 1.0554 - val_accuracy: 0.5791\n",
      "Epoch 189/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0304 - accuracy: 0.5834 - val_loss: 1.0399 - val_accuracy: 0.5928\n",
      "Epoch 190/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0357 - accuracy: 0.5824 - val_loss: 1.0439 - val_accuracy: 0.5895\n",
      "Epoch 191/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0333 - accuracy: 0.5818 - val_loss: 1.0737 - val_accuracy: 0.5753\n",
      "Epoch 192/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0316 - accuracy: 0.5855 - val_loss: 1.0687 - val_accuracy: 0.5854\n",
      "Epoch 193/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0333 - accuracy: 0.5854 - val_loss: 1.0518 - val_accuracy: 0.5919\n",
      "Epoch 194/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0289 - accuracy: 0.5830 - val_loss: 1.0470 - val_accuracy: 0.5857\n",
      "Epoch 195/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0303 - accuracy: 0.5817 - val_loss: 1.0476 - val_accuracy: 0.5871\n",
      "Epoch 196/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0282 - accuracy: 0.5875 - val_loss: 1.0415 - val_accuracy: 0.5874\n",
      "Epoch 197/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0309 - accuracy: 0.5841 - val_loss: 1.0487 - val_accuracy: 0.5922\n",
      "Epoch 198/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0297 - accuracy: 0.5875 - val_loss: 1.0568 - val_accuracy: 0.5889\n",
      "Epoch 199/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0300 - accuracy: 0.5847 - val_loss: 1.0439 - val_accuracy: 0.5857\n",
      "Epoch 200/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0311 - accuracy: 0.5809 - val_loss: 1.0684 - val_accuracy: 0.5771\n",
      "Epoch 201/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0300 - accuracy: 0.5831 - val_loss: 1.0475 - val_accuracy: 0.5785\n",
      "Epoch 202/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0307 - accuracy: 0.5851 - val_loss: 1.0667 - val_accuracy: 0.5880\n",
      "Epoch 203/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0313 - accuracy: 0.5805 - val_loss: 1.0945 - val_accuracy: 0.5717\n",
      "Epoch 204/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0284 - accuracy: 0.5860 - val_loss: 1.0467 - val_accuracy: 0.5868\n",
      "Epoch 205/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0296 - accuracy: 0.5844 - val_loss: 1.0345 - val_accuracy: 0.5895\n",
      "Epoch 206/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0311 - accuracy: 0.5837 - val_loss: 1.0378 - val_accuracy: 0.5868\n",
      "Epoch 207/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0298 - accuracy: 0.5850 - val_loss: 1.0497 - val_accuracy: 0.5836\n",
      "Epoch 208/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0308 - accuracy: 0.5847 - val_loss: 1.0446 - val_accuracy: 0.5824\n",
      "Epoch 209/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0301 - accuracy: 0.5840 - val_loss: 1.0534 - val_accuracy: 0.5868\n",
      "Epoch 210/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0318 - accuracy: 0.5857 - val_loss: 1.0356 - val_accuracy: 0.5910\n",
      "Epoch 211/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0257 - accuracy: 0.5845 - val_loss: 1.0353 - val_accuracy: 0.5913\n",
      "Epoch 212/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0253 - accuracy: 0.5866 - val_loss: 1.0474 - val_accuracy: 0.5868\n",
      "Epoch 213/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0274 - accuracy: 0.5833 - val_loss: 1.0544 - val_accuracy: 0.5851\n",
      "Epoch 214/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0314 - accuracy: 0.5828 - val_loss: 1.0579 - val_accuracy: 0.5931\n",
      "Epoch 215/215\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 1.0302 - accuracy: 0.5838 - val_loss: 1.0359 - val_accuracy: 0.5860\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22a94158340>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_acc_epoch = hist1.history[\"val_accuracy\"]\n",
    "best_epoch = val_acc_epoch.index(max(val_acc_epoch)) + 1\n",
    "\n",
    "model_a = build_model_a()\n",
    "model_a.fit(X_train, y_train, epochs=best_epoch, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "323d6b39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist2.history[\"accuracy\"], label=\"accuracy\")\n",
    "plt.plot(hist2.history[\"val_accuracy\"], label=\"val_accuracy\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.legend([\"train\", \"test\"], loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "abef339e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/233\n",
      "422/422 [==============================] - 3s 5ms/step - loss: 1.4190 - accuracy: 0.4147 - val_loss: 1.4108 - val_accuracy: 0.4132\n",
      "Epoch 2/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.4057 - accuracy: 0.4137 - val_loss: 1.4083 - val_accuracy: 0.4132\n",
      "Epoch 3/233\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.3986 - accuracy: 0.4157 - val_loss: 1.4137 - val_accuracy: 0.4132\n",
      "Epoch 4/233\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.3964 - accuracy: 0.4183 - val_loss: 1.3964 - val_accuracy: 0.4241\n",
      "Epoch 5/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.3888 - accuracy: 0.4223 - val_loss: 1.3921 - val_accuracy: 0.4185\n",
      "Epoch 6/233\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.3813 - accuracy: 0.4303 - val_loss: 1.3834 - val_accuracy: 0.4203\n",
      "Epoch 7/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.3678 - accuracy: 0.4391 - val_loss: 1.3593 - val_accuracy: 0.4422\n",
      "Epoch 8/233\n",
      "422/422 [==============================] - 2s 6ms/step - loss: 1.3498 - accuracy: 0.4489 - val_loss: 1.3487 - val_accuracy: 0.4398\n",
      "Epoch 9/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.3345 - accuracy: 0.4569 - val_loss: 1.3510 - val_accuracy: 0.4369\n",
      "Epoch 10/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.3157 - accuracy: 0.4654 - val_loss: 1.3038 - val_accuracy: 0.4822\n",
      "Epoch 11/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.2997 - accuracy: 0.4689 - val_loss: 1.2883 - val_accuracy: 0.4819\n",
      "Epoch 12/233\n",
      "422/422 [==============================] - 2s 6ms/step - loss: 1.2844 - accuracy: 0.4742 - val_loss: 1.2786 - val_accuracy: 0.4754\n",
      "Epoch 13/233\n",
      "422/422 [==============================] - 2s 6ms/step - loss: 1.2762 - accuracy: 0.4722 - val_loss: 1.2687 - val_accuracy: 0.4772\n",
      "Epoch 14/233\n",
      "422/422 [==============================] - 3s 7ms/step - loss: 1.2654 - accuracy: 0.4749 - val_loss: 1.2580 - val_accuracy: 0.4849\n",
      "Epoch 15/233\n",
      "422/422 [==============================] - 3s 6ms/step - loss: 1.2554 - accuracy: 0.4749 - val_loss: 1.2578 - val_accuracy: 0.4745\n",
      "Epoch 16/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.2543 - accuracy: 0.4709 - val_loss: 1.2462 - val_accuracy: 0.4804\n",
      "Epoch 17/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.2463 - accuracy: 0.4728 - val_loss: 1.2390 - val_accuracy: 0.4825\n",
      "Epoch 18/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.2421 - accuracy: 0.4765 - val_loss: 1.2411 - val_accuracy: 0.4858\n",
      "Epoch 19/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.2346 - accuracy: 0.4785 - val_loss: 1.2280 - val_accuracy: 0.4887\n",
      "Epoch 20/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.2280 - accuracy: 0.4850 - val_loss: 1.2182 - val_accuracy: 0.4979\n",
      "Epoch 21/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.2220 - accuracy: 0.4881 - val_loss: 1.2218 - val_accuracy: 0.5027\n",
      "Epoch 22/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.2164 - accuracy: 0.4926 - val_loss: 1.2117 - val_accuracy: 0.5098\n",
      "Epoch 23/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.2123 - accuracy: 0.4961 - val_loss: 1.2053 - val_accuracy: 0.5116\n",
      "Epoch 24/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.2076 - accuracy: 0.4960 - val_loss: 1.2082 - val_accuracy: 0.4988\n",
      "Epoch 25/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.2022 - accuracy: 0.5010 - val_loss: 1.2028 - val_accuracy: 0.5255\n",
      "Epoch 26/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.2013 - accuracy: 0.5032 - val_loss: 1.2011 - val_accuracy: 0.5240\n",
      "Epoch 27/233\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1992 - accuracy: 0.5064 - val_loss: 1.2463 - val_accuracy: 0.4944\n",
      "Epoch 28/233\n",
      "422/422 [==============================] - 3s 6ms/step - loss: 1.1969 - accuracy: 0.5044 - val_loss: 1.1907 - val_accuracy: 0.5264\n",
      "Epoch 29/233\n",
      "422/422 [==============================] - 2s 6ms/step - loss: 1.1906 - accuracy: 0.5098 - val_loss: 1.1931 - val_accuracy: 0.5065\n",
      "Epoch 30/233\n",
      "422/422 [==============================] - 2s 6ms/step - loss: 1.1959 - accuracy: 0.5067 - val_loss: 1.2034 - val_accuracy: 0.5012\n",
      "Epoch 31/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.1914 - accuracy: 0.5102 - val_loss: 1.1891 - val_accuracy: 0.5353\n",
      "Epoch 32/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.1877 - accuracy: 0.5119 - val_loss: 1.1858 - val_accuracy: 0.5338\n",
      "Epoch 33/233\n",
      "422/422 [==============================] - 3s 6ms/step - loss: 1.1842 - accuracy: 0.5171 - val_loss: 1.2003 - val_accuracy: 0.5276\n",
      "Epoch 34/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.1846 - accuracy: 0.5178 - val_loss: 1.1799 - val_accuracy: 0.5237\n",
      "Epoch 35/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.1837 - accuracy: 0.5147 - val_loss: 1.1974 - val_accuracy: 0.5059\n",
      "Epoch 36/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.1824 - accuracy: 0.5136 - val_loss: 1.1855 - val_accuracy: 0.5341\n",
      "Epoch 37/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.1812 - accuracy: 0.5122 - val_loss: 1.1743 - val_accuracy: 0.5302\n",
      "Epoch 38/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.1829 - accuracy: 0.5136 - val_loss: 1.1945 - val_accuracy: 0.5249\n",
      "Epoch 39/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.1770 - accuracy: 0.5164 - val_loss: 1.1739 - val_accuracy: 0.5302\n",
      "Epoch 40/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.1771 - accuracy: 0.5199 - val_loss: 1.1685 - val_accuracy: 0.5344\n",
      "Epoch 41/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.1730 - accuracy: 0.5219 - val_loss: 1.1685 - val_accuracy: 0.5362\n",
      "Epoch 42/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.1745 - accuracy: 0.5199 - val_loss: 1.1713 - val_accuracy: 0.5397\n",
      "Epoch 43/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.1734 - accuracy: 0.5213 - val_loss: 1.1793 - val_accuracy: 0.5332\n",
      "Epoch 44/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.1727 - accuracy: 0.5213 - val_loss: 1.1672 - val_accuracy: 0.5427\n",
      "Epoch 45/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.1709 - accuracy: 0.5201 - val_loss: 1.1724 - val_accuracy: 0.5406\n",
      "Epoch 46/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.1663 - accuracy: 0.5256 - val_loss: 1.1790 - val_accuracy: 0.5317\n",
      "Epoch 47/233\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1662 - accuracy: 0.5256 - val_loss: 1.1651 - val_accuracy: 0.5356\n",
      "Epoch 48/233\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1675 - accuracy: 0.5227 - val_loss: 1.1700 - val_accuracy: 0.5468\n",
      "Epoch 49/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.1649 - accuracy: 0.5265 - val_loss: 1.1764 - val_accuracy: 0.5264\n",
      "Epoch 50/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.1708 - accuracy: 0.5218 - val_loss: 1.1827 - val_accuracy: 0.5296\n",
      "Epoch 51/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.1627 - accuracy: 0.5271 - val_loss: 1.1664 - val_accuracy: 0.5293\n",
      "Epoch 52/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.1629 - accuracy: 0.5264 - val_loss: 1.1632 - val_accuracy: 0.5320\n",
      "Epoch 53/233\n",
      "422/422 [==============================] - 2s 6ms/step - loss: 1.1611 - accuracy: 0.5282 - val_loss: 1.1741 - val_accuracy: 0.5231\n",
      "Epoch 54/233\n",
      "422/422 [==============================] - 2s 6ms/step - loss: 1.1656 - accuracy: 0.5222 - val_loss: 1.1727 - val_accuracy: 0.5400\n",
      "Epoch 55/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.1629 - accuracy: 0.5292 - val_loss: 1.1591 - val_accuracy: 0.5453\n",
      "Epoch 56/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.1628 - accuracy: 0.5233 - val_loss: 1.1568 - val_accuracy: 0.5382\n",
      "Epoch 57/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.1585 - accuracy: 0.5273 - val_loss: 1.1643 - val_accuracy: 0.5474\n",
      "Epoch 58/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.1571 - accuracy: 0.5282 - val_loss: 1.1646 - val_accuracy: 0.5418\n",
      "Epoch 59/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.1584 - accuracy: 0.5302 - val_loss: 1.1726 - val_accuracy: 0.5370\n",
      "Epoch 60/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.1587 - accuracy: 0.5305 - val_loss: 1.1657 - val_accuracy: 0.5409\n",
      "Epoch 61/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.1549 - accuracy: 0.5272 - val_loss: 1.1556 - val_accuracy: 0.5347\n",
      "Epoch 62/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.1590 - accuracy: 0.5269 - val_loss: 1.1576 - val_accuracy: 0.5329\n",
      "Epoch 63/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.1525 - accuracy: 0.5289 - val_loss: 1.1560 - val_accuracy: 0.5486\n",
      "Epoch 64/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.1511 - accuracy: 0.5358 - val_loss: 1.1496 - val_accuracy: 0.5590\n",
      "Epoch 65/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.1420 - accuracy: 0.5388 - val_loss: 1.1452 - val_accuracy: 0.5590\n",
      "Epoch 66/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.1434 - accuracy: 0.5396 - val_loss: 1.1569 - val_accuracy: 0.5486\n",
      "Epoch 67/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.1378 - accuracy: 0.5456 - val_loss: 1.1341 - val_accuracy: 0.5649\n",
      "Epoch 68/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.1345 - accuracy: 0.5470 - val_loss: 1.1708 - val_accuracy: 0.5320\n",
      "Epoch 69/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.1280 - accuracy: 0.5438 - val_loss: 1.1223 - val_accuracy: 0.5655\n",
      "Epoch 70/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.1195 - accuracy: 0.5488 - val_loss: 1.1319 - val_accuracy: 0.5637\n",
      "Epoch 71/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.1223 - accuracy: 0.5460 - val_loss: 1.1530 - val_accuracy: 0.5368\n",
      "Epoch 72/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.1123 - accuracy: 0.5525 - val_loss: 1.1331 - val_accuracy: 0.5507\n",
      "Epoch 73/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.1126 - accuracy: 0.5519 - val_loss: 1.1142 - val_accuracy: 0.5673\n",
      "Epoch 74/233\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1104 - accuracy: 0.5520 - val_loss: 1.1321 - val_accuracy: 0.5516\n",
      "Epoch 75/233\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1076 - accuracy: 0.5530 - val_loss: 1.1249 - val_accuracy: 0.5477\n",
      "Epoch 76/233\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1062 - accuracy: 0.5544 - val_loss: 1.1157 - val_accuracy: 0.5510\n",
      "Epoch 77/233\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1048 - accuracy: 0.5525 - val_loss: 1.1313 - val_accuracy: 0.5578\n",
      "Epoch 78/233\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1047 - accuracy: 0.5531 - val_loss: 1.1007 - val_accuracy: 0.5685\n",
      "Epoch 79/233\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1034 - accuracy: 0.5559 - val_loss: 1.1224 - val_accuracy: 0.5551\n",
      "Epoch 80/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.1056 - accuracy: 0.5530 - val_loss: 1.1185 - val_accuracy: 0.5652\n",
      "Epoch 81/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0966 - accuracy: 0.5553 - val_loss: 1.1133 - val_accuracy: 0.5705\n",
      "Epoch 82/233\n",
      "422/422 [==============================] - 2s 6ms/step - loss: 1.0975 - accuracy: 0.5599 - val_loss: 1.1042 - val_accuracy: 0.5756\n",
      "Epoch 83/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0935 - accuracy: 0.5587 - val_loss: 1.0927 - val_accuracy: 0.5765\n",
      "Epoch 84/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0929 - accuracy: 0.5599 - val_loss: 1.0978 - val_accuracy: 0.5771\n",
      "Epoch 85/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0955 - accuracy: 0.5593 - val_loss: 1.1175 - val_accuracy: 0.5664\n",
      "Epoch 86/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0922 - accuracy: 0.5604 - val_loss: 1.0938 - val_accuracy: 0.5806\n",
      "Epoch 87/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0916 - accuracy: 0.5605 - val_loss: 1.0972 - val_accuracy: 0.5714\n",
      "Epoch 88/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0932 - accuracy: 0.5601 - val_loss: 1.1103 - val_accuracy: 0.5708\n",
      "Epoch 89/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0885 - accuracy: 0.5625 - val_loss: 1.0939 - val_accuracy: 0.5782\n",
      "Epoch 90/233\n",
      "422/422 [==============================] - 2s 6ms/step - loss: 1.0888 - accuracy: 0.5640 - val_loss: 1.1119 - val_accuracy: 0.5634\n",
      "Epoch 91/233\n",
      "422/422 [==============================] - 3s 7ms/step - loss: 1.0869 - accuracy: 0.5632 - val_loss: 1.1151 - val_accuracy: 0.5699\n",
      "Epoch 92/233\n",
      "422/422 [==============================] - 3s 6ms/step - loss: 1.0898 - accuracy: 0.5606 - val_loss: 1.0923 - val_accuracy: 0.5720\n",
      "Epoch 93/233\n",
      "422/422 [==============================] - 3s 6ms/step - loss: 1.0873 - accuracy: 0.5634 - val_loss: 1.0858 - val_accuracy: 0.5741\n",
      "Epoch 94/233\n",
      "422/422 [==============================] - 3s 6ms/step - loss: 1.0853 - accuracy: 0.5637 - val_loss: 1.0990 - val_accuracy: 0.5697\n",
      "Epoch 95/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0888 - accuracy: 0.5579 - val_loss: 1.0838 - val_accuracy: 0.5771\n",
      "Epoch 96/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0893 - accuracy: 0.5594 - val_loss: 1.0869 - val_accuracy: 0.5744\n",
      "Epoch 97/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0873 - accuracy: 0.5611 - val_loss: 1.0965 - val_accuracy: 0.5768\n",
      "Epoch 98/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0868 - accuracy: 0.5611 - val_loss: 1.0826 - val_accuracy: 0.5815\n",
      "Epoch 99/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0875 - accuracy: 0.5622 - val_loss: 1.0898 - val_accuracy: 0.5756\n",
      "Epoch 100/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0838 - accuracy: 0.5604 - val_loss: 1.1188 - val_accuracy: 0.5539\n",
      "Epoch 101/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0854 - accuracy: 0.5646 - val_loss: 1.0966 - val_accuracy: 0.5726\n",
      "Epoch 102/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0822 - accuracy: 0.5656 - val_loss: 1.0878 - val_accuracy: 0.5765\n",
      "Epoch 103/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0814 - accuracy: 0.5624 - val_loss: 1.0892 - val_accuracy: 0.5797\n",
      "Epoch 104/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0820 - accuracy: 0.5677 - val_loss: 1.0902 - val_accuracy: 0.5782\n",
      "Epoch 105/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0847 - accuracy: 0.5626 - val_loss: 1.0959 - val_accuracy: 0.5658\n",
      "Epoch 106/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0825 - accuracy: 0.5658 - val_loss: 1.1097 - val_accuracy: 0.5634\n",
      "Epoch 107/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0806 - accuracy: 0.5645 - val_loss: 1.0831 - val_accuracy: 0.5833\n",
      "Epoch 108/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0830 - accuracy: 0.5642 - val_loss: 1.0943 - val_accuracy: 0.5806\n",
      "Epoch 109/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0853 - accuracy: 0.5611 - val_loss: 1.0825 - val_accuracy: 0.5800\n",
      "Epoch 110/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0810 - accuracy: 0.5655 - val_loss: 1.0826 - val_accuracy: 0.5741\n",
      "Epoch 111/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0840 - accuracy: 0.5622 - val_loss: 1.0849 - val_accuracy: 0.5806\n",
      "Epoch 112/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0776 - accuracy: 0.5674 - val_loss: 1.0930 - val_accuracy: 0.5803\n",
      "Epoch 113/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0808 - accuracy: 0.5637 - val_loss: 1.0883 - val_accuracy: 0.5777\n",
      "Epoch 114/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0771 - accuracy: 0.5655 - val_loss: 1.0816 - val_accuracy: 0.5806\n",
      "Epoch 115/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0789 - accuracy: 0.5660 - val_loss: 1.0859 - val_accuracy: 0.5661\n",
      "Epoch 116/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0815 - accuracy: 0.5622 - val_loss: 1.0743 - val_accuracy: 0.5762\n",
      "Epoch 117/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0755 - accuracy: 0.5671 - val_loss: 1.0891 - val_accuracy: 0.5753\n",
      "Epoch 118/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0771 - accuracy: 0.5672 - val_loss: 1.0847 - val_accuracy: 0.5797\n",
      "Epoch 119/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0757 - accuracy: 0.5632 - val_loss: 1.1144 - val_accuracy: 0.5596\n",
      "Epoch 120/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0769 - accuracy: 0.5647 - val_loss: 1.0865 - val_accuracy: 0.5774\n",
      "Epoch 121/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0752 - accuracy: 0.5685 - val_loss: 1.0951 - val_accuracy: 0.5658\n",
      "Epoch 122/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0738 - accuracy: 0.5652 - val_loss: 1.0798 - val_accuracy: 0.5785\n",
      "Epoch 123/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0744 - accuracy: 0.5648 - val_loss: 1.0797 - val_accuracy: 0.5797\n",
      "Epoch 124/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0775 - accuracy: 0.5648 - val_loss: 1.0719 - val_accuracy: 0.5797\n",
      "Epoch 125/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0745 - accuracy: 0.5633 - val_loss: 1.0939 - val_accuracy: 0.5667\n",
      "Epoch 126/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0770 - accuracy: 0.5665 - val_loss: 1.0783 - val_accuracy: 0.5821\n",
      "Epoch 127/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0736 - accuracy: 0.5668 - val_loss: 1.0721 - val_accuracy: 0.5756\n",
      "Epoch 128/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0734 - accuracy: 0.5657 - val_loss: 1.0755 - val_accuracy: 0.5800\n",
      "Epoch 129/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0731 - accuracy: 0.5668 - val_loss: 1.1011 - val_accuracy: 0.5628\n",
      "Epoch 130/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0722 - accuracy: 0.5676 - val_loss: 1.0946 - val_accuracy: 0.5685\n",
      "Epoch 131/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0722 - accuracy: 0.5677 - val_loss: 1.0686 - val_accuracy: 0.5779\n",
      "Epoch 132/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0730 - accuracy: 0.5670 - val_loss: 1.0720 - val_accuracy: 0.5827\n",
      "Epoch 133/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0716 - accuracy: 0.5683 - val_loss: 1.1404 - val_accuracy: 0.5477\n",
      "Epoch 134/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0734 - accuracy: 0.5645 - val_loss: 1.0687 - val_accuracy: 0.5809\n",
      "Epoch 135/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0696 - accuracy: 0.5665 - val_loss: 1.0727 - val_accuracy: 0.5759\n",
      "Epoch 136/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0677 - accuracy: 0.5675 - val_loss: 1.0877 - val_accuracy: 0.5723\n",
      "Epoch 137/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0658 - accuracy: 0.5677 - val_loss: 1.1937 - val_accuracy: 0.5098\n",
      "Epoch 138/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0696 - accuracy: 0.5668 - val_loss: 1.0671 - val_accuracy: 0.5782\n",
      "Epoch 139/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0675 - accuracy: 0.5694 - val_loss: 1.0668 - val_accuracy: 0.5833\n",
      "Epoch 140/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0737 - accuracy: 0.5658 - val_loss: 1.0753 - val_accuracy: 0.5848\n",
      "Epoch 141/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0686 - accuracy: 0.5667 - val_loss: 1.0767 - val_accuracy: 0.5815\n",
      "Epoch 142/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0669 - accuracy: 0.5716 - val_loss: 1.0826 - val_accuracy: 0.5717\n",
      "Epoch 143/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0700 - accuracy: 0.5710 - val_loss: 1.0742 - val_accuracy: 0.5824\n",
      "Epoch 144/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0662 - accuracy: 0.5686 - val_loss: 1.0767 - val_accuracy: 0.5851\n",
      "Epoch 145/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0626 - accuracy: 0.5699 - val_loss: 1.0693 - val_accuracy: 0.5777\n",
      "Epoch 146/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0646 - accuracy: 0.5711 - val_loss: 1.0651 - val_accuracy: 0.5800\n",
      "Epoch 147/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0648 - accuracy: 0.5697 - val_loss: 1.0643 - val_accuracy: 0.5842\n",
      "Epoch 148/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0626 - accuracy: 0.5703 - val_loss: 1.0831 - val_accuracy: 0.5765\n",
      "Epoch 149/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0630 - accuracy: 0.5706 - val_loss: 1.0686 - val_accuracy: 0.5803\n",
      "Epoch 150/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0662 - accuracy: 0.5691 - val_loss: 1.0612 - val_accuracy: 0.5806\n",
      "Epoch 151/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0646 - accuracy: 0.5709 - val_loss: 1.0734 - val_accuracy: 0.5806\n",
      "Epoch 152/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0611 - accuracy: 0.5721 - val_loss: 1.0745 - val_accuracy: 0.5759\n",
      "Epoch 153/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0645 - accuracy: 0.5720 - val_loss: 1.0594 - val_accuracy: 0.5812\n",
      "Epoch 154/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0632 - accuracy: 0.5700 - val_loss: 1.0656 - val_accuracy: 0.5836\n",
      "Epoch 155/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0652 - accuracy: 0.5696 - val_loss: 1.1325 - val_accuracy: 0.5516\n",
      "Epoch 156/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0638 - accuracy: 0.5701 - val_loss: 1.0651 - val_accuracy: 0.5774\n",
      "Epoch 157/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0586 - accuracy: 0.5697 - val_loss: 1.0661 - val_accuracy: 0.5797\n",
      "Epoch 158/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0648 - accuracy: 0.5685 - val_loss: 1.0616 - val_accuracy: 0.5779\n",
      "Epoch 159/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0645 - accuracy: 0.5705 - val_loss: 1.0643 - val_accuracy: 0.5827\n",
      "Epoch 160/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0598 - accuracy: 0.5711 - val_loss: 1.0603 - val_accuracy: 0.5865\n",
      "Epoch 161/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0590 - accuracy: 0.5725 - val_loss: 1.0607 - val_accuracy: 0.5904\n",
      "Epoch 162/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0599 - accuracy: 0.5728 - val_loss: 1.0567 - val_accuracy: 0.5848\n",
      "Epoch 163/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0599 - accuracy: 0.5714 - val_loss: 1.0617 - val_accuracy: 0.5845\n",
      "Epoch 164/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0551 - accuracy: 0.5744 - val_loss: 1.0667 - val_accuracy: 0.5788\n",
      "Epoch 165/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0608 - accuracy: 0.5715 - val_loss: 1.0843 - val_accuracy: 0.5664\n",
      "Epoch 166/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0578 - accuracy: 0.5754 - val_loss: 1.0631 - val_accuracy: 0.5854\n",
      "Epoch 167/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0576 - accuracy: 0.5720 - val_loss: 1.0836 - val_accuracy: 0.5774\n",
      "Epoch 168/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0543 - accuracy: 0.5767 - val_loss: 1.0575 - val_accuracy: 0.5827\n",
      "Epoch 169/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0580 - accuracy: 0.5725 - val_loss: 1.0547 - val_accuracy: 0.5877\n",
      "Epoch 170/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0544 - accuracy: 0.5723 - val_loss: 1.0768 - val_accuracy: 0.5711\n",
      "Epoch 171/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0627 - accuracy: 0.5708 - val_loss: 1.0662 - val_accuracy: 0.5809\n",
      "Epoch 172/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0549 - accuracy: 0.5760 - val_loss: 1.0625 - val_accuracy: 0.5797\n",
      "Epoch 173/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0555 - accuracy: 0.5705 - val_loss: 1.0648 - val_accuracy: 0.5842\n",
      "Epoch 174/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0545 - accuracy: 0.5728 - val_loss: 1.0671 - val_accuracy: 0.5794\n",
      "Epoch 175/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0544 - accuracy: 0.5727 - val_loss: 1.0501 - val_accuracy: 0.5868\n",
      "Epoch 176/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0587 - accuracy: 0.5723 - val_loss: 1.0530 - val_accuracy: 0.5886\n",
      "Epoch 177/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0514 - accuracy: 0.5739 - val_loss: 1.0590 - val_accuracy: 0.5779\n",
      "Epoch 178/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0535 - accuracy: 0.5743 - val_loss: 1.0541 - val_accuracy: 0.5871\n",
      "Epoch 179/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0527 - accuracy: 0.5751 - val_loss: 1.0561 - val_accuracy: 0.5765\n",
      "Epoch 180/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0544 - accuracy: 0.5726 - val_loss: 1.0565 - val_accuracy: 0.5889\n",
      "Epoch 181/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0510 - accuracy: 0.5760 - val_loss: 1.0607 - val_accuracy: 0.5862\n",
      "Epoch 182/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0525 - accuracy: 0.5777 - val_loss: 1.0508 - val_accuracy: 0.5836\n",
      "Epoch 183/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0492 - accuracy: 0.5720 - val_loss: 1.0758 - val_accuracy: 0.5806\n",
      "Epoch 184/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0521 - accuracy: 0.5734 - val_loss: 1.0693 - val_accuracy: 0.5886\n",
      "Epoch 185/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0488 - accuracy: 0.5780 - val_loss: 1.0547 - val_accuracy: 0.5836\n",
      "Epoch 186/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0475 - accuracy: 0.5783 - val_loss: 1.0480 - val_accuracy: 0.5848\n",
      "Epoch 187/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0551 - accuracy: 0.5743 - val_loss: 1.0525 - val_accuracy: 0.5860\n",
      "Epoch 188/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0503 - accuracy: 0.5771 - val_loss: 1.0602 - val_accuracy: 0.5860\n",
      "Epoch 189/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0482 - accuracy: 0.5740 - val_loss: 1.0579 - val_accuracy: 0.5919\n",
      "Epoch 190/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0516 - accuracy: 0.5728 - val_loss: 1.0617 - val_accuracy: 0.5827\n",
      "Epoch 191/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0487 - accuracy: 0.5737 - val_loss: 1.0548 - val_accuracy: 0.5868\n",
      "Epoch 192/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0481 - accuracy: 0.5725 - val_loss: 1.0623 - val_accuracy: 0.5842\n",
      "Epoch 193/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0472 - accuracy: 0.5752 - val_loss: 1.1132 - val_accuracy: 0.5667\n",
      "Epoch 194/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0518 - accuracy: 0.5767 - val_loss: 1.0711 - val_accuracy: 0.5812\n",
      "Epoch 195/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0481 - accuracy: 0.5792 - val_loss: 1.0591 - val_accuracy: 0.5836\n",
      "Epoch 196/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0473 - accuracy: 0.5768 - val_loss: 1.0499 - val_accuracy: 0.5928\n",
      "Epoch 197/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0470 - accuracy: 0.5777 - val_loss: 1.0628 - val_accuracy: 0.5785\n",
      "Epoch 198/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0525 - accuracy: 0.5746 - val_loss: 1.0546 - val_accuracy: 0.5892\n",
      "Epoch 199/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0506 - accuracy: 0.5740 - val_loss: 1.0500 - val_accuracy: 0.5803\n",
      "Epoch 200/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0496 - accuracy: 0.5741 - val_loss: 1.0577 - val_accuracy: 0.5904\n",
      "Epoch 201/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0489 - accuracy: 0.5740 - val_loss: 1.0516 - val_accuracy: 0.5833\n",
      "Epoch 202/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0474 - accuracy: 0.5767 - val_loss: 1.0447 - val_accuracy: 0.5916\n",
      "Epoch 203/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0465 - accuracy: 0.5767 - val_loss: 1.0552 - val_accuracy: 0.5877\n",
      "Epoch 204/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0480 - accuracy: 0.5746 - val_loss: 1.0539 - val_accuracy: 0.5862\n",
      "Epoch 205/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0462 - accuracy: 0.5736 - val_loss: 1.0484 - val_accuracy: 0.5925\n",
      "Epoch 206/233\n",
      "422/422 [==============================] - 2s 6ms/step - loss: 1.0440 - accuracy: 0.5757 - val_loss: 1.0433 - val_accuracy: 0.5945\n",
      "Epoch 207/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0443 - accuracy: 0.5794 - val_loss: 1.0468 - val_accuracy: 0.5948\n",
      "Epoch 208/233\n",
      "422/422 [==============================] - 2s 6ms/step - loss: 1.0424 - accuracy: 0.5795 - val_loss: 1.0719 - val_accuracy: 0.5883\n",
      "Epoch 209/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0412 - accuracy: 0.5792 - val_loss: 1.0535 - val_accuracy: 0.5833\n",
      "Epoch 210/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0427 - accuracy: 0.5792 - val_loss: 1.1196 - val_accuracy: 0.5504\n",
      "Epoch 211/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0436 - accuracy: 0.5748 - val_loss: 1.0415 - val_accuracy: 0.5919\n",
      "Epoch 212/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0451 - accuracy: 0.5797 - val_loss: 1.0609 - val_accuracy: 0.5860\n",
      "Epoch 213/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0448 - accuracy: 0.5760 - val_loss: 1.0603 - val_accuracy: 0.5812\n",
      "Epoch 214/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0400 - accuracy: 0.5803 - val_loss: 1.0599 - val_accuracy: 0.5851\n",
      "Epoch 215/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0504 - accuracy: 0.5781 - val_loss: 1.0433 - val_accuracy: 0.5892\n",
      "Epoch 216/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0420 - accuracy: 0.5775 - val_loss: 1.0558 - val_accuracy: 0.5827\n",
      "Epoch 217/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0421 - accuracy: 0.5803 - val_loss: 1.0501 - val_accuracy: 0.5883\n",
      "Epoch 218/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0400 - accuracy: 0.5787 - val_loss: 1.0600 - val_accuracy: 0.5848\n",
      "Epoch 219/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0423 - accuracy: 0.5781 - val_loss: 1.0478 - val_accuracy: 0.5913\n",
      "Epoch 220/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0429 - accuracy: 0.5779 - val_loss: 1.0417 - val_accuracy: 0.5907\n",
      "Epoch 221/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0438 - accuracy: 0.5780 - val_loss: 1.0475 - val_accuracy: 0.5945\n",
      "Epoch 222/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0394 - accuracy: 0.5764 - val_loss: 1.0469 - val_accuracy: 0.5791\n",
      "Epoch 223/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0428 - accuracy: 0.5804 - val_loss: 1.0832 - val_accuracy: 0.5762\n",
      "Epoch 224/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0427 - accuracy: 0.5794 - val_loss: 1.0465 - val_accuracy: 0.5937\n",
      "Epoch 225/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0415 - accuracy: 0.5773 - val_loss: 1.0628 - val_accuracy: 0.5871\n",
      "Epoch 226/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0407 - accuracy: 0.5767 - val_loss: 1.0409 - val_accuracy: 0.5916\n",
      "Epoch 227/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0380 - accuracy: 0.5791 - val_loss: 1.0456 - val_accuracy: 0.5865\n",
      "Epoch 228/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0425 - accuracy: 0.5795 - val_loss: 1.0440 - val_accuracy: 0.5883\n",
      "Epoch 229/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0415 - accuracy: 0.5809 - val_loss: 1.0390 - val_accuracy: 0.5910\n",
      "Epoch 230/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0387 - accuracy: 0.5806 - val_loss: 1.0414 - val_accuracy: 0.5907\n",
      "Epoch 231/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0445 - accuracy: 0.5790 - val_loss: 1.0440 - val_accuracy: 0.5865\n",
      "Epoch 232/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0371 - accuracy: 0.5803 - val_loss: 1.0679 - val_accuracy: 0.5803\n",
      "Epoch 233/233\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 1.0363 - accuracy: 0.5774 - val_loss: 1.0557 - val_accuracy: 0.5833\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22a84205dc0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_acc_epoch = hist2.history[\"val_accuracy\"]\n",
    "best_epoch = val_acc_epoch.index(max(val_acc_epoch)) + 1\n",
    "\n",
    "model_b = build_model_b()\n",
    "model_b.fit(X_train, y_train, epochs=best_epoch, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "219ea844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260/260 [==============================] - 1s 3ms/step - loss: 1.0472 - accuracy: 0.5744\n",
      "[1.0472266674041748, 0.5744373798370361]\n",
      "260/260 [==============================] - 1s 2ms/step - loss: 1.0313 - accuracy: 0.5765\n",
      "[1.0312726497650146, 0.5764833092689514]\n",
      "260/260 [==============================] - 1s 2ms/step - loss: 1.0570 - accuracy: 0.5696\n",
      "[1.0569791793823242, 0.5696232914924622]\n"
     ]
    }
   ],
   "source": [
    "print(model.evaluate(X_test, y_test))\n",
    "print(model_a.evaluate(X_test, y_test))\n",
    "print(model_b.evaluate(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e98cb18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260/260 [==============================] - 0s 1ms/step\n",
      "(array([0, 1, 2, 3, 4], dtype=int64), array([4389, 2241,    9, 1665,    5], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(np.argmax(model_b.predict(X_test), axis=1), return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "518081fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.564087134432543"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLPClassifier(max_iter=10000, learning_rate=\"adaptive\")\n",
    "\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "mlp.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0ccb7f6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_examples, train_score, valid_score = learning_curve(mlp, X_train, y_train, cv=StratifiedKFold(3))\n",
    "plt.plot(training_examples, np.mean(train_score, axis=1), label=\"train_score\")\n",
    "plt.plot(training_examples, np.mean(valid_score, axis=1), label=\"valid_score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "aed4108c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 6, 7], dtype=int64),\n",
       " array([2845, 1476,   13,   18, 1120], dtype=int64))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(mlp.predict(X_test), return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c12eaa",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "148fc156",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-100-233bc4989a44>:1: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  a = KerasClassifier(build_model_a, epochs=400, batch_size=10)\n",
      "<ipython-input-100-233bc4989a44>:3: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  b = KerasClassifier(build_model_b, epochs=400, batch_size=10)\n",
      "<ipython-input-100-233bc4989a44>:5: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  c = KerasClassifier(build_model_c, epochs=400, batch_size=10)\n"
     ]
    }
   ],
   "source": [
    "a = KerasClassifier(build_model_a, epochs=400, batch_size=10)\n",
    "a._estimator_type = \"classifier\"\n",
    "b = KerasClassifier(build_model_b, epochs=400, batch_size=10)\n",
    "b._estimator_type = \"classifier\"\n",
    "c = KerasClassifier(build_model_c, epochs=400, batch_size=10)\n",
    "c._estimator_type = \"classifier\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "cd0e348c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ens = VotingClassifier([(\"a\", a), (\"b\", b), (\"c\", c)], voting=\"hard\", weights=[0.37, 0.33, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "864149bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.5836 - accuracy: 0.3984\n",
      "Epoch 2/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.5716 - accuracy: 0.3989\n",
      "Epoch 3/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.5707 - accuracy: 0.3989\n",
      "Epoch 4/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.5706 - accuracy: 0.3989\n",
      "Epoch 5/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.5703 - accuracy: 0.3989\n",
      "Epoch 6/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.5701 - accuracy: 0.3989\n",
      "Epoch 7/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.5699 - accuracy: 0.3989\n",
      "Epoch 8/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.5685 - accuracy: 0.3989\n",
      "Epoch 9/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.5652 - accuracy: 0.3994\n",
      "Epoch 10/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.5642 - accuracy: 0.3994\n",
      "Epoch 11/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.5573 - accuracy: 0.4002\n",
      "Epoch 12/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.5528 - accuracy: 0.4012\n",
      "Epoch 13/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.5479 - accuracy: 0.4039\n",
      "Epoch 14/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.5444 - accuracy: 0.4062\n",
      "Epoch 15/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.5403 - accuracy: 0.4059\n",
      "Epoch 16/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.5366 - accuracy: 0.4135\n",
      "Epoch 17/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.5272 - accuracy: 0.4168\n",
      "Epoch 18/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.5160 - accuracy: 0.4224\n",
      "Epoch 19/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.5049 - accuracy: 0.4310\n",
      "Epoch 20/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.4943 - accuracy: 0.4372\n",
      "Epoch 21/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.4829 - accuracy: 0.4424\n",
      "Epoch 22/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.4688 - accuracy: 0.4476\n",
      "Epoch 23/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.4560 - accuracy: 0.4503\n",
      "Epoch 24/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.4402 - accuracy: 0.4582\n",
      "Epoch 25/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.4276 - accuracy: 0.4598\n",
      "Epoch 26/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.4209 - accuracy: 0.4635\n",
      "Epoch 27/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.4087 - accuracy: 0.4649\n",
      "Epoch 28/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.4046 - accuracy: 0.4665\n",
      "Epoch 29/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.3937 - accuracy: 0.4690\n",
      "Epoch 30/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.3905 - accuracy: 0.4680\n",
      "Epoch 31/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.3837 - accuracy: 0.4716\n",
      "Epoch 32/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.3693 - accuracy: 0.4765\n",
      "Epoch 33/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.3661 - accuracy: 0.4791\n",
      "Epoch 34/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.3559 - accuracy: 0.4860\n",
      "Epoch 35/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.3498 - accuracy: 0.4909\n",
      "Epoch 36/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.3473 - accuracy: 0.4855\n",
      "Epoch 37/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.3373 - accuracy: 0.4980\n",
      "Epoch 38/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.3341 - accuracy: 0.5057\n",
      "Epoch 39/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.3300 - accuracy: 0.4989\n",
      "Epoch 40/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.3225 - accuracy: 0.5037\n",
      "Epoch 41/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.3196 - accuracy: 0.5140\n",
      "Epoch 42/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.3193 - accuracy: 0.5133\n",
      "Epoch 43/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.3157 - accuracy: 0.5110\n",
      "Epoch 44/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.3135 - accuracy: 0.5144\n",
      "Epoch 45/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.3062 - accuracy: 0.5169\n",
      "Epoch 46/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.3038 - accuracy: 0.5212\n",
      "Epoch 47/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2986 - accuracy: 0.5274\n",
      "Epoch 48/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2974 - accuracy: 0.5241\n",
      "Epoch 49/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2993 - accuracy: 0.5273\n",
      "Epoch 50/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2941 - accuracy: 0.5297\n",
      "Epoch 51/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2920 - accuracy: 0.5288\n",
      "Epoch 52/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2952 - accuracy: 0.5298\n",
      "Epoch 53/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2901 - accuracy: 0.5266\n",
      "Epoch 54/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2872 - accuracy: 0.5309\n",
      "Epoch 55/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2907 - accuracy: 0.5345\n",
      "Epoch 56/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2834 - accuracy: 0.5336\n",
      "Epoch 57/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2894 - accuracy: 0.5321\n",
      "Epoch 58/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2785 - accuracy: 0.5391\n",
      "Epoch 59/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2773 - accuracy: 0.5430\n",
      "Epoch 60/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2772 - accuracy: 0.5402\n",
      "Epoch 61/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2780 - accuracy: 0.5393\n",
      "Epoch 62/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2783 - accuracy: 0.5395\n",
      "Epoch 63/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2740 - accuracy: 0.5398\n",
      "Epoch 64/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2663 - accuracy: 0.5494\n",
      "Epoch 65/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2725 - accuracy: 0.5437\n",
      "Epoch 66/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2719 - accuracy: 0.5460\n",
      "Epoch 67/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2722 - accuracy: 0.5506\n",
      "Epoch 68/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2677 - accuracy: 0.5451\n",
      "Epoch 69/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2702 - accuracy: 0.5474\n",
      "Epoch 70/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2636 - accuracy: 0.5492\n",
      "Epoch 71/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2810 - accuracy: 0.5426\n",
      "Epoch 72/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.2651 - accuracy: 0.5499\n",
      "Epoch 73/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2683 - accuracy: 0.5469\n",
      "Epoch 74/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.2625 - accuracy: 0.5504\n",
      "Epoch 75/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2589 - accuracy: 0.5555\n",
      "Epoch 76/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2610 - accuracy: 0.5559\n",
      "Epoch 77/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2618 - accuracy: 0.5504\n",
      "Epoch 78/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2592 - accuracy: 0.5484\n",
      "Epoch 79/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2574 - accuracy: 0.5561\n",
      "Epoch 80/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2526 - accuracy: 0.5585\n",
      "Epoch 81/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2601 - accuracy: 0.5491\n",
      "Epoch 82/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2596 - accuracy: 0.5518\n",
      "Epoch 83/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2546 - accuracy: 0.5567\n",
      "Epoch 84/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2574 - accuracy: 0.5541\n",
      "Epoch 85/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2569 - accuracy: 0.5535\n",
      "Epoch 86/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2520 - accuracy: 0.5563\n",
      "Epoch 87/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2520 - accuracy: 0.5565\n",
      "Epoch 88/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2460 - accuracy: 0.5618\n",
      "Epoch 89/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2487 - accuracy: 0.5580\n",
      "Epoch 90/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2486 - accuracy: 0.5586\n",
      "Epoch 91/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2541 - accuracy: 0.5546\n",
      "Epoch 92/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2436 - accuracy: 0.5592\n",
      "Epoch 93/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2431 - accuracy: 0.5597\n",
      "Epoch 94/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.2428 - accuracy: 0.5580\n",
      "Epoch 95/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2537 - accuracy: 0.5544\n",
      "Epoch 96/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2505 - accuracy: 0.5580\n",
      "Epoch 97/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2432 - accuracy: 0.5594\n",
      "Epoch 98/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2424 - accuracy: 0.5557\n",
      "Epoch 99/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2413 - accuracy: 0.5657\n",
      "Epoch 100/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2474 - accuracy: 0.5592\n",
      "Epoch 101/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2441 - accuracy: 0.5584\n",
      "Epoch 102/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2383 - accuracy: 0.5609\n",
      "Epoch 103/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2399 - accuracy: 0.5645\n",
      "Epoch 104/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2411 - accuracy: 0.5578\n",
      "Epoch 105/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2413 - accuracy: 0.5582\n",
      "Epoch 106/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2427 - accuracy: 0.5604\n",
      "Epoch 107/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2379 - accuracy: 0.5647\n",
      "Epoch 108/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2436 - accuracy: 0.5581\n",
      "Epoch 109/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2337 - accuracy: 0.5598\n",
      "Epoch 110/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2302 - accuracy: 0.5609\n",
      "Epoch 111/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2351 - accuracy: 0.5615\n",
      "Epoch 112/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2332 - accuracy: 0.5614\n",
      "Epoch 113/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2323 - accuracy: 0.5601\n",
      "Epoch 114/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2328 - accuracy: 0.5627\n",
      "Epoch 115/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2342 - accuracy: 0.5606\n",
      "Epoch 116/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2284 - accuracy: 0.5634\n",
      "Epoch 117/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2295 - accuracy: 0.5663\n",
      "Epoch 118/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2254 - accuracy: 0.5635\n",
      "Epoch 119/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2326 - accuracy: 0.5628\n",
      "Epoch 120/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2196 - accuracy: 0.5682\n",
      "Epoch 121/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2247 - accuracy: 0.5671\n",
      "Epoch 122/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2234 - accuracy: 0.5678\n",
      "Epoch 123/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2267 - accuracy: 0.5573\n",
      "Epoch 124/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2253 - accuracy: 0.5639\n",
      "Epoch 125/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2289 - accuracy: 0.5617\n",
      "Epoch 126/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2184 - accuracy: 0.5654\n",
      "Epoch 127/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2225 - accuracy: 0.5591\n",
      "Epoch 128/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2183 - accuracy: 0.5680\n",
      "Epoch 129/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2141 - accuracy: 0.5683\n",
      "Epoch 130/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2202 - accuracy: 0.5652\n",
      "Epoch 131/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2214 - accuracy: 0.5612\n",
      "Epoch 132/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2161 - accuracy: 0.5653\n",
      "Epoch 133/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2125 - accuracy: 0.5670\n",
      "Epoch 134/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2148 - accuracy: 0.5692\n",
      "Epoch 135/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2143 - accuracy: 0.5668\n",
      "Epoch 136/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2094 - accuracy: 0.5664\n",
      "Epoch 137/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2081 - accuracy: 0.5668\n",
      "Epoch 138/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2123 - accuracy: 0.5649\n",
      "Epoch 139/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2038 - accuracy: 0.5699\n",
      "Epoch 140/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2079 - accuracy: 0.5679\n",
      "Epoch 141/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2063 - accuracy: 0.5666\n",
      "Epoch 142/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2080 - accuracy: 0.5694\n",
      "Epoch 143/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2098 - accuracy: 0.5678\n",
      "Epoch 144/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2043 - accuracy: 0.5675\n",
      "Epoch 145/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2061 - accuracy: 0.5643\n",
      "Epoch 146/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2116 - accuracy: 0.5682\n",
      "Epoch 147/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2075 - accuracy: 0.5684\n",
      "Epoch 148/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2003 - accuracy: 0.5723\n",
      "Epoch 149/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2018 - accuracy: 0.5699\n",
      "Epoch 150/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1978 - accuracy: 0.5700\n",
      "Epoch 151/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2029 - accuracy: 0.5700\n",
      "Epoch 152/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2044 - accuracy: 0.5668\n",
      "Epoch 153/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2003 - accuracy: 0.5735\n",
      "Epoch 154/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2014 - accuracy: 0.5708\n",
      "Epoch 155/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1999 - accuracy: 0.5701\n",
      "Epoch 156/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2053 - accuracy: 0.5703\n",
      "Epoch 157/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1992 - accuracy: 0.5724\n",
      "Epoch 158/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2059 - accuracy: 0.5732\n",
      "Epoch 159/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1946 - accuracy: 0.5730\n",
      "Epoch 160/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1982 - accuracy: 0.5724\n",
      "Epoch 161/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1938 - accuracy: 0.5766\n",
      "Epoch 162/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1944 - accuracy: 0.5783\n",
      "Epoch 163/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1936 - accuracy: 0.5745\n",
      "Epoch 164/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1929 - accuracy: 0.5707\n",
      "Epoch 165/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1986 - accuracy: 0.5715\n",
      "Epoch 166/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1932 - accuracy: 0.5729\n",
      "Epoch 167/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1935 - accuracy: 0.5729\n",
      "Epoch 168/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1872 - accuracy: 0.5764\n",
      "Epoch 169/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1891 - accuracy: 0.5776\n",
      "Epoch 170/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1905 - accuracy: 0.5758\n",
      "Epoch 171/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1947 - accuracy: 0.5729\n",
      "Epoch 172/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1881 - accuracy: 0.5730\n",
      "Epoch 173/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1885 - accuracy: 0.5758\n",
      "Epoch 174/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1874 - accuracy: 0.5747\n",
      "Epoch 175/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1926 - accuracy: 0.5705\n",
      "Epoch 176/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1824 - accuracy: 0.5776\n",
      "Epoch 177/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1864 - accuracy: 0.5751\n",
      "Epoch 178/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1840 - accuracy: 0.5736\n",
      "Epoch 179/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1852 - accuracy: 0.5764\n",
      "Epoch 180/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1839 - accuracy: 0.5778\n",
      "Epoch 181/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1854 - accuracy: 0.5749\n",
      "Epoch 182/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1796 - accuracy: 0.5773\n",
      "Epoch 183/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1878 - accuracy: 0.5756\n",
      "Epoch 184/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1810 - accuracy: 0.5786\n",
      "Epoch 185/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1788 - accuracy: 0.5755\n",
      "Epoch 186/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1800 - accuracy: 0.5790\n",
      "Epoch 187/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1812 - accuracy: 0.5796\n",
      "Epoch 188/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1783 - accuracy: 0.5808\n",
      "Epoch 189/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1823 - accuracy: 0.5841\n",
      "Epoch 190/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1830 - accuracy: 0.5782\n",
      "Epoch 191/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1767 - accuracy: 0.5802\n",
      "Epoch 192/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1779 - accuracy: 0.5813\n",
      "Epoch 193/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1759 - accuracy: 0.5810\n",
      "Epoch 194/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1743 - accuracy: 0.5826\n",
      "Epoch 195/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1805 - accuracy: 0.5829\n",
      "Epoch 196/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1804 - accuracy: 0.5819\n",
      "Epoch 197/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1742 - accuracy: 0.5835\n",
      "Epoch 198/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1739 - accuracy: 0.5796\n",
      "Epoch 199/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1768 - accuracy: 0.5812\n",
      "Epoch 200/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1726 - accuracy: 0.5845\n",
      "Epoch 201/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1771 - accuracy: 0.5835\n",
      "Epoch 202/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1705 - accuracy: 0.5871\n",
      "Epoch 203/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1715 - accuracy: 0.5849\n",
      "Epoch 204/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1776 - accuracy: 0.5796\n",
      "Epoch 205/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1676 - accuracy: 0.5886\n",
      "Epoch 206/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1712 - accuracy: 0.5845\n",
      "Epoch 207/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1724 - accuracy: 0.5869\n",
      "Epoch 208/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1680 - accuracy: 0.5861\n",
      "Epoch 209/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1742 - accuracy: 0.5841\n",
      "Epoch 210/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1721 - accuracy: 0.5853\n",
      "Epoch 211/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1773 - accuracy: 0.5870\n",
      "Epoch 212/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1640 - accuracy: 0.5860\n",
      "Epoch 213/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1767 - accuracy: 0.5838\n",
      "Epoch 214/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1705 - accuracy: 0.5868\n",
      "Epoch 215/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1667 - accuracy: 0.5882\n",
      "Epoch 216/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1728 - accuracy: 0.5867\n",
      "Epoch 217/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1673 - accuracy: 0.5877\n",
      "Epoch 218/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1762 - accuracy: 0.5884\n",
      "Epoch 219/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1709 - accuracy: 0.5857\n",
      "Epoch 220/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1657 - accuracy: 0.5862\n",
      "Epoch 221/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1708 - accuracy: 0.5880\n",
      "Epoch 222/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1706 - accuracy: 0.5863\n",
      "Epoch 223/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1666 - accuracy: 0.5858\n",
      "Epoch 224/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1690 - accuracy: 0.5866\n",
      "Epoch 225/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1640 - accuracy: 0.5881\n",
      "Epoch 226/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1684 - accuracy: 0.5884\n",
      "Epoch 227/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1690 - accuracy: 0.5889\n",
      "Epoch 228/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1649 - accuracy: 0.5826\n",
      "Epoch 229/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1623 - accuracy: 0.5885\n",
      "Epoch 230/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1706 - accuracy: 0.5837\n",
      "Epoch 231/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1664 - accuracy: 0.5858\n",
      "Epoch 232/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1638 - accuracy: 0.5908\n",
      "Epoch 233/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1654 - accuracy: 0.5881\n",
      "Epoch 234/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1629 - accuracy: 0.5913\n",
      "Epoch 235/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1581 - accuracy: 0.5896\n",
      "Epoch 236/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1624 - accuracy: 0.5867\n",
      "Epoch 237/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1654 - accuracy: 0.5911\n",
      "Epoch 238/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1668 - accuracy: 0.5887\n",
      "Epoch 239/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1634 - accuracy: 0.5921\n",
      "Epoch 240/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1619 - accuracy: 0.5911\n",
      "Epoch 241/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1688 - accuracy: 0.5853\n",
      "Epoch 242/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1607 - accuracy: 0.5912\n",
      "Epoch 243/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1653 - accuracy: 0.5917\n",
      "Epoch 244/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1654 - accuracy: 0.5863\n",
      "Epoch 245/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1632 - accuracy: 0.5883\n",
      "Epoch 246/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1629 - accuracy: 0.5905\n",
      "Epoch 247/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1575 - accuracy: 0.5916\n",
      "Epoch 248/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1621 - accuracy: 0.5923\n",
      "Epoch 249/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1594 - accuracy: 0.5924\n",
      "Epoch 250/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1606 - accuracy: 0.5889\n",
      "Epoch 251/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1667 - accuracy: 0.5887\n",
      "Epoch 252/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1661 - accuracy: 0.5879\n",
      "Epoch 253/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1630 - accuracy: 0.5902\n",
      "Epoch 254/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1605 - accuracy: 0.5940\n",
      "Epoch 255/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1667 - accuracy: 0.5905\n",
      "Epoch 256/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1610 - accuracy: 0.5927\n",
      "Epoch 257/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1578 - accuracy: 0.5933\n",
      "Epoch 258/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1653 - accuracy: 0.5889\n",
      "Epoch 259/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1571 - accuracy: 0.5932\n",
      "Epoch 260/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1575 - accuracy: 0.5933\n",
      "Epoch 261/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1594 - accuracy: 0.5921\n",
      "Epoch 262/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1527 - accuracy: 0.5969\n",
      "Epoch 263/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1558 - accuracy: 0.5935\n",
      "Epoch 264/400\n",
      "1111/1111 [==============================] - 5s 4ms/step - loss: 1.1574 - accuracy: 0.5954\n",
      "Epoch 265/400\n",
      "1111/1111 [==============================] - 5s 4ms/step - loss: 1.1541 - accuracy: 0.5959\n",
      "Epoch 266/400\n",
      "1111/1111 [==============================] - 4s 4ms/step - loss: 1.1552 - accuracy: 0.5925\n",
      "Epoch 267/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1537 - accuracy: 0.5925\n",
      "Epoch 268/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1552 - accuracy: 0.5923\n",
      "Epoch 269/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1583 - accuracy: 0.5898\n",
      "Epoch 270/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1529 - accuracy: 0.5957\n",
      "Epoch 271/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1582 - accuracy: 0.5945\n",
      "Epoch 272/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1570 - accuracy: 0.5945\n",
      "Epoch 273/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1622 - accuracy: 0.5899\n",
      "Epoch 274/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1618 - accuracy: 0.5908\n",
      "Epoch 275/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1595 - accuracy: 0.5961\n",
      "Epoch 276/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1543 - accuracy: 0.5926\n",
      "Epoch 277/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1540 - accuracy: 0.5929\n",
      "Epoch 278/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1607 - accuracy: 0.5908\n",
      "Epoch 279/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1583 - accuracy: 0.5927\n",
      "Epoch 280/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1503 - accuracy: 0.5943\n",
      "Epoch 281/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1583 - accuracy: 0.5916\n",
      "Epoch 282/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1525 - accuracy: 0.5979\n",
      "Epoch 283/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1592 - accuracy: 0.5889\n",
      "Epoch 284/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1595 - accuracy: 0.5905\n",
      "Epoch 285/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1554 - accuracy: 0.5936\n",
      "Epoch 286/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1571 - accuracy: 0.5932\n",
      "Epoch 287/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1493 - accuracy: 0.5952\n",
      "Epoch 288/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1563 - accuracy: 0.5925\n",
      "Epoch 289/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1577 - accuracy: 0.5902\n",
      "Epoch 290/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1546 - accuracy: 0.5960\n",
      "Epoch 291/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.1510 - accuracy: 0.5992\n",
      "Epoch 292/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1579 - accuracy: 0.5916\n",
      "Epoch 293/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1554 - accuracy: 0.5915\n",
      "Epoch 294/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1532 - accuracy: 0.5951\n",
      "Epoch 295/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1487 - accuracy: 0.5952\n",
      "Epoch 296/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1522 - accuracy: 0.5951\n",
      "Epoch 297/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1542 - accuracy: 0.5953\n",
      "Epoch 298/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1504 - accuracy: 0.5958\n",
      "Epoch 299/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1546 - accuracy: 0.5929\n",
      "Epoch 300/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1520 - accuracy: 0.5916\n",
      "Epoch 301/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1487 - accuracy: 0.5971\n",
      "Epoch 302/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1506 - accuracy: 0.5991\n",
      "Epoch 303/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1531 - accuracy: 0.5947\n",
      "Epoch 304/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1526 - accuracy: 0.5928\n",
      "Epoch 305/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1538 - accuracy: 0.5942\n",
      "Epoch 306/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1527 - accuracy: 0.5939\n",
      "Epoch 307/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1533 - accuracy: 0.5915\n",
      "Epoch 308/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1536 - accuracy: 0.5957\n",
      "Epoch 309/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1497 - accuracy: 0.5960\n",
      "Epoch 310/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1539 - accuracy: 0.5967\n",
      "Epoch 311/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1548 - accuracy: 0.5950\n",
      "Epoch 312/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.1559 - accuracy: 0.5955\n",
      "Epoch 313/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1503 - accuracy: 0.5976\n",
      "Epoch 314/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.1559 - accuracy: 0.5960\n",
      "Epoch 315/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.1551 - accuracy: 0.5940\n",
      "Epoch 316/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.1509 - accuracy: 0.5980\n",
      "Epoch 317/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.1473 - accuracy: 0.5970\n",
      "Epoch 318/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.1454 - accuracy: 0.5982\n",
      "Epoch 319/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.1462 - accuracy: 0.5972\n",
      "Epoch 320/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1526 - accuracy: 0.5948\n",
      "Epoch 321/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.1448 - accuracy: 0.5979\n",
      "Epoch 322/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1474 - accuracy: 0.5999\n",
      "Epoch 323/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.1478 - accuracy: 0.5985\n",
      "Epoch 324/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1455 - accuracy: 0.5979\n",
      "Epoch 325/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1462 - accuracy: 0.5988\n",
      "Epoch 326/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1520 - accuracy: 0.5946\n",
      "Epoch 327/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1481 - accuracy: 0.5991\n",
      "Epoch 328/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1434 - accuracy: 0.6002\n",
      "Epoch 329/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1475 - accuracy: 0.5973\n",
      "Epoch 330/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1470 - accuracy: 0.5959\n",
      "Epoch 331/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.1448 - accuracy: 0.5969\n",
      "Epoch 332/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1486 - accuracy: 0.5987\n",
      "Epoch 333/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1502 - accuracy: 0.5955\n",
      "Epoch 334/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1482 - accuracy: 0.5970\n",
      "Epoch 335/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1498 - accuracy: 0.5950\n",
      "Epoch 336/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1601 - accuracy: 0.5907\n",
      "Epoch 337/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1465 - accuracy: 0.5980\n",
      "Epoch 338/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1483 - accuracy: 0.5952\n",
      "Epoch 339/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1449 - accuracy: 0.5998\n",
      "Epoch 340/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1449 - accuracy: 0.5969\n",
      "Epoch 341/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1459 - accuracy: 0.5966\n",
      "Epoch 342/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1436 - accuracy: 0.5956\n",
      "Epoch 343/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1514 - accuracy: 0.5997\n",
      "Epoch 344/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1526 - accuracy: 0.5967\n",
      "Epoch 345/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1502 - accuracy: 0.5972\n",
      "Epoch 346/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1467 - accuracy: 0.5977\n",
      "Epoch 347/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1442 - accuracy: 0.5951\n",
      "Epoch 348/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1466 - accuracy: 0.6001\n",
      "Epoch 349/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1422 - accuracy: 0.5997\n",
      "Epoch 350/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1443 - accuracy: 0.5984\n",
      "Epoch 351/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1450 - accuracy: 0.5992\n",
      "Epoch 352/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.1489 - accuracy: 0.5949\n",
      "Epoch 353/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1465 - accuracy: 0.5976\n",
      "Epoch 354/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.1506 - accuracy: 0.5954\n",
      "Epoch 355/400\n",
      "1111/1111 [==============================] - 4s 4ms/step - loss: 1.1471 - accuracy: 0.6009\n",
      "Epoch 356/400\n",
      "1111/1111 [==============================] - 4s 4ms/step - loss: 1.1502 - accuracy: 0.5963\n",
      "Epoch 357/400\n",
      "1111/1111 [==============================] - 4s 4ms/step - loss: 1.1456 - accuracy: 0.6004\n",
      "Epoch 358/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.1434 - accuracy: 0.6006\n",
      "Epoch 359/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.1482 - accuracy: 0.5997\n",
      "Epoch 360/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.1449 - accuracy: 0.5972\n",
      "Epoch 361/400\n",
      "1111/1111 [==============================] - 5s 4ms/step - loss: 1.1485 - accuracy: 0.5975\n",
      "Epoch 362/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.1509 - accuracy: 0.5958\n",
      "Epoch 363/400\n",
      "1111/1111 [==============================] - 4s 4ms/step - loss: 1.1464 - accuracy: 0.6016\n",
      "Epoch 364/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.1430 - accuracy: 0.5998\n",
      "Epoch 365/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.1468 - accuracy: 0.5987\n",
      "Epoch 366/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.1424 - accuracy: 0.6001\n",
      "Epoch 367/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.1426 - accuracy: 0.5990\n",
      "Epoch 368/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1420 - accuracy: 0.5981\n",
      "Epoch 369/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1450 - accuracy: 0.5974\n",
      "Epoch 370/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1434 - accuracy: 0.5975\n",
      "Epoch 371/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1432 - accuracy: 0.6010\n",
      "Epoch 372/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.1468 - accuracy: 0.5968\n",
      "Epoch 373/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1419 - accuracy: 0.6030\n",
      "Epoch 374/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1429 - accuracy: 0.6010\n",
      "Epoch 375/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1447 - accuracy: 0.5991\n",
      "Epoch 376/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1492 - accuracy: 0.5998\n",
      "Epoch 377/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1486 - accuracy: 0.5958\n",
      "Epoch 378/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1467 - accuracy: 0.5996\n",
      "Epoch 379/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1395 - accuracy: 0.6003\n",
      "Epoch 380/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1450 - accuracy: 0.6008\n",
      "Epoch 381/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1421 - accuracy: 0.6010\n",
      "Epoch 382/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1451 - accuracy: 0.6015\n",
      "Epoch 383/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1444 - accuracy: 0.5987\n",
      "Epoch 384/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1439 - accuracy: 0.5997\n",
      "Epoch 385/400\n",
      "1111/1111 [==============================] - 4s 4ms/step - loss: 1.1432 - accuracy: 0.6002\n",
      "Epoch 386/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.1461 - accuracy: 0.5964\n",
      "Epoch 387/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1444 - accuracy: 0.5988\n",
      "Epoch 388/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1380 - accuracy: 0.6024\n",
      "Epoch 389/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1443 - accuracy: 0.5993\n",
      "Epoch 390/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1448 - accuracy: 0.6011\n",
      "Epoch 391/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1441 - accuracy: 0.5989\n",
      "Epoch 392/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1460 - accuracy: 0.5974\n",
      "Epoch 393/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.1476 - accuracy: 0.5973\n",
      "Epoch 394/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.1459 - accuracy: 0.6005\n",
      "Epoch 395/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1426 - accuracy: 0.6001\n",
      "Epoch 396/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1453 - accuracy: 0.5988\n",
      "Epoch 397/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.1387 - accuracy: 0.6049\n",
      "Epoch 398/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.1427 - accuracy: 0.5980\n",
      "Epoch 399/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1367 - accuracy: 0.5992\n",
      "Epoch 400/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1493 - accuracy: 0.5953\n",
      "Epoch 1/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.6619 - accuracy: 0.3979\n",
      "Epoch 2/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.5581 - accuracy: 0.3992\n",
      "Epoch 3/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.5390 - accuracy: 0.4069\n",
      "Epoch 4/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.5224 - accuracy: 0.4115\n",
      "Epoch 5/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.4994 - accuracy: 0.4281\n",
      "Epoch 6/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.4730 - accuracy: 0.4450\n",
      "Epoch 7/400\n",
      "1111/1111 [==============================] - 4s 4ms/step - loss: 1.4481 - accuracy: 0.4608\n",
      "Epoch 8/400\n",
      "1111/1111 [==============================] - 4s 4ms/step - loss: 1.4238 - accuracy: 0.4754\n",
      "Epoch 9/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.4006 - accuracy: 0.4869\n",
      "Epoch 10/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.3812 - accuracy: 0.4910\n",
      "Epoch 11/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.3633 - accuracy: 0.5079\n",
      "Epoch 12/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.3513 - accuracy: 0.5062\n",
      "Epoch 13/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.3395 - accuracy: 0.5122\n",
      "Epoch 14/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.3278 - accuracy: 0.5173\n",
      "Epoch 15/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.3233 - accuracy: 0.5230\n",
      "Epoch 16/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.3185 - accuracy: 0.5205\n",
      "Epoch 17/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.3111 - accuracy: 0.5236\n",
      "Epoch 18/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.3035 - accuracy: 0.5257\n",
      "Epoch 19/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2990 - accuracy: 0.5329\n",
      "Epoch 20/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.2959 - accuracy: 0.5284\n",
      "Epoch 21/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.2876 - accuracy: 0.5359\n",
      "Epoch 22/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.2847 - accuracy: 0.5382\n",
      "Epoch 23/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.2793 - accuracy: 0.5348\n",
      "Epoch 24/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.2732 - accuracy: 0.5403\n",
      "Epoch 25/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.2758 - accuracy: 0.5396\n",
      "Epoch 26/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2741 - accuracy: 0.5416\n",
      "Epoch 27/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.2674 - accuracy: 0.5426\n",
      "Epoch 28/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.2636 - accuracy: 0.5438\n",
      "Epoch 29/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.2609 - accuracy: 0.5457\n",
      "Epoch 30/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.2571 - accuracy: 0.5477\n",
      "Epoch 31/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2561 - accuracy: 0.5476\n",
      "Epoch 32/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.2556 - accuracy: 0.5446\n",
      "Epoch 33/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.2531 - accuracy: 0.5507\n",
      "Epoch 34/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.2524 - accuracy: 0.5478\n",
      "Epoch 35/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.2545 - accuracy: 0.5457\n",
      "Epoch 36/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.2468 - accuracy: 0.5469\n",
      "Epoch 37/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.2519 - accuracy: 0.5454\n",
      "Epoch 38/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.2462 - accuracy: 0.5497\n",
      "Epoch 39/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.2423 - accuracy: 0.5530\n",
      "Epoch 40/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.2403 - accuracy: 0.5533\n",
      "Epoch 41/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.2407 - accuracy: 0.5476\n",
      "Epoch 42/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2401 - accuracy: 0.5564\n",
      "Epoch 43/400\n",
      "1111/1111 [==============================] - 4s 4ms/step - loss: 1.2353 - accuracy: 0.5576\n",
      "Epoch 44/400\n",
      "1111/1111 [==============================] - 4s 4ms/step - loss: 1.2319 - accuracy: 0.5564\n",
      "Epoch 45/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.2316 - accuracy: 0.5583\n",
      "Epoch 46/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.2305 - accuracy: 0.5570\n",
      "Epoch 47/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.2242 - accuracy: 0.5644\n",
      "Epoch 48/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.2251 - accuracy: 0.5631\n",
      "Epoch 49/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.2180 - accuracy: 0.5675\n",
      "Epoch 50/400\n",
      "1111/1111 [==============================] - 4s 4ms/step - loss: 1.2178 - accuracy: 0.5689\n",
      "Epoch 51/400\n",
      "1111/1111 [==============================] - 5s 5ms/step - loss: 1.2138 - accuracy: 0.5731\n",
      "Epoch 52/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.2136 - accuracy: 0.5718\n",
      "Epoch 53/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.2109 - accuracy: 0.5675\n",
      "Epoch 54/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.2079 - accuracy: 0.5692\n",
      "Epoch 55/400\n",
      "1111/1111 [==============================] - 4s 4ms/step - loss: 1.2089 - accuracy: 0.5720\n",
      "Epoch 56/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.2080 - accuracy: 0.5703\n",
      "Epoch 57/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.2050 - accuracy: 0.5715\n",
      "Epoch 58/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.2036 - accuracy: 0.5738\n",
      "Epoch 59/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.2024 - accuracy: 0.5699\n",
      "Epoch 60/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.2006 - accuracy: 0.5722\n",
      "Epoch 61/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.1981 - accuracy: 0.5769\n",
      "Epoch 62/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.1983 - accuracy: 0.5708\n",
      "Epoch 63/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.1963 - accuracy: 0.5728\n",
      "Epoch 64/400\n",
      "1111/1111 [==============================] - 4s 4ms/step - loss: 1.1922 - accuracy: 0.5781\n",
      "Epoch 65/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.1899 - accuracy: 0.5794\n",
      "Epoch 66/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.1940 - accuracy: 0.5773\n",
      "Epoch 67/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.1865 - accuracy: 0.5791\n",
      "Epoch 68/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.1883 - accuracy: 0.5808\n",
      "Epoch 69/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.1899 - accuracy: 0.5756\n",
      "Epoch 70/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.1843 - accuracy: 0.5812\n",
      "Epoch 71/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1866 - accuracy: 0.5782\n",
      "Epoch 72/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.1862 - accuracy: 0.5783\n",
      "Epoch 73/400\n",
      "1111/1111 [==============================] - 4s 4ms/step - loss: 1.1839 - accuracy: 0.5842\n",
      "Epoch 74/400\n",
      "1111/1111 [==============================] - 5s 5ms/step - loss: 1.1839 - accuracy: 0.5783\n",
      "Epoch 75/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1815 - accuracy: 0.5807\n",
      "Epoch 76/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.1814 - accuracy: 0.5808\n",
      "Epoch 77/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1772 - accuracy: 0.5801\n",
      "Epoch 78/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1798 - accuracy: 0.5828\n",
      "Epoch 79/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1813 - accuracy: 0.5837\n",
      "Epoch 80/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.1801 - accuracy: 0.5767\n",
      "Epoch 81/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.1750 - accuracy: 0.5829\n",
      "Epoch 82/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1737 - accuracy: 0.5830\n",
      "Epoch 83/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.1737 - accuracy: 0.5797\n",
      "Epoch 84/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1761 - accuracy: 0.5790\n",
      "Epoch 85/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1733 - accuracy: 0.5812\n",
      "Epoch 86/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1731 - accuracy: 0.5825\n",
      "Epoch 87/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1703 - accuracy: 0.5852\n",
      "Epoch 88/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1733 - accuracy: 0.5848\n",
      "Epoch 89/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1691 - accuracy: 0.5866\n",
      "Epoch 90/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.1688 - accuracy: 0.5840\n",
      "Epoch 91/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.1669 - accuracy: 0.5854\n",
      "Epoch 92/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.1704 - accuracy: 0.5843\n",
      "Epoch 93/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1668 - accuracy: 0.5867\n",
      "Epoch 94/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1680 - accuracy: 0.5841\n",
      "Epoch 95/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.1699 - accuracy: 0.5867\n",
      "Epoch 96/400\n",
      "1111/1111 [==============================] - 4s 4ms/step - loss: 1.1666 - accuracy: 0.5823\n",
      "Epoch 97/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1674 - accuracy: 0.5810\n",
      "Epoch 98/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1649 - accuracy: 0.5898\n",
      "Epoch 99/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1664 - accuracy: 0.5871\n",
      "Epoch 100/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1612 - accuracy: 0.5875\n",
      "Epoch 101/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1639 - accuracy: 0.5871\n",
      "Epoch 102/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1631 - accuracy: 0.5871\n",
      "Epoch 103/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1634 - accuracy: 0.5927\n",
      "Epoch 104/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1627 - accuracy: 0.5876\n",
      "Epoch 105/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1646 - accuracy: 0.5853\n",
      "Epoch 106/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1601 - accuracy: 0.5916\n",
      "Epoch 107/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1573 - accuracy: 0.5901\n",
      "Epoch 108/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1570 - accuracy: 0.5927\n",
      "Epoch 109/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1599 - accuracy: 0.5906\n",
      "Epoch 110/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1625 - accuracy: 0.5862\n",
      "Epoch 111/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1610 - accuracy: 0.5837\n",
      "Epoch 112/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1566 - accuracy: 0.5892\n",
      "Epoch 113/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1590 - accuracy: 0.5891\n",
      "Epoch 114/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1585 - accuracy: 0.5887\n",
      "Epoch 115/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1548 - accuracy: 0.5937\n",
      "Epoch 116/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1559 - accuracy: 0.5867\n",
      "Epoch 117/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1557 - accuracy: 0.5909\n",
      "Epoch 118/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1563 - accuracy: 0.5887\n",
      "Epoch 119/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1554 - accuracy: 0.5908\n",
      "Epoch 120/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1567 - accuracy: 0.5875\n",
      "Epoch 121/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1540 - accuracy: 0.5907\n",
      "Epoch 122/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1518 - accuracy: 0.5938\n",
      "Epoch 123/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1538 - accuracy: 0.5910\n",
      "Epoch 124/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1545 - accuracy: 0.5906\n",
      "Epoch 125/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1518 - accuracy: 0.5892\n",
      "Epoch 126/400\n",
      "1111/1111 [==============================] - 4s 4ms/step - loss: 1.1541 - accuracy: 0.5898\n",
      "Epoch 127/400\n",
      "1111/1111 [==============================] - 4s 4ms/step - loss: 1.1574 - accuracy: 0.5875\n",
      "Epoch 128/400\n",
      "1111/1111 [==============================] - 4s 4ms/step - loss: 1.1519 - accuracy: 0.5904\n",
      "Epoch 129/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1492 - accuracy: 0.5913\n",
      "Epoch 130/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1486 - accuracy: 0.5912\n",
      "Epoch 131/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1488 - accuracy: 0.5905\n",
      "Epoch 132/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1517 - accuracy: 0.5927\n",
      "Epoch 133/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1505 - accuracy: 0.5929\n",
      "Epoch 134/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1473 - accuracy: 0.5946\n",
      "Epoch 135/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1489 - accuracy: 0.5927\n",
      "Epoch 136/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1487 - accuracy: 0.5895\n",
      "Epoch 137/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1490 - accuracy: 0.5915\n",
      "Epoch 138/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1472 - accuracy: 0.5950\n",
      "Epoch 139/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.1504 - accuracy: 0.5933\n",
      "Epoch 140/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1476 - accuracy: 0.5950\n",
      "Epoch 141/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1465 - accuracy: 0.5952\n",
      "Epoch 142/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1455 - accuracy: 0.5960\n",
      "Epoch 143/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1461 - accuracy: 0.5953\n",
      "Epoch 144/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1454 - accuracy: 0.5944\n",
      "Epoch 145/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1457 - accuracy: 0.5952\n",
      "Epoch 146/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1444 - accuracy: 0.5903\n",
      "Epoch 147/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1475 - accuracy: 0.5919\n",
      "Epoch 148/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1429 - accuracy: 0.5949\n",
      "Epoch 149/400\n",
      "1111/1111 [==============================] - 3s 2ms/step - loss: 1.1406 - accuracy: 0.5925\n",
      "Epoch 150/400\n",
      "1111/1111 [==============================] - 3s 2ms/step - loss: 1.1439 - accuracy: 0.5948\n",
      "Epoch 151/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1434 - accuracy: 0.5949\n",
      "Epoch 152/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1416 - accuracy: 0.5937\n",
      "Epoch 153/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1451 - accuracy: 0.5960\n",
      "Epoch 154/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1443 - accuracy: 0.5934\n",
      "Epoch 155/400\n",
      "1111/1111 [==============================] - 3s 2ms/step - loss: 1.1416 - accuracy: 0.5938\n",
      "Epoch 156/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1454 - accuracy: 0.5952\n",
      "Epoch 157/400\n",
      "1111/1111 [==============================] - 3s 2ms/step - loss: 1.1419 - accuracy: 0.5954\n",
      "Epoch 158/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1437 - accuracy: 0.5958\n",
      "Epoch 159/400\n",
      "1111/1111 [==============================] - 3s 2ms/step - loss: 1.1444 - accuracy: 0.5922\n",
      "Epoch 160/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1416 - accuracy: 0.5964\n",
      "Epoch 161/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1396 - accuracy: 0.5944\n",
      "Epoch 162/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1398 - accuracy: 0.5936\n",
      "Epoch 163/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1437 - accuracy: 0.5952\n",
      "Epoch 164/400\n",
      "1111/1111 [==============================] - 3s 2ms/step - loss: 1.1389 - accuracy: 0.5955\n",
      "Epoch 165/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1406 - accuracy: 0.5960\n",
      "Epoch 166/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1363 - accuracy: 0.5997\n",
      "Epoch 167/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1394 - accuracy: 0.5987\n",
      "Epoch 168/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1397 - accuracy: 0.5972\n",
      "Epoch 169/400\n",
      "1111/1111 [==============================] - 3s 2ms/step - loss: 1.1409 - accuracy: 0.5934\n",
      "Epoch 170/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1410 - accuracy: 0.5978\n",
      "Epoch 171/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1396 - accuracy: 0.5998\n",
      "Epoch 172/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1364 - accuracy: 0.5961\n",
      "Epoch 173/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1400 - accuracy: 0.5973\n",
      "Epoch 174/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1378 - accuracy: 0.5961\n",
      "Epoch 175/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1385 - accuracy: 0.5976\n",
      "Epoch 176/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1375 - accuracy: 0.5951\n",
      "Epoch 177/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1373 - accuracy: 0.5974\n",
      "Epoch 178/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1356 - accuracy: 0.6008\n",
      "Epoch 179/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1373 - accuracy: 0.5988\n",
      "Epoch 180/400\n",
      "1111/1111 [==============================] - 3s 2ms/step - loss: 1.1401 - accuracy: 0.5972\n",
      "Epoch 181/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1349 - accuracy: 0.6008\n",
      "Epoch 182/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1379 - accuracy: 0.5987\n",
      "Epoch 183/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1352 - accuracy: 0.5988\n",
      "Epoch 184/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1339 - accuracy: 0.5989\n",
      "Epoch 185/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1361 - accuracy: 0.5981\n",
      "Epoch 186/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1344 - accuracy: 0.5958\n",
      "Epoch 187/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1376 - accuracy: 0.5988\n",
      "Epoch 188/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1338 - accuracy: 0.5992\n",
      "Epoch 189/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1345 - accuracy: 0.5974\n",
      "Epoch 190/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1338 - accuracy: 0.6000\n",
      "Epoch 191/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1348 - accuracy: 0.5986\n",
      "Epoch 192/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1382 - accuracy: 0.5952\n",
      "Epoch 193/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1361 - accuracy: 0.5961\n",
      "Epoch 194/400\n",
      "1111/1111 [==============================] - 3s 2ms/step - loss: 1.1354 - accuracy: 0.5989\n",
      "Epoch 195/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1377 - accuracy: 0.5975\n",
      "Epoch 196/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1317 - accuracy: 0.5999\n",
      "Epoch 197/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1350 - accuracy: 0.6000\n",
      "Epoch 198/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1322 - accuracy: 0.6006\n",
      "Epoch 199/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1394 - accuracy: 0.6001\n",
      "Epoch 200/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1329 - accuracy: 0.5998\n",
      "Epoch 201/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1337 - accuracy: 0.5988\n",
      "Epoch 202/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1371 - accuracy: 0.5947\n",
      "Epoch 203/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1350 - accuracy: 0.5992\n",
      "Epoch 204/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1331 - accuracy: 0.5997\n",
      "Epoch 205/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1327 - accuracy: 0.5957\n",
      "Epoch 206/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1343 - accuracy: 0.5985\n",
      "Epoch 207/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1296 - accuracy: 0.6003\n",
      "Epoch 208/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1316 - accuracy: 0.5994\n",
      "Epoch 209/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1321 - accuracy: 0.5987\n",
      "Epoch 210/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1300 - accuracy: 0.6033\n",
      "Epoch 211/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1315 - accuracy: 0.5987\n",
      "Epoch 212/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1309 - accuracy: 0.6019\n",
      "Epoch 213/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1282 - accuracy: 0.5985\n",
      "Epoch 214/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1338 - accuracy: 0.5984\n",
      "Epoch 215/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1301 - accuracy: 0.5996\n",
      "Epoch 216/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1279 - accuracy: 0.6014\n",
      "Epoch 217/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1299 - accuracy: 0.6010\n",
      "Epoch 218/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1281 - accuracy: 0.6018\n",
      "Epoch 219/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1347 - accuracy: 0.5988\n",
      "Epoch 220/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1288 - accuracy: 0.6020\n",
      "Epoch 221/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1274 - accuracy: 0.6011\n",
      "Epoch 222/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1294 - accuracy: 0.6012\n",
      "Epoch 223/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1311 - accuracy: 0.6001\n",
      "Epoch 224/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1335 - accuracy: 0.5958\n",
      "Epoch 225/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1274 - accuracy: 0.6000\n",
      "Epoch 226/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1288 - accuracy: 0.6021\n",
      "Epoch 227/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1268 - accuracy: 0.5988\n",
      "Epoch 228/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1295 - accuracy: 0.6006\n",
      "Epoch 229/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1261 - accuracy: 0.6017\n",
      "Epoch 230/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1292 - accuracy: 0.5984\n",
      "Epoch 231/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1243 - accuracy: 0.6050\n",
      "Epoch 232/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1298 - accuracy: 0.6012\n",
      "Epoch 233/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1262 - accuracy: 0.6023\n",
      "Epoch 234/400\n",
      "1111/1111 [==============================] - 6s 5ms/step - loss: 1.1290 - accuracy: 0.6008\n",
      "Epoch 235/400\n",
      "1111/1111 [==============================] - 5s 4ms/step - loss: 1.1302 - accuracy: 0.6005\n",
      "Epoch 236/400\n",
      "1111/1111 [==============================] - 5s 5ms/step - loss: 1.1287 - accuracy: 0.5999\n",
      "Epoch 237/400\n",
      "1111/1111 [==============================] - 5s 4ms/step - loss: 1.1264 - accuracy: 0.5997\n",
      "Epoch 238/400\n",
      "1111/1111 [==============================] - 5s 4ms/step - loss: 1.1256 - accuracy: 0.6004\n",
      "Epoch 239/400\n",
      "1111/1111 [==============================] - 5s 4ms/step - loss: 1.1279 - accuracy: 0.6010\n",
      "Epoch 240/400\n",
      "1111/1111 [==============================] - 5s 4ms/step - loss: 1.1287 - accuracy: 0.6016\n",
      "Epoch 241/400\n",
      "1111/1111 [==============================] - 5s 4ms/step - loss: 1.1299 - accuracy: 0.6002\n",
      "Epoch 242/400\n",
      "1111/1111 [==============================] - 5s 4ms/step - loss: 1.1302 - accuracy: 0.6000\n",
      "Epoch 243/400\n",
      "1111/1111 [==============================] - 5s 4ms/step - loss: 1.1278 - accuracy: 0.5970\n",
      "Epoch 244/400\n",
      "1111/1111 [==============================] - 5s 4ms/step - loss: 1.1276 - accuracy: 0.6042\n",
      "Epoch 245/400\n",
      "1111/1111 [==============================] - 5s 4ms/step - loss: 1.1229 - accuracy: 0.6015\n",
      "Epoch 246/400\n",
      "1111/1111 [==============================] - 5s 4ms/step - loss: 1.1287 - accuracy: 0.6028\n",
      "Epoch 247/400\n",
      "1111/1111 [==============================] - 5s 4ms/step - loss: 1.1248 - accuracy: 0.6033\n",
      "Epoch 248/400\n",
      "1111/1111 [==============================] - 5s 4ms/step - loss: 1.1294 - accuracy: 0.5980\n",
      "Epoch 249/400\n",
      "1111/1111 [==============================] - 5s 4ms/step - loss: 1.1283 - accuracy: 0.6013\n",
      "Epoch 250/400\n",
      "1111/1111 [==============================] - 4s 4ms/step - loss: 1.1260 - accuracy: 0.6002\n",
      "Epoch 251/400\n",
      "1111/1111 [==============================] - 4s 4ms/step - loss: 1.1279 - accuracy: 0.6006\n",
      "Epoch 252/400\n",
      "1111/1111 [==============================] - 4s 4ms/step - loss: 1.1243 - accuracy: 0.6020\n",
      "Epoch 253/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.1289 - accuracy: 0.6012\n",
      "Epoch 254/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.1307 - accuracy: 0.5995\n",
      "Epoch 255/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.1295 - accuracy: 0.6009\n",
      "Epoch 256/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.1260 - accuracy: 0.6020\n",
      "Epoch 257/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.1228 - accuracy: 0.6042\n",
      "Epoch 258/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.1281 - accuracy: 0.6000\n",
      "Epoch 259/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.1286 - accuracy: 0.5997\n",
      "Epoch 260/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.1246 - accuracy: 0.6027\n",
      "Epoch 261/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.1266 - accuracy: 0.6002\n",
      "Epoch 262/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.1232 - accuracy: 0.6036\n",
      "Epoch 263/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.1232 - accuracy: 0.6014\n",
      "Epoch 264/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.1260 - accuracy: 0.5998\n",
      "Epoch 265/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.1212 - accuracy: 0.6046\n",
      "Epoch 266/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.1274 - accuracy: 0.6023\n",
      "Epoch 267/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.1261 - accuracy: 0.6025\n",
      "Epoch 268/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1226 - accuracy: 0.6046\n",
      "Epoch 269/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1197 - accuracy: 0.6024\n",
      "Epoch 270/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1220 - accuracy: 0.6013\n",
      "Epoch 271/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1244 - accuracy: 0.6033\n",
      "Epoch 272/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1237 - accuracy: 0.6004\n",
      "Epoch 273/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1211 - accuracy: 0.6039\n",
      "Epoch 274/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1287 - accuracy: 0.6036\n",
      "Epoch 275/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1213 - accuracy: 0.6040\n",
      "Epoch 276/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1220 - accuracy: 0.6015\n",
      "Epoch 277/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1236 - accuracy: 0.6023\n",
      "Epoch 278/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1197 - accuracy: 0.6073\n",
      "Epoch 279/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1219 - accuracy: 0.6001\n",
      "Epoch 280/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1225 - accuracy: 0.6005\n",
      "Epoch 281/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1230 - accuracy: 0.6075\n",
      "Epoch 282/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1245 - accuracy: 0.6031\n",
      "Epoch 283/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1212 - accuracy: 0.6061\n",
      "Epoch 284/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1228 - accuracy: 0.6029\n",
      "Epoch 285/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1206 - accuracy: 0.6079\n",
      "Epoch 286/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1230 - accuracy: 0.6041\n",
      "Epoch 287/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1250 - accuracy: 0.5994\n",
      "Epoch 288/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1231 - accuracy: 0.6018\n",
      "Epoch 289/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1203 - accuracy: 0.6071\n",
      "Epoch 290/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1233 - accuracy: 0.6023\n",
      "Epoch 291/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1210 - accuracy: 0.6041\n",
      "Epoch 292/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1221 - accuracy: 0.6045\n",
      "Epoch 293/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1239 - accuracy: 0.6013\n",
      "Epoch 294/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1177 - accuracy: 0.6045\n",
      "Epoch 295/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1228 - accuracy: 0.6013\n",
      "Epoch 296/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1202 - accuracy: 0.6041\n",
      "Epoch 297/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1207 - accuracy: 0.6060\n",
      "Epoch 298/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1191 - accuracy: 0.6051\n",
      "Epoch 299/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1225 - accuracy: 0.6041\n",
      "Epoch 300/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1266 - accuracy: 0.6006\n",
      "Epoch 301/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1198 - accuracy: 0.6039\n",
      "Epoch 302/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1196 - accuracy: 0.6018\n",
      "Epoch 303/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1212 - accuracy: 0.6030\n",
      "Epoch 304/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1171 - accuracy: 0.6051\n",
      "Epoch 305/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1209 - accuracy: 0.6036\n",
      "Epoch 306/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1212 - accuracy: 0.6024\n",
      "Epoch 307/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1189 - accuracy: 0.6061\n",
      "Epoch 308/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1182 - accuracy: 0.6079\n",
      "Epoch 309/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1190 - accuracy: 0.6049\n",
      "Epoch 310/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1214 - accuracy: 0.6006\n",
      "Epoch 311/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1221 - accuracy: 0.6015\n",
      "Epoch 312/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1223 - accuracy: 0.6044\n",
      "Epoch 313/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1197 - accuracy: 0.6055\n",
      "Epoch 314/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1181 - accuracy: 0.6038\n",
      "Epoch 315/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1208 - accuracy: 0.6025\n",
      "Epoch 316/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1144 - accuracy: 0.6060\n",
      "Epoch 317/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1215 - accuracy: 0.6024\n",
      "Epoch 318/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1191 - accuracy: 0.6068\n",
      "Epoch 319/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1167 - accuracy: 0.6053\n",
      "Epoch 320/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1168 - accuracy: 0.6068\n",
      "Epoch 321/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1154 - accuracy: 0.6072\n",
      "Epoch 322/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1199 - accuracy: 0.6024\n",
      "Epoch 323/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1185 - accuracy: 0.6033\n",
      "Epoch 324/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1168 - accuracy: 0.6037\n",
      "Epoch 325/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1169 - accuracy: 0.6084\n",
      "Epoch 326/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1140 - accuracy: 0.6079\n",
      "Epoch 327/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1175 - accuracy: 0.6054\n",
      "Epoch 328/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1161 - accuracy: 0.6079\n",
      "Epoch 329/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1141 - accuracy: 0.6048\n",
      "Epoch 330/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1176 - accuracy: 0.6071\n",
      "Epoch 331/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1210 - accuracy: 0.6059\n",
      "Epoch 332/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1158 - accuracy: 0.6051\n",
      "Epoch 333/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1162 - accuracy: 0.6023\n",
      "Epoch 334/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1181 - accuracy: 0.6042\n",
      "Epoch 335/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1166 - accuracy: 0.6052\n",
      "Epoch 336/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1177 - accuracy: 0.6050\n",
      "Epoch 337/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1209 - accuracy: 0.6038\n",
      "Epoch 338/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1152 - accuracy: 0.6033\n",
      "Epoch 339/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1166 - accuracy: 0.6044\n",
      "Epoch 340/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1138 - accuracy: 0.6066\n",
      "Epoch 341/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1194 - accuracy: 0.6066\n",
      "Epoch 342/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1169 - accuracy: 0.6054\n",
      "Epoch 343/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1154 - accuracy: 0.6065\n",
      "Epoch 344/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1200 - accuracy: 0.6056\n",
      "Epoch 345/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1144 - accuracy: 0.6059\n",
      "Epoch 346/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1144 - accuracy: 0.6073\n",
      "Epoch 347/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1127 - accuracy: 0.6055\n",
      "Epoch 348/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1163 - accuracy: 0.6054\n",
      "Epoch 349/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1175 - accuracy: 0.6085\n",
      "Epoch 350/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1155 - accuracy: 0.6041\n",
      "Epoch 351/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1132 - accuracy: 0.6057\n",
      "Epoch 352/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1154 - accuracy: 0.6096\n",
      "Epoch 353/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1139 - accuracy: 0.6062\n",
      "Epoch 354/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1136 - accuracy: 0.6059\n",
      "Epoch 355/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1115 - accuracy: 0.6055\n",
      "Epoch 356/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1160 - accuracy: 0.6046\n",
      "Epoch 357/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1152 - accuracy: 0.6052\n",
      "Epoch 358/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1158 - accuracy: 0.6023\n",
      "Epoch 359/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1159 - accuracy: 0.6045\n",
      "Epoch 360/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1133 - accuracy: 0.6096\n",
      "Epoch 361/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1134 - accuracy: 0.6080\n",
      "Epoch 362/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1141 - accuracy: 0.6062\n",
      "Epoch 363/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1121 - accuracy: 0.6072\n",
      "Epoch 364/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1123 - accuracy: 0.6060\n",
      "Epoch 365/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1118 - accuracy: 0.6059\n",
      "Epoch 366/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1133 - accuracy: 0.6058\n",
      "Epoch 367/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1122 - accuracy: 0.6041\n",
      "Epoch 368/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1154 - accuracy: 0.6075\n",
      "Epoch 369/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1139 - accuracy: 0.6079\n",
      "Epoch 370/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1147 - accuracy: 0.6075\n",
      "Epoch 371/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1150 - accuracy: 0.6092\n",
      "Epoch 372/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1129 - accuracy: 0.6090\n",
      "Epoch 373/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1131 - accuracy: 0.6051\n",
      "Epoch 374/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1138 - accuracy: 0.6062\n",
      "Epoch 375/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1146 - accuracy: 0.6048\n",
      "Epoch 376/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1140 - accuracy: 0.6067\n",
      "Epoch 377/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1175 - accuracy: 0.6063\n",
      "Epoch 378/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1132 - accuracy: 0.6082\n",
      "Epoch 379/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1133 - accuracy: 0.6056\n",
      "Epoch 380/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1084 - accuracy: 0.6083\n",
      "Epoch 381/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1130 - accuracy: 0.6093\n",
      "Epoch 382/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1106 - accuracy: 0.6097\n",
      "Epoch 383/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1126 - accuracy: 0.6069\n",
      "Epoch 384/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1104 - accuracy: 0.6057\n",
      "Epoch 385/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1088 - accuracy: 0.6103\n",
      "Epoch 386/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1104 - accuracy: 0.6062\n",
      "Epoch 387/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1116 - accuracy: 0.6076\n",
      "Epoch 388/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1115 - accuracy: 0.6057\n",
      "Epoch 389/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1074 - accuracy: 0.6099\n",
      "Epoch 390/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1111 - accuracy: 0.6097\n",
      "Epoch 391/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1108 - accuracy: 0.6083\n",
      "Epoch 392/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1108 - accuracy: 0.6096\n",
      "Epoch 393/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1106 - accuracy: 0.6064\n",
      "Epoch 394/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1083 - accuracy: 0.6117\n",
      "Epoch 395/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1123 - accuracy: 0.6066\n",
      "Epoch 396/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1108 - accuracy: 0.6071\n",
      "Epoch 397/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1131 - accuracy: 0.6052\n",
      "Epoch 398/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1078 - accuracy: 0.6108\n",
      "Epoch 399/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1101 - accuracy: 0.6078\n",
      "Epoch 400/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1133 - accuracy: 0.6099\n",
      "Epoch 1/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.6659 - accuracy: 0.3967\n",
      "Epoch 2/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.5626 - accuracy: 0.3994\n",
      "Epoch 3/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.5450 - accuracy: 0.4030\n",
      "Epoch 4/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.5292 - accuracy: 0.4100\n",
      "Epoch 5/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.5085 - accuracy: 0.4158\n",
      "Epoch 6/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.4891 - accuracy: 0.4306\n",
      "Epoch 7/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.4615 - accuracy: 0.4533\n",
      "Epoch 8/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.4324 - accuracy: 0.4780\n",
      "Epoch 9/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.4051 - accuracy: 0.4869\n",
      "Epoch 10/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.3813 - accuracy: 0.5031\n",
      "Epoch 11/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.3570 - accuracy: 0.5096\n",
      "Epoch 12/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.3412 - accuracy: 0.5196\n",
      "Epoch 13/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.3225 - accuracy: 0.5282\n",
      "Epoch 14/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.3177 - accuracy: 0.5300\n",
      "Epoch 15/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.3022 - accuracy: 0.5345\n",
      "Epoch 16/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.2901 - accuracy: 0.5378\n",
      "Epoch 17/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.2848 - accuracy: 0.5432\n",
      "Epoch 18/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2742 - accuracy: 0.5506\n",
      "Epoch 19/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2673 - accuracy: 0.5514\n",
      "Epoch 20/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2614 - accuracy: 0.5531\n",
      "Epoch 21/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2583 - accuracy: 0.5537\n",
      "Epoch 22/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2529 - accuracy: 0.5565\n",
      "Epoch 23/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2505 - accuracy: 0.5565\n",
      "Epoch 24/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2464 - accuracy: 0.5583\n",
      "Epoch 25/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2435 - accuracy: 0.5590\n",
      "Epoch 26/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2430 - accuracy: 0.5601\n",
      "Epoch 27/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2349 - accuracy: 0.5652\n",
      "Epoch 28/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2324 - accuracy: 0.5680\n",
      "Epoch 29/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2269 - accuracy: 0.5703\n",
      "Epoch 30/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2253 - accuracy: 0.5700\n",
      "Epoch 31/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2244 - accuracy: 0.5695\n",
      "Epoch 32/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2260 - accuracy: 0.5714\n",
      "Epoch 33/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2214 - accuracy: 0.5704\n",
      "Epoch 34/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2192 - accuracy: 0.5732\n",
      "Epoch 35/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2210 - accuracy: 0.5696\n",
      "Epoch 36/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2200 - accuracy: 0.5711\n",
      "Epoch 37/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2130 - accuracy: 0.5742\n",
      "Epoch 38/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2108 - accuracy: 0.5782\n",
      "Epoch 39/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2114 - accuracy: 0.5755\n",
      "Epoch 40/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2094 - accuracy: 0.5767\n",
      "Epoch 41/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2077 - accuracy: 0.5763\n",
      "Epoch 42/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2091 - accuracy: 0.5717\n",
      "Epoch 43/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2072 - accuracy: 0.5743\n",
      "Epoch 44/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2134 - accuracy: 0.5729\n",
      "Epoch 45/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2041 - accuracy: 0.5761\n",
      "Epoch 46/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2040 - accuracy: 0.5781\n",
      "Epoch 47/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.2023 - accuracy: 0.5796\n",
      "Epoch 48/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1994 - accuracy: 0.5762\n",
      "Epoch 49/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1965 - accuracy: 0.5778\n",
      "Epoch 50/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1974 - accuracy: 0.5786\n",
      "Epoch 51/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1945 - accuracy: 0.5823\n",
      "Epoch 52/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1990 - accuracy: 0.5794\n",
      "Epoch 53/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1974 - accuracy: 0.5772\n",
      "Epoch 54/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1927 - accuracy: 0.5790\n",
      "Epoch 55/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1915 - accuracy: 0.5814\n",
      "Epoch 56/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1914 - accuracy: 0.5811\n",
      "Epoch 57/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1935 - accuracy: 0.5809\n",
      "Epoch 58/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1927 - accuracy: 0.5781\n",
      "Epoch 59/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1872 - accuracy: 0.5832\n",
      "Epoch 60/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1843 - accuracy: 0.5847\n",
      "Epoch 61/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1906 - accuracy: 0.5818\n",
      "Epoch 62/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1862 - accuracy: 0.5808\n",
      "Epoch 63/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1853 - accuracy: 0.5829\n",
      "Epoch 64/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1826 - accuracy: 0.5848\n",
      "Epoch 65/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1819 - accuracy: 0.5853\n",
      "Epoch 66/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1789 - accuracy: 0.5837\n",
      "Epoch 67/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1790 - accuracy: 0.5866\n",
      "Epoch 68/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1831 - accuracy: 0.5813\n",
      "Epoch 69/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1807 - accuracy: 0.5831\n",
      "Epoch 70/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1852 - accuracy: 0.5782\n",
      "Epoch 71/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1742 - accuracy: 0.5878\n",
      "Epoch 72/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1826 - accuracy: 0.5771\n",
      "Epoch 73/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1773 - accuracy: 0.5844\n",
      "Epoch 74/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1749 - accuracy: 0.5866\n",
      "Epoch 75/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1796 - accuracy: 0.5827\n",
      "Epoch 76/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1761 - accuracy: 0.5845\n",
      "Epoch 77/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1763 - accuracy: 0.5862\n",
      "Epoch 78/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1792 - accuracy: 0.5873\n",
      "Epoch 79/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1752 - accuracy: 0.5843\n",
      "Epoch 80/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1752 - accuracy: 0.5820\n",
      "Epoch 81/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1720 - accuracy: 0.5857\n",
      "Epoch 82/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1718 - accuracy: 0.5844\n",
      "Epoch 83/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1663 - accuracy: 0.5907\n",
      "Epoch 84/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1713 - accuracy: 0.5844\n",
      "Epoch 85/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1759 - accuracy: 0.5843\n",
      "Epoch 86/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1698 - accuracy: 0.5894\n",
      "Epoch 87/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1707 - accuracy: 0.5825\n",
      "Epoch 88/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1672 - accuracy: 0.5867\n",
      "Epoch 89/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1677 - accuracy: 0.5887\n",
      "Epoch 90/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1662 - accuracy: 0.5896\n",
      "Epoch 91/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1684 - accuracy: 0.5872\n",
      "Epoch 92/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1647 - accuracy: 0.5928\n",
      "Epoch 93/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1628 - accuracy: 0.5889\n",
      "Epoch 94/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1639 - accuracy: 0.5884\n",
      "Epoch 95/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1646 - accuracy: 0.5896\n",
      "Epoch 96/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1606 - accuracy: 0.5935\n",
      "Epoch 97/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1599 - accuracy: 0.5912\n",
      "Epoch 98/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1612 - accuracy: 0.5936\n",
      "Epoch 99/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1662 - accuracy: 0.5857\n",
      "Epoch 100/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1591 - accuracy: 0.5910\n",
      "Epoch 101/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1608 - accuracy: 0.5882\n",
      "Epoch 102/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1598 - accuracy: 0.5863\n",
      "Epoch 103/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1586 - accuracy: 0.5920\n",
      "Epoch 104/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1613 - accuracy: 0.5925\n",
      "Epoch 105/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1593 - accuracy: 0.5914\n",
      "Epoch 106/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1563 - accuracy: 0.5903\n",
      "Epoch 107/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1634 - accuracy: 0.5845\n",
      "Epoch 108/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1619 - accuracy: 0.5923\n",
      "Epoch 109/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1576 - accuracy: 0.5904\n",
      "Epoch 110/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1578 - accuracy: 0.5931\n",
      "Epoch 111/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1572 - accuracy: 0.5908\n",
      "Epoch 112/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1513 - accuracy: 0.5924\n",
      "Epoch 113/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1556 - accuracy: 0.5910\n",
      "Epoch 114/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1512 - accuracy: 0.5895\n",
      "Epoch 115/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1550 - accuracy: 0.5897\n",
      "Epoch 116/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1506 - accuracy: 0.5942\n",
      "Epoch 117/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1498 - accuracy: 0.5925\n",
      "Epoch 118/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1499 - accuracy: 0.5935\n",
      "Epoch 119/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1585 - accuracy: 0.5882\n",
      "Epoch 120/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1503 - accuracy: 0.5916\n",
      "Epoch 121/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1498 - accuracy: 0.5933\n",
      "Epoch 122/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1549 - accuracy: 0.5925\n",
      "Epoch 123/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1511 - accuracy: 0.5949\n",
      "Epoch 124/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1488 - accuracy: 0.5935\n",
      "Epoch 125/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1534 - accuracy: 0.5912\n",
      "Epoch 126/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1514 - accuracy: 0.5897\n",
      "Epoch 127/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1516 - accuracy: 0.5942\n",
      "Epoch 128/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1498 - accuracy: 0.5940\n",
      "Epoch 129/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1512 - accuracy: 0.5967\n",
      "Epoch 130/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1473 - accuracy: 0.5947\n",
      "Epoch 131/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1533 - accuracy: 0.5906\n",
      "Epoch 132/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1487 - accuracy: 0.5916\n",
      "Epoch 133/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1461 - accuracy: 0.5925\n",
      "Epoch 134/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1525 - accuracy: 0.5911\n",
      "Epoch 135/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1510 - accuracy: 0.5917\n",
      "Epoch 136/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1447 - accuracy: 0.5971\n",
      "Epoch 137/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1453 - accuracy: 0.5934\n",
      "Epoch 138/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1461 - accuracy: 0.5909\n",
      "Epoch 139/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1429 - accuracy: 0.5947\n",
      "Epoch 140/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1431 - accuracy: 0.5928\n",
      "Epoch 141/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1451 - accuracy: 0.5927\n",
      "Epoch 142/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1458 - accuracy: 0.5917\n",
      "Epoch 143/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1440 - accuracy: 0.5934\n",
      "Epoch 144/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1394 - accuracy: 0.5978\n",
      "Epoch 145/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1464 - accuracy: 0.5959\n",
      "Epoch 146/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1375 - accuracy: 0.5973\n",
      "Epoch 147/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1415 - accuracy: 0.5903\n",
      "Epoch 148/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1404 - accuracy: 0.5981\n",
      "Epoch 149/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1440 - accuracy: 0.5959\n",
      "Epoch 150/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1413 - accuracy: 0.5963\n",
      "Epoch 151/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1396 - accuracy: 0.5958\n",
      "Epoch 152/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1426 - accuracy: 0.5974\n",
      "Epoch 153/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1418 - accuracy: 0.5955\n",
      "Epoch 154/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1379 - accuracy: 0.6010\n",
      "Epoch 155/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1405 - accuracy: 0.5971\n",
      "Epoch 156/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1366 - accuracy: 0.5968\n",
      "Epoch 157/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1468 - accuracy: 0.5936\n",
      "Epoch 158/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1345 - accuracy: 0.5988\n",
      "Epoch 159/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1371 - accuracy: 0.5974\n",
      "Epoch 160/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1397 - accuracy: 0.5958\n",
      "Epoch 161/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1413 - accuracy: 0.5915\n",
      "Epoch 162/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1423 - accuracy: 0.5970\n",
      "Epoch 163/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1424 - accuracy: 0.5962\n",
      "Epoch 164/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1390 - accuracy: 0.5934\n",
      "Epoch 165/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1334 - accuracy: 0.5980\n",
      "Epoch 166/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1372 - accuracy: 0.5957\n",
      "Epoch 167/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1386 - accuracy: 0.5963\n",
      "Epoch 168/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1366 - accuracy: 0.5967\n",
      "Epoch 169/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1402 - accuracy: 0.5975\n",
      "Epoch 170/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1338 - accuracy: 0.5972\n",
      "Epoch 171/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1387 - accuracy: 0.5944\n",
      "Epoch 172/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1358 - accuracy: 0.5988\n",
      "Epoch 173/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1362 - accuracy: 0.6004\n",
      "Epoch 174/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1339 - accuracy: 0.5973\n",
      "Epoch 175/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1353 - accuracy: 0.5999\n",
      "Epoch 176/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1344 - accuracy: 0.5957\n",
      "Epoch 177/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1422 - accuracy: 0.5933\n",
      "Epoch 178/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1332 - accuracy: 0.5980\n",
      "Epoch 179/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1367 - accuracy: 0.5992\n",
      "Epoch 180/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1344 - accuracy: 0.5990\n",
      "Epoch 181/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1313 - accuracy: 0.6010\n",
      "Epoch 182/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1337 - accuracy: 0.6000\n",
      "Epoch 183/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1362 - accuracy: 0.5961\n",
      "Epoch 184/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1349 - accuracy: 0.5992\n",
      "Epoch 185/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1353 - accuracy: 0.5962\n",
      "Epoch 186/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1362 - accuracy: 0.5965\n",
      "Epoch 187/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1300 - accuracy: 0.6007\n",
      "Epoch 188/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1300 - accuracy: 0.6006\n",
      "Epoch 189/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1283 - accuracy: 0.6026\n",
      "Epoch 190/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1373 - accuracy: 0.5961\n",
      "Epoch 191/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1347 - accuracy: 0.5977\n",
      "Epoch 192/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1356 - accuracy: 0.5963\n",
      "Epoch 193/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1342 - accuracy: 0.6003\n",
      "Epoch 194/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1280 - accuracy: 0.6043\n",
      "Epoch 195/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1319 - accuracy: 0.5980\n",
      "Epoch 196/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1267 - accuracy: 0.6014\n",
      "Epoch 197/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1325 - accuracy: 0.5985\n",
      "Epoch 198/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1308 - accuracy: 0.6043\n",
      "Epoch 199/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1291 - accuracy: 0.6016\n",
      "Epoch 200/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1293 - accuracy: 0.5993\n",
      "Epoch 201/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1305 - accuracy: 0.6002\n",
      "Epoch 202/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1351 - accuracy: 0.5970\n",
      "Epoch 203/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1366 - accuracy: 0.6015\n",
      "Epoch 204/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1312 - accuracy: 0.5989\n",
      "Epoch 205/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1270 - accuracy: 0.6000\n",
      "Epoch 206/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1262 - accuracy: 0.6017\n",
      "Epoch 207/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1255 - accuracy: 0.5997\n",
      "Epoch 208/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1254 - accuracy: 0.5997\n",
      "Epoch 209/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1236 - accuracy: 0.6041\n",
      "Epoch 210/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1285 - accuracy: 0.6014\n",
      "Epoch 211/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1236 - accuracy: 0.6000\n",
      "Epoch 212/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1249 - accuracy: 0.6005\n",
      "Epoch 213/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1250 - accuracy: 0.6006\n",
      "Epoch 214/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1246 - accuracy: 0.6012\n",
      "Epoch 215/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1301 - accuracy: 0.6012\n",
      "Epoch 216/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1324 - accuracy: 0.5994\n",
      "Epoch 217/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1303 - accuracy: 0.5959\n",
      "Epoch 218/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1256 - accuracy: 0.5998\n",
      "Epoch 219/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1285 - accuracy: 0.5997\n",
      "Epoch 220/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1259 - accuracy: 0.6015\n",
      "Epoch 221/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1257 - accuracy: 0.5998\n",
      "Epoch 222/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1272 - accuracy: 0.5991\n",
      "Epoch 223/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1291 - accuracy: 0.5985\n",
      "Epoch 224/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1241 - accuracy: 0.6008\n",
      "Epoch 225/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1237 - accuracy: 0.6013\n",
      "Epoch 226/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1288 - accuracy: 0.6024\n",
      "Epoch 227/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1275 - accuracy: 0.6013\n",
      "Epoch 228/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1296 - accuracy: 0.5964\n",
      "Epoch 229/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1260 - accuracy: 0.6015\n",
      "Epoch 230/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1215 - accuracy: 0.6063\n",
      "Epoch 231/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1247 - accuracy: 0.6015\n",
      "Epoch 232/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1246 - accuracy: 0.6041\n",
      "Epoch 233/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1235 - accuracy: 0.6018\n",
      "Epoch 234/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1226 - accuracy: 0.6028\n",
      "Epoch 235/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1244 - accuracy: 0.6007\n",
      "Epoch 236/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1185 - accuracy: 0.6041\n",
      "Epoch 237/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1231 - accuracy: 0.6015\n",
      "Epoch 238/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1261 - accuracy: 0.6036\n",
      "Epoch 239/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1217 - accuracy: 0.6000\n",
      "Epoch 240/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1245 - accuracy: 0.5995\n",
      "Epoch 241/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1215 - accuracy: 0.5990\n",
      "Epoch 242/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1204 - accuracy: 0.6023\n",
      "Epoch 243/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1210 - accuracy: 0.6008\n",
      "Epoch 244/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1217 - accuracy: 0.6023\n",
      "Epoch 245/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1246 - accuracy: 0.6046\n",
      "Epoch 246/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1233 - accuracy: 0.6017\n",
      "Epoch 247/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1235 - accuracy: 0.6015\n",
      "Epoch 248/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1199 - accuracy: 0.6051\n",
      "Epoch 249/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1218 - accuracy: 0.6024\n",
      "Epoch 250/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1275 - accuracy: 0.6022\n",
      "Epoch 251/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1196 - accuracy: 0.6008\n",
      "Epoch 252/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1184 - accuracy: 0.6021\n",
      "Epoch 253/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1231 - accuracy: 0.6035\n",
      "Epoch 254/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1228 - accuracy: 0.6021\n",
      "Epoch 255/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1175 - accuracy: 0.6020\n",
      "Epoch 256/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1173 - accuracy: 0.6060\n",
      "Epoch 257/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1219 - accuracy: 0.6022\n",
      "Epoch 258/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1195 - accuracy: 0.6020\n",
      "Epoch 259/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1180 - accuracy: 0.6028\n",
      "Epoch 260/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1225 - accuracy: 0.6023\n",
      "Epoch 261/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1179 - accuracy: 0.6039\n",
      "Epoch 262/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1203 - accuracy: 0.6050\n",
      "Epoch 263/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1167 - accuracy: 0.6031\n",
      "Epoch 264/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1188 - accuracy: 0.6044\n",
      "Epoch 265/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1223 - accuracy: 0.6043\n",
      "Epoch 266/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1197 - accuracy: 0.6011\n",
      "Epoch 267/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1167 - accuracy: 0.6055\n",
      "Epoch 268/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1207 - accuracy: 0.5996\n",
      "Epoch 269/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1173 - accuracy: 0.6079\n",
      "Epoch 270/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1153 - accuracy: 0.6013\n",
      "Epoch 271/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1167 - accuracy: 0.6061\n",
      "Epoch 272/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1217 - accuracy: 0.6043\n",
      "Epoch 273/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1196 - accuracy: 0.6036\n",
      "Epoch 274/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1172 - accuracy: 0.6036\n",
      "Epoch 275/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1211 - accuracy: 0.6005\n",
      "Epoch 276/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1205 - accuracy: 0.6006\n",
      "Epoch 277/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1181 - accuracy: 0.6056\n",
      "Epoch 278/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1217 - accuracy: 0.6056\n",
      "Epoch 279/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1144 - accuracy: 0.6046\n",
      "Epoch 280/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1117 - accuracy: 0.6061\n",
      "Epoch 281/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1152 - accuracy: 0.6026\n",
      "Epoch 282/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1195 - accuracy: 0.6033\n",
      "Epoch 283/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1150 - accuracy: 0.6097\n",
      "Epoch 284/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1206 - accuracy: 0.6033\n",
      "Epoch 285/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1153 - accuracy: 0.6048\n",
      "Epoch 286/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1186 - accuracy: 0.6022\n",
      "Epoch 287/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1132 - accuracy: 0.6038\n",
      "Epoch 288/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1160 - accuracy: 0.6051\n",
      "Epoch 289/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1190 - accuracy: 0.6021\n",
      "Epoch 290/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1171 - accuracy: 0.6028\n",
      "Epoch 291/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1160 - accuracy: 0.6038\n",
      "Epoch 292/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1165 - accuracy: 0.6012\n",
      "Epoch 293/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1151 - accuracy: 0.6069\n",
      "Epoch 294/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1145 - accuracy: 0.6050\n",
      "Epoch 295/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1150 - accuracy: 0.6062\n",
      "Epoch 296/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1152 - accuracy: 0.6025\n",
      "Epoch 297/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1153 - accuracy: 0.6038\n",
      "Epoch 298/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1169 - accuracy: 0.6056\n",
      "Epoch 299/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1169 - accuracy: 0.6057\n",
      "Epoch 300/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1165 - accuracy: 0.6046\n",
      "Epoch 301/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1158 - accuracy: 0.6025\n",
      "Epoch 302/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1184 - accuracy: 0.6030\n",
      "Epoch 303/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1126 - accuracy: 0.6069\n",
      "Epoch 304/400\n",
      "1111/1111 [==============================] - 3s 2ms/step - loss: 1.1169 - accuracy: 0.6088\n",
      "Epoch 305/400\n",
      "1111/1111 [==============================] - 3s 2ms/step - loss: 1.1260 - accuracy: 0.6012\n",
      "Epoch 306/400\n",
      "1111/1111 [==============================] - 3s 2ms/step - loss: 1.1157 - accuracy: 0.6035\n",
      "Epoch 307/400\n",
      "1111/1111 [==============================] - 3s 2ms/step - loss: 1.1157 - accuracy: 0.6049\n",
      "Epoch 308/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1146 - accuracy: 0.6052\n",
      "Epoch 309/400\n",
      "1111/1111 [==============================] - 162s 146ms/step - loss: 1.1156 - accuracy: 0.6060\n",
      "Epoch 310/400\n",
      "1111/1111 [==============================] - 4s 4ms/step - loss: 1.1187 - accuracy: 0.6042\n",
      "Epoch 311/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.1176 - accuracy: 0.6029\n",
      "Epoch 312/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1108 - accuracy: 0.6070\n",
      "Epoch 313/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1161 - accuracy: 0.6067\n",
      "Epoch 314/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1113 - accuracy: 0.6060\n",
      "Epoch 315/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1119 - accuracy: 0.6058\n",
      "Epoch 316/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1120 - accuracy: 0.6069\n",
      "Epoch 317/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1125 - accuracy: 0.6071\n",
      "Epoch 318/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1168 - accuracy: 0.6026\n",
      "Epoch 319/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.1104 - accuracy: 0.6057\n",
      "Epoch 320/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1159 - accuracy: 0.6063\n",
      "Epoch 321/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1140 - accuracy: 0.6070\n",
      "Epoch 322/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1107 - accuracy: 0.6082\n",
      "Epoch 323/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1178 - accuracy: 0.6088\n",
      "Epoch 324/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1147 - accuracy: 0.6059\n",
      "Epoch 325/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1095 - accuracy: 0.6074\n",
      "Epoch 326/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1095 - accuracy: 0.6087\n",
      "Epoch 327/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1138 - accuracy: 0.6032\n",
      "Epoch 328/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1131 - accuracy: 0.6084\n",
      "Epoch 329/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1100 - accuracy: 0.6060\n",
      "Epoch 330/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1134 - accuracy: 0.6065\n",
      "Epoch 331/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1156 - accuracy: 0.6048\n",
      "Epoch 332/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1158 - accuracy: 0.6090\n",
      "Epoch 333/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1128 - accuracy: 0.6055\n",
      "Epoch 334/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1139 - accuracy: 0.6032\n",
      "Epoch 335/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1139 - accuracy: 0.6054\n",
      "Epoch 336/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1134 - accuracy: 0.6048\n",
      "Epoch 337/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1123 - accuracy: 0.6084\n",
      "Epoch 338/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1123 - accuracy: 0.6070\n",
      "Epoch 339/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1102 - accuracy: 0.6050\n",
      "Epoch 340/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1062 - accuracy: 0.6077\n",
      "Epoch 341/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1138 - accuracy: 0.6074\n",
      "Epoch 342/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1100 - accuracy: 0.6088\n",
      "Epoch 343/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1090 - accuracy: 0.6092\n",
      "Epoch 344/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.1086 - accuracy: 0.6062\n",
      "Epoch 345/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.1075 - accuracy: 0.6081\n",
      "Epoch 346/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1138 - accuracy: 0.6074\n",
      "Epoch 347/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1112 - accuracy: 0.6082\n",
      "Epoch 348/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1064 - accuracy: 0.6090\n",
      "Epoch 349/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1133 - accuracy: 0.6078\n",
      "Epoch 350/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1087 - accuracy: 0.6091\n",
      "Epoch 351/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1077 - accuracy: 0.6062\n",
      "Epoch 352/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1106 - accuracy: 0.6085\n",
      "Epoch 353/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1085 - accuracy: 0.6121\n",
      "Epoch 354/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1090 - accuracy: 0.6067\n",
      "Epoch 355/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1130 - accuracy: 0.6087\n",
      "Epoch 356/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1075 - accuracy: 0.6087\n",
      "Epoch 357/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1098 - accuracy: 0.6072\n",
      "Epoch 358/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1071 - accuracy: 0.6083\n",
      "Epoch 359/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1107 - accuracy: 0.6062\n",
      "Epoch 360/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1057 - accuracy: 0.6087\n",
      "Epoch 361/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1123 - accuracy: 0.6035\n",
      "Epoch 362/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1098 - accuracy: 0.6071\n",
      "Epoch 363/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1067 - accuracy: 0.6077\n",
      "Epoch 364/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1100 - accuracy: 0.6067\n",
      "Epoch 365/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1062 - accuracy: 0.6079\n",
      "Epoch 366/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1115 - accuracy: 0.6074\n",
      "Epoch 367/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1103 - accuracy: 0.6066\n",
      "Epoch 368/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1116 - accuracy: 0.6086\n",
      "Epoch 369/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1117 - accuracy: 0.6068\n",
      "Epoch 370/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1109 - accuracy: 0.6081\n",
      "Epoch 371/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1104 - accuracy: 0.6095\n",
      "Epoch 372/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1051 - accuracy: 0.6093\n",
      "Epoch 373/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1088 - accuracy: 0.6070\n",
      "Epoch 374/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1038 - accuracy: 0.6090\n",
      "Epoch 375/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1044 - accuracy: 0.6102\n",
      "Epoch 376/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1124 - accuracy: 0.6040\n",
      "Epoch 377/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1115 - accuracy: 0.6040\n",
      "Epoch 378/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1116 - accuracy: 0.6054\n",
      "Epoch 379/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1124 - accuracy: 0.6046\n",
      "Epoch 380/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1096 - accuracy: 0.6084\n",
      "Epoch 381/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1120 - accuracy: 0.6095\n",
      "Epoch 382/400\n",
      "1111/1111 [==============================] - 4s 3ms/step - loss: 1.1085 - accuracy: 0.6059\n",
      "Epoch 383/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1048 - accuracy: 0.6081\n",
      "Epoch 384/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1061 - accuracy: 0.6116\n",
      "Epoch 385/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1080 - accuracy: 0.6068\n",
      "Epoch 386/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1108 - accuracy: 0.6069\n",
      "Epoch 387/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1108 - accuracy: 0.6067\n",
      "Epoch 388/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1081 - accuracy: 0.6076\n",
      "Epoch 389/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1060 - accuracy: 0.6071\n",
      "Epoch 390/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1094 - accuracy: 0.6113\n",
      "Epoch 391/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1088 - accuracy: 0.6088\n",
      "Epoch 392/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1090 - accuracy: 0.6088\n",
      "Epoch 393/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1020 - accuracy: 0.6090\n",
      "Epoch 394/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1100 - accuracy: 0.6071\n",
      "Epoch 395/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1109 - accuracy: 0.6088\n",
      "Epoch 396/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1058 - accuracy: 0.6097\n",
      "Epoch 397/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1060 - accuracy: 0.6099\n",
      "Epoch 398/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1059 - accuracy: 0.6075\n",
      "Epoch 399/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1060 - accuracy: 0.6120\n",
      "Epoch 400/400\n",
      "1111/1111 [==============================] - 3s 3ms/step - loss: 1.1062 - accuracy: 0.6066\n",
      "171/171 [==============================] - 0s 1ms/step\n",
      "171/171 [==============================] - 0s 981us/step\n",
      "171/171 [==============================] - 0s 965us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5975877192982456"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ens.fit(X_train, y_train)\n",
    "\n",
    "ens.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7e656c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171/171 [==============================] - 0s 1ms/step\n",
      "171/171 [==============================] - 0s 970us/step\n",
      "171/171 [==============================] - 0s 939us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 5, 6, 7], dtype=int64),\n",
       " array([2450, 1638,   12,    4,   61, 1307], dtype=int64))"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(ens.predict(X_test), return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9c277315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171/171 [==============================] - 0s 2ms/step\n",
      "171/171 [==============================] - 0s 891us/step\n",
      "171/171 [==============================] - 0s 932us/step\n",
      "[0 1 1 ... 0 1 7]\n",
      "[0 0 0 ... 0 3 7]\n"
     ]
    }
   ],
   "source": [
    "print(ens.predict(X_test))\n",
    "print(y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
